{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Optimisation Différentiable \n",
    "<h3 align=\"right\"> ENSAE, Avril 2018 </h3>\n",
    "\n",
    "\n",
    "<h4 align=\"right\"> Hicham Janati </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de Newton\n",
    "### 1.1 Introduction\n",
    "Soit $f$ une fonction de classe $\\mathcal{C}^1$ de $\\mathbb{R}^n$ dans $\\mathbb{R}^n$. Le but de la méthode de Newton est de résoudre $f(x) = 0$.\n",
    "Soit $x_0$ dans $\\mathbb{R}^n$. L'approximation de premier ordre de $f$ dans un voisinage de $x_0$ est donnée par:\n",
    "$$ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) $$\n",
    "Oû $J$ est la matrice Jacobienne de f.\n",
    "Annuler la tangeante $\\hat{f}$ donne $x = x_0 - J^{-1}f(x_0)$ et obtient donc la suite d'itérés:\n",
    "$$x_k = x_{k-1} - J^{-1}f(x_{k-1})$$\n",
    "\n",
    "\n",
    "#### Question 1\n",
    "Soit $x^{\\star}$ un zéro de $f$. Supposons que $J(x^{\\star})$ est inversible et que $f$ est de classe $\\mathcal{C}^2$. Montrez que la méthode de Newton a une convergence localement quadratique i.e qu'il existe une boule B centrée en $x^{\\star}$ telle que pour tout $x_0$ dans B, il existe $\\alpha > 0$ telle que la suite de Newton vérifie: $$ \\|x_{k+1} - x^{\\star}\\| < \\alpha \\|x_{k} - x^{\\star}\\|^2 $$\n",
    "###### indication: Écrire l'approximation de deuxième ordre de f avec reste intégral.\n",
    "\n",
    "---------\n",
    "\n",
    "Deux remarques importantes:\n",
    "- Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de $x_0$.\n",
    "- $J$ doit être inversible.\n",
    "\n",
    "---------\n",
    "\n",
    "### 1.2 Optimisation sans contraintes\n",
    "Soit $g$ une fonction de classe $\\mathcal{C}^2$ de $\\mathbb{R}^n$ dans $\\mathbb{R}$.\n",
    "\n",
    "##### Question 2 \n",
    "Adapter la méthode de Newton pour résoudre $\\min_{x \\in \\mathbb{R}^n} g(x)$.\n",
    "\n",
    " <br> <font color=\"blue\"> Pour les questions 3-4-5,  On prend $g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2$, avec $A \\in \\mathcal{M}_{m, n}(\\mathbb{R})$, $b \\in \\mathbb{R}^m $ et $\\gamma \\geq 0 $</font>\n",
    "\n",
    "##### Question 3\n",
    "Donner le gradient et la hessienne de g et complétez les fonctions `gradient` et `hessian` ci-dessous.\n",
    "Vérifiez votre gradient avec l'approximation numérique donnée par ```scipy.optimize.check_grad```.\n",
    "##### Question 4\n",
    "Lorsque $\\gamma > 0$, montrez que la méthode de Newton converge en une itération indépendemment de $x_0$.\n",
    "\n",
    "##### Question 5\n",
    "\n",
    "Complétez la fonction `newton` ci-dessous pour résoudre (2). Calculer l'inverse de la hessienne est très couteux (complexité $O(n^3)$), comment peut-on y remédier ?\n",
    "Vérifiez le point (4) numériquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 1729 # Seed du générateur aléatoire\n",
    "m, n = 50, 100\n",
    "rnd = np.random.RandomState(seed) # générateur aléatoire\n",
    "A = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\n",
    "b = rnd.randn(m) # on génére b aléatoirement également \n",
    "gamma = 1.\n",
    "\n",
    "def g(x):\n",
    "    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n",
    "    Ax = A.dot(x)\n",
    "    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n",
    "    return gx\n",
    "\n",
    "def gradient_g(x):\n",
    "    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n",
    "    # A faire\n",
    "    \n",
    "def hessian_g(x):\n",
    "    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n",
    "    # A faire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy `scipy.optimize.check_grad`. \n",
    "Exécutez ```scipy.optimize.check_grad?``` pour obtenir la documentation de la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "x_test = rnd.randn(n) # point où on veut évaluer le gradient\n",
    "check_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n",
    "    \"\"\"Solve min g with newton method\"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    if verbose:\n",
    "        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n",
    "        strings = [s.center(13) for s in strings]\n",
    "        strings = \" | \".join(strings)\n",
    "        print(strings)\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        H = hessian(x)\n",
    "        d = gradient(x)\n",
    "        \n",
    "        if verbose:\n",
    "            obj = g(x)\n",
    "            strings = [i, obj, abs(d).max()] # On affiche des trucs \n",
    "            strings = [str(s).center(13) for s in strings]\n",
    "            strings = \" | \".join(strings)\n",
    "            print(strings)\n",
    "        \n",
    "        # A faire\n",
    "        x = \n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = rnd.randn(n)\n",
    "x = newton(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Optimisation avec contraintes d'égalité\n",
    "\n",
    "On s'intéresse à présent au problème avec contrainte linéaire: $$ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 $$\n",
    "\n",
    "##### Question 6\n",
    "Donnez (en justifiant) le système KKT du problème.\n",
    "\n",
    "##### Question 7\n",
    "Expliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n",
    "\n",
    "##### Question 8\n",
    "Implémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n",
    "\n",
    "##### Question 9\n",
    "Implémentez la version de newton adaptée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 5 # nombre de contraintes\n",
    "C = rnd.randn(p, n)\n",
    "d = rnd.randn(p)\n",
    "\n",
    "def F(...):\n",
    "    \"\"\"Compute the function F.\"\"\"\n",
    "    # A faire\n",
    "    \n",
    "def jac_F(x):\n",
    "    \"\"\"Compute the jacobian of F.\"\"\"\n",
    "    # A faire\n",
    "\n",
    "def newton_constrained( ):\n",
    "    # A faire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
