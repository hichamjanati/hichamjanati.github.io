{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* * *\n",
        "<pre> INSEA            <i> Techniques de réduction de dimension - 2025 </i></pre>\n",
        "* * *\n",
        "\n",
        "\n",
        "<h1 align=\"center\"> TP 5: Optimization and supervised representations  </h1>\n",
        "\n",
        "<pre align=\"left\">           <i> Author: Hicham Janati </i></pre>\n",
        "* * *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to follow this lab:\n",
        "\n",
        "- The goal is to **understand AND retain in the long term**: resist copy-pasting, prefer typing manually.\n",
        "- Getting stuck while programming is completely normal: search online, use documentation, or use the *AI*.\n",
        "- When prompting the AI, you must be specific. Explain that your goal is to learn, not to get an instant solution no matter what. Ask for short, explained answers with alternatives.\n",
        "- **NEVER ASK THE AI TO PRODUCE MORE THAN ONE LINE OF CODE!**\n",
        "- Adopt the `Solve-It` method: always try to solve a question or predict the output of code *before* running it. Learning happens when you confirm your understanding—and even more when you're wrong and surprised.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Logistic Regression from Scratch\n",
        "\n",
        "In this first part, we will implement logistic regression from scratch using only NumPy. This will help you understand the fundamental concepts of machine learning: loss functions, gradients, and optimization.\n",
        "\n",
        "We'll work with the digits dataset from scikit-learn (that we used last week), which contains images of handwritten digits (0-9). For simplicity, we'll start with a binary classification problem: distinguishing digit 0 from digit 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Loading and preparing the data\n",
        "\n",
        "Let's start by loading the digits dataset and preparing it for binary classification:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.optimize import check_grad\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"Unique labels: {np.unique(y)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1:\n",
        "Filter the dataset to keep only digits 0 and 1. Then split the data into training and test sets (use 80% for training, 20% for test). What are the shapes of your training and test sets?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2:\n",
        "Normalize the features by subtracting the mean and dividing by the standard deviation. Why is this important? Apply the normalization to both training and test sets, but compute the mean and standard deviation only from the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 The logistic regression model\n",
        "\n",
        "Logistic regression models the probability that a sample belongs to class 1 using the sigmoid function:\n",
        "\n",
        "$$P(y=1 | x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}$$\n",
        "\n",
        "where $w$ is the weight vector (including the bias term) and $\\sigma$ is the sigmoid function.\n",
        "\n",
        "### Question 3:\n",
        "Implement the sigmoid function. What is the range of its output? What happens when the input is very large (positive or negative)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function: sigma(z) = 1 / (1 + exp(-z))\n",
        "    \n",
        "    Args:\n",
        "        z: Input (can be a scalar or array)\n",
        "    \n",
        "    Returns:\n",
        "        Sigmoid of z\n",
        "    \"\"\"\n",
        "    # TODO: implement the sigmoid function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4:\n",
        "Implement a function that computes the predictions (probabilities) for a given weight vector $w$ and input data $X$. The function should return probabilities for each sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_proba(X, w):\n",
        "    \"\"\"\n",
        "    Compute the probability P(y=1 | x) for each sample in X.\n",
        "    \n",
        "    Args:\n",
        "        X: Input data (n_samples, n_features)\n",
        "        w: Weight vector (n_features,)\n",
        "    \n",
        "    Returns:\n",
        "        Probabilities (n_samples,)\n",
        "    \"\"\"\n",
        "\n",
        "# test it with random weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 The loss function\n",
        "\n",
        "For binary classification, we use the binary cross-entropy loss (also called log loss):\n",
        "\n",
        "$$L(w) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(w^T x_i)) + (1-y_i) \\log(1-\\sigma(w^T x_i)) \\right]$$\n",
        "\n",
        "This loss function penalizes confident wrong predictions more than uncertain ones.\n",
        "\n",
        "### Question 5:\n",
        "Implement the loss function. What happens if $\\sigma(w^T x_i) = 0$ when $y_i = 1$? How can we avoid numerical issues?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(X, y, w):\n",
        "    \"\"\"\n",
        "    Compute the binary cross-entropy loss.\n",
        "    \n",
        "    Args:\n",
        "        X: Input data (n_samples, n_features)\n",
        "        y: True labels (n_samples,)\n",
        "        w: Weight vector (n_features,)\n",
        "    \n",
        "    Returns:\n",
        "        Loss value (scalar)\n",
        "    \"\"\"\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Test the loss function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Computing the gradient\n",
        "\n",
        "To minimize the loss function using gradient descent, we need to compute its gradient with respect to the weights $w$. Compute the gradient of the binary cross-entropy loss with pen and paper then implement the gradient function. Verify that the output has the same shape as the weight vector $w$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient(X, y, w):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the loss with respect to w.\n",
        "    \n",
        "    Args:\n",
        "        X: Input data (n_samples, n_features)\n",
        "        y: True labels (n_samples,)\n",
        "        w: Weight vector (n_features,)\n",
        "    \n",
        "    Returns:\n",
        "        Gradient vector (n_features,)\n",
        "    \"\"\"\n",
        "\n",
        "    return gradient\n",
        "\n",
        "# Test the gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Gradient checking\n",
        "\n",
        "Before implementing gradient descent, it's crucial to verify that our gradient computation is correct. We can do this using numerical differentiation and comparing it with our analytical gradient.\n",
        "\n",
        "### Question 7:\n",
        "We can use `scipy.optimize.check_grad` to verify that your gradient implementation of the loss is correct. Here's an example with the function:\n",
        "$$ x \\mapsto f(x, a, b) = a \\|x\\|^2 + b^\\top x $$\n",
        "its gradient is given by: $$ \\nabla_x f(x, a, b) = 2 a x + b$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrapper functions for check_grad\n",
        "import numpy as np\n",
        "from scipy.optimize import check_grad\n",
        "\n",
        "dim = 10\n",
        "a = 5\n",
        "b = np.random.randn(dim)\n",
        "\n",
        "def f(x, a, b):\n",
        "    return a * np.linalg.norm(x)**2 + b @ x\n",
        "\n",
        "def grad_f(x, a, b):\n",
        "    return 2 * a * x + b\n",
        "\n",
        "\n",
        "def loss_wrapper(x):\n",
        "    return f(x, a, b)\n",
        "\n",
        "def grad_wrapper(x):\n",
        "    return grad_f(x, a, b)\n",
        "\n",
        "x_check = np.random.randn(dim)\n",
        "\n",
        "error = check_grad(loss_wrapper, grad_wrapper, x_check)\n",
        "\n",
        "print(f\"Gradient check error: {error:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Gradient descent\n",
        "\n",
        "Now that we've verified our gradient, we can implement gradient descent to minimize the loss function. The update rule is:\n",
        "\n",
        "$$w_{t+1} = w_t - \\alpha \\nabla_w L(w_t)$$\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "### Question 8:\n",
        "Implement gradient descent. The function should:\n",
        "1. Initialize weights (you can use zeros or small random values)\n",
        "2. For each iteration:\n",
        "   - Compute the gradient\n",
        "   - Update the weights\n",
        "   - Optionally store the loss for visualization\n",
        "3. Return the final weights and the history of losses\n",
        "\n",
        "\n",
        "Visualize the curve of the loss as function of the iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, verbose=True):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to minimize the loss.\n",
        "    \n",
        "    Args:\n",
        "        X: Input data (n_samples, n_features)\n",
        "        y: True labels (n_samples,)\n",
        "        learning_rate: Step size for gradient descent\n",
        "        n_iterations: Number of iterations\n",
        "        verbose: Whether to print progress\n",
        "    \n",
        "    Returns:\n",
        "        w: Final weight vector\n",
        "        loss_history: List of loss values at each iteration\n",
        "    \"\"\"\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.7 Effect of learning rate\n",
        "\n",
        "The learning rate is a crucial hyperparameter. If it's too small, convergence is slow. If it's too large, the algorithm might diverge or oscillate.\n",
        "\n",
        "### Question 10:\n",
        "Run gradient descent with different learning rates (e.g., 0.001, 0.01, 0.1, 1.0) and compare:\n",
        "- The convergence speed\n",
        "- The final loss value\n",
        "- Whether the algorithm converges or diverges\n",
        "\n",
        "Visualize the loss curves for different learning rates on the same plot and explain what you observe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.8 Evaluating on test data\n",
        "\n",
        "Now that we've trained our model, we need to evaluate its performance on unseen test data. This is crucial to assess whether our model generalizes well.\n",
        "\n",
        "### Question 12:\n",
        "Implement a function that:\n",
        "1. Computes predictions (probabilities) for test data\n",
        "2. Converts probabilities to binary predictions (threshold = 0.5)\n",
        "3. Computes the accuracy: (number of correct predictions) / (total number of samples)\n",
        "\n",
        "What is the accuracy on the training set? On the test set? Are they similar?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(X, w, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Make binary predictions.\n",
        "    \n",
        "    Args:\n",
        "        X: Input data (n_samples, n_features)\n",
        "        w: Weight vector (n_features,)\n",
        "        threshold: Decision threshold\n",
        "    \n",
        "    Returns:\n",
        "        Binary predictions (n_samples,)\n",
        "    \"\"\"\n",
        "\n",
        "def compute_accuracy(X, y, w):\n",
        "    \"\"\"\n",
        "    Compute the accuracy of predictions.\n",
        "    \n",
        "    Args:\n",
        "        X: Input data (n_samples, n_features)\n",
        "        y: True labels (n_samples,)\n",
        "        w: Weight vector (n_features,)\n",
        "    \n",
        "    Returns:\n",
        "        Accuracy (scalar between 0 and 1)\n",
        "    \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Neural Networks with PyTorch\n",
        "\n",
        "In this second part, we'll explore neural networks using PyTorch. We'll build a neural network with one hidden layer and learn about automatic differentiation, which is one of the key features that makes deep learning frameworks powerful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Introduction to PyTorch\n",
        "\n",
        "PyTorch is a deep learning framework that provides automatic differentiation (autograd). This means we don't need to manually compute gradients—PyTorch tracks operations and can compute gradients automatically.\n",
        "\n",
        "Let's start by importing PyTorch and understanding tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Create a simple tensor\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"Tensor x: {x}\")\n",
        "print(f\"Tensor shape: {x.shape}\")\n",
        "print(f\"Tensor dtype: {x.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Automatic Differentiation (Autograd)\n",
        "\n",
        "The key feature of PyTorch is automatic differentiation. When we create a tensor with `requires_grad=True`, PyTorch tracks all operations on it and can compute gradients automatically.\n",
        "\n",
        "### Question 14:\n",
        "Run the following code and explain what happens. What is the difference between `requires_grad=True` and `requires_grad=False`?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tensor that requires gradient computation\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "print(f\"x: {x}\")\n",
        "print(f\"x.requires_grad: {x.requires_grad}\")\n",
        "\n",
        "# Define a simple function: y = x^2\n",
        "y = x ** 2\n",
        "print(f\"y: {y}\")\n",
        "print(f\"y.requires_grad: {y.requires_grad}\")\n",
        "print(f\"y.grad_fn: {y.grad_fn}\")  # This shows the operation that created y\n",
        "\n",
        "# Compute the gradient\n",
        "y.backward()  # This computes dy/dx\n",
        "print(f\"x.grad: {x.grad}\")  # Should be 2*x = 4.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 15:\n",
        "Try a more complex function: $z = x^2 + 2xy + y^2$ where $x=2$ and $y=3$. Compute $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$ using PyTorch's autograd. Verify manually that the gradients are correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Building a Neural Network\n",
        "\n",
        "Now let's build a neural network with one hidden layer. We'll use PyTorch's `nn.Module` class, which provides a convenient way to define neural networks.\n",
        "\n",
        "Our network will have:\n",
        "- Input layer: 64 features (8x8 image flattened)\n",
        "- Hidden layer: 32 neurons with ReLU activation\n",
        "- Output layer: 10 neurons (one for each digit 0-9) with softmax\n",
        "\n",
        "### Question 17:\n",
        "We create a neural network class that inherits from `nn.Module`. What is the purpose of the `__init__` and `forward` methods?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=32, output_size=10):\n",
        "        \"\"\"\n",
        "        Initialize the neural network.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_size: Number of neurons in the hidden layer\n",
        "            output_size: Number of output classes\n",
        "        \"\"\"\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Hidden to output\n",
        "        self.relu = nn.ReLU()  # Activation function\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor (batch_size, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = NeuralNetwork(input_size=64, hidden_size=32, output_size=10)\n",
        "print(model)\n",
        "\n",
        "# Test with a random input\n",
        "x_test = torch.randn(5, 64)  # Batch of 5 samples\n",
        "output = model(x_test)\n",
        "print(f\"\\nInput shape: {x_test.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Preparing the Data\n",
        "\n",
        "Before training, we need to convert our NumPy arrays to PyTorch tensors and create data loaders for efficient batch processing.\n",
        "\n",
        "### Question 18:\n",
        "We convert the digits dataset to PyTorch tensors. Use all 10 classes (not just 0 and 1). What is the difference between `torch.tensor()` and `torch.from_numpy()`?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load full digits dataset (all 10 classes)\n",
        "digits = load_digits()\n",
        "X_full = digits.data\n",
        "y_full = digits.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Normalize\n",
        "mean_train_full = X_train_full.mean(axis=0)\n",
        "std_train_full = X_train_full.std(axis=0)\n",
        "X_train_full_norm = (X_train_full - mean_train_full) / (std_train_full + 1e-8)\n",
        "X_test_full_norm = (X_test_full - mean_train_full) / (std_train_full + 1e-8)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.from_numpy(X_train_full_norm).float()\n",
        "y_train_tensor = torch.from_numpy(y_train_full).long()\n",
        "X_test_tensor = torch.from_numpy(X_test_full_norm).float()\n",
        "y_test_tensor = torch.from_numpy(y_test_full).long()\n",
        "\n",
        "print(f\"Training set: {X_train_tensor.shape}, {y_train_tensor.shape}\")\n",
        "print(f\"Test set: {X_test_tensor.shape}, {y_test_tensor.shape}\")\n",
        "\n",
        "# Create data loaders for batch processing\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nNumber of batches in training set: {len(train_loader)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Training the Model\n",
        "\n",
        "Now we'll train the neural network. The training loop involves:\n",
        "1. Forward pass: compute predictions\n",
        "2. Compute loss\n",
        "3. Backward pass: compute gradients\n",
        "4. Update weights using an optimizer\n",
        "\n",
        "We implement the training loop. Using:\n",
        "- Cross-entropy loss (`nn.CrossEntropyLoss`)\n",
        "- Stochastic gradient descent optimizer (`optim.SGD`)\n",
        "- Learning rate of 0.01\n",
        "\n",
        "Train for 100 epochs and print the loss every 10 epochs (iterations).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model, loss function, and optimizer\n",
        "model = NeuralNetwork(input_size=64, hidden_size=32, output_size=10)\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 100\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    n_batches = 0\n",
        "    \n",
        "    # Iterate over batches\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        n_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / n_batches\n",
        "    train_losses.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 20:\n",
        "Evaluate the model on the test set. Compute the accuracy. How does it compare to the training accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a dataset.\n",
        "    \n",
        "    Returns:\n",
        "        accuracy: Classification accuracy\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "        for batch_X, batch_y in data_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on training and test sets\n",
        "train_accuracy = evaluate_model(model, train_loader)\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Extracting Hidden Layer Representations\n",
        "\n",
        "One of the powerful aspects of neural networks is that the hidden layers learn useful representations of the input data. These representations can be used for visualization, transfer learning, or other tasks.\n",
        "\n",
        "### Question 21:\n",
        "Modify the `forward` method to also return the hidden layer activations. Then extract the hidden layer representations for all test samples and visualize them using PCA (project to 2D). Color the points by their true labels. Do the hidden representations separate the classes well?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 22:\n",
        "Compare the PCA visualization of the hidden representations with a PCA visualization of the original input data. Which one separates the classes better? What does this tell you about what the neural network learned?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
