{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdea7dd3",
   "metadata": {},
   "source": [
    "* * *\n",
    "<pre> INSEA            <i> Techniques de réduction de dimension - 2025 </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> TP 3: Numpy: Linear algebra and PCA </h1>\n",
    "\n",
    "<pre align=\"left\">           <i> Author: Hicham Janati </i></pre>\n",
    "* * *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2302ba",
   "metadata": {},
   "source": [
    "# How to follow this lab:\n",
    "\n",
    "- The goal is to **understand AND retain in the long term**: resist copy-pasting, prefer typing manually.\n",
    "- Getting stuck while programming is completely normal: search online, use documentation, or use *AI*.\n",
    "- When prompting the AI, you must be specific. Explain that your goal is to learn, not to get an instant solution no matter what. Ask for short, explained answers with alternatives.\n",
    "- **NEVER ASK THE AI TO PRODUCE MORE THAN ONE LINE OF CODE!**\n",
    "- Adopt the `Solve-It` method: always try to solve a question or predict the output of code *before* running it. Learning happens when you confirm your understanding—and even more when you’re wrong and surprised.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf61f12",
   "metadata": {},
   "source": [
    "# 1 - Eigenvalues, eigenvectors, singular values, singular vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0f664",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors can be computed using the `np.linalg.eig` function. Example with a symmetric matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.random.randn(5, 5)\n",
    "A = A + A.T\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b61143",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Verify the eigen property $Ax = \\lambda x$ for all eigenvalues in a vectorized form. Do these eigenvectors form an orthogonal matrix ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ccf092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e287fb",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "If the matrix is Hermitian (Hermitian is equivalent to Symmetric if the matrix is real), `np.linalg.eigh` must be prefered, it is more stable and faster. Compare their output and computation time for large matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8736f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdce402b",
   "metadata": {},
   "source": [
    "The SVD can be computed for any rectangular matrix using `np.linalg.svd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(5, 3)\n",
    "U, S, V_t = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b9b7b",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Verify the SVD formula $A = USV^\\top$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e6601ad",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "What is the link between the singular values of A and the eigenvalues of $A^\\top A$ ? Verify that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfba296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b283ddad",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "We can define several different norms of a rectangular matrix of rank r. In the theoretical exercises, we have demonstrated that they can all be obtained using the singular values:\n",
    "- The usual Frobenius norm: $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2} = \\sqrt{\\sum_{k=1}^r \\sigma_k^2}$\n",
    "- The spectral norm (also known as operator norm or L2 norm): $\\|A\\|_{op} = \\max_{x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2} = \\max_k {\\sigma_k}$\n",
    "- The nuclear norm (also known as Trace norm) $\\|A\\|_{*} = \\sum_{k} \\sigma_k$\n",
    "Verify how to compute each of these norms using the svd and directly via `np.linalg.norm` by specifying the correct `ord` argument. Check the table in the Notes for the different `ord` values: https://numpy.org/devdocs/reference/generated/numpy.linalg.norm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17b4dd",
   "metadata": {},
   "source": [
    "# 2 - PCA with Python\n",
    "\n",
    "The goal of this section is to make your own implementation of PCA with `numpy`. We will first be working with random data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c7eb4",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "Center the data and compute the empirical covariance matrix $\\Sigma$ of $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3da563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79f6d0e8",
   "metadata": {},
   "source": [
    "### Question 7:\n",
    "Compute the principal components of the PCA using the spectral decomposition of $\\Sigma$ and project the data on the first `k` components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c1368",
   "metadata": {},
   "source": [
    "### Question 8:\n",
    "How could you have obtained this projection without computing $\\Sigma$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc70310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c5f0cea",
   "metadata": {},
   "source": [
    "### Question 9:\n",
    "Package this logic in a function that takes a dataset, a number of components and returns the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4eb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(dataset, n_components):\n",
    "    # todo\n",
    "    return eigenvalues, eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd4d44",
   "metadata": {},
   "source": [
    "### Question 10:\n",
    "Compute the explained variance ratio of each component and visualize with matplotlib the scree plot. The plotting code is given below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "percentages = # todo\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(percentages, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1182743",
   "metadata": {},
   "source": [
    "### Question 11:\n",
    "Using `np.cumsum` make this plot visualize the cumulated explained variance percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663679bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10626dc8",
   "metadata": {},
   "source": [
    "### Question 12:\n",
    "How can we obtain the PCA projection in 2D of the data ? Visualize the obtained data in 2d, here's the matplotlib code for that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d_projection = # todo\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(pca_2d_projection[:, 0], pca_2d_projection[:, 1], marker='o')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Projection in 2D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0cc25a",
   "metadata": {},
   "source": [
    "### Question 13:\n",
    "We would like to project a new data point on the PCA space. Complete the following function that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_pca(data, eigenvalues, eigenvectors, n_components):\n",
    "    # todo\n",
    "    return projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb3505",
   "metadata": {},
   "source": [
    "# 3. Introduction to Object oriented programming (OOP) in Python:\n",
    "The goal of this section is to nicely package the PCA code into a clean abstraction using `class`. A `class` is a way to define a new object with attributes. Here's an example. We create a \"Player\" character in some 2D video game which can move around. Each character is defined with:\n",
    "- its speed (fixed)\n",
    "- its coordinates (x, y) that change during the game and are initially set to (0, 0).\n",
    "\n",
    "A Class needs a `constructor` function called `__init__` that sets attributes `speed`, `x` and `y`. The `self` argument is the object itself. The `__init__` function is called when we create a new instance of the object `player`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21312b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    def __init__(self, speed):\n",
    "        self.speed = speed\n",
    "        self.x = 0\n",
    "        self.y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b53b0",
   "metadata": {},
   "source": [
    "We can create a variable of type `player` with speed=5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3123b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Player(5) # here __init__ is called with speed=5\n",
    "\n",
    "print(f\"p1.speed: {p1.speed} | p1.x: {p1.x} | p1.y: {p1.y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594c49b",
   "metadata": {},
   "source": [
    "We can modify the attributes of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72006312",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.x += 10\n",
    "print(f\"p1.x: {p1.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7664b05a",
   "metadata": {},
   "source": [
    "But that is not a good practice ! To keep things clean, we never change attributes directly, instead we implement functions inside `class` called `methods`. We add a `move` method that changes the coordinates of the player given a direction and duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    def __init__(self, speed):\n",
    "        self.speed = speed\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "    \n",
    "    def move(self, direction_x, direction_y, dt=1):\n",
    "        # directions must be -1, 0 or 1\n",
    "        direction_x = np.sign(direction_x)\n",
    "        direction_y = np.sign(direction_y)\n",
    "        self.x += direction_x * self.speed * dt\n",
    "        self.y += direction_y * self.speed * dt\n",
    "\n",
    "p1 = Player(3)\n",
    "p1.move(direction_x=1, direction_y=-1, dt=5)\n",
    "print(f\"p1.x: {p1.x} | p1.y: {p1.y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a5bbd",
   "metadata": {},
   "source": [
    "We would like to apply this to create our own PCA class. A PCA class is defined with its number of components and can be applied to any data. \n",
    "### Question 14:\n",
    "Complete the following code using your previous implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPCA():\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None\n",
    "        # add attributes if needed\n",
    "\n",
    "    def computePCA(self, data):\n",
    "        # todo\n",
    "\n",
    "    def applyPCA(self, new_data):\n",
    "        # todo\n",
    "    \n",
    "    def plot_scree_plot(self):\n",
    "        # todo\n",
    "\n",
    "    def plot_cumulated_variance(self):\n",
    "        # todo\n",
    "\n",
    "    def plot_pca_projection(self, new_data):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab48606",
   "metadata": {},
   "source": [
    "You can now create your own `PCA` object. Test its functions on random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100, 5)\n",
    "\n",
    "pca = MyPCA(2)\n",
    "\n",
    "# todo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c21e2",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "Compare the PCA you obtained with scikit-learn's implementation which can be computed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5816ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "data = np.random.randn(100, 5)\n",
    "\n",
    "\n",
    "pca_sklearn = PCA(n_components=2)\n",
    "pca_sklearn.fit(data)\n",
    "explained_variance_ratio = pca_sklearn.explained_variance_ratio_\n",
    "projected_data = pca_sklearn.transform(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604e42d",
   "metadata": {},
   "source": [
    "# 4 - PCA for Machine learning\n",
    "\n",
    "PCA is mostly used as a preprocessing step if the dimension is very high. We will investigate this using synthetic data. The following code creates a dataset of variables (X) and binary classes (y) that we want to predict using k-NN. To keep this simple, we keep the number of observations (samples) fixed `n_samples`=100 and only change the dimension (`n_features`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38140cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=200, n_features=300, n_informative=10)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(f\"the first 5 y are: {y[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e15f8d",
   "metadata": {},
   "source": [
    "To test the performance of k-NN, we first split the data into a training and a test set. The train set is where we assume `y` is known. On the other hand, the test set is used to evaluate the model: we predict the labels `y` with the model, and evaluate its accuracy using `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d784f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25cb656",
   "metadata": {},
   "source": [
    "We can fit a k-NN classifier with 5 neighbors and predict the labels of `X_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde31819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(f\"the first 5 y_pred are: {y_pred[:5]}\")\n",
    "print(f\"the first 5 y_test are: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bc0c8",
   "metadata": {},
   "source": [
    "### Question 16:\n",
    "Write a function with numpy that computes the average accuracy comparing `y_test` with `y_true`. Compute the accuracy of the predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_true):\n",
    "    # todo \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b95e0",
   "metadata": {},
   "source": [
    "### Question 17:\n",
    "Apply PCA keeping only 20 components, then fit the k-NN model on the transformed data. Is the performance better or worse ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7830f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
