[
  {
    "objectID": "media/Opti/tpnewton.html",
    "href": "media/Opti/tpnewton.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/Opti/tpnewton.html#méthode-de-newton",
    "href": "media/Opti/tpnewton.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad. ##### Question 4 Lorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).\n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    \ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = \n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(...):\n    \"\"\"Compute the function F.\"\"\"\n    # A faire\n    \ndef jac_F(x):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # A faire\n\ndef newton_constrained( ):\n    # A faire"
  },
  {
    "objectID": "media/teaching/Opti/tpnewton.html",
    "href": "media/teaching/Opti/tpnewton.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/teaching/Opti/tpnewton.html#méthode-de-newton",
    "href": "media/teaching/Opti/tpnewton.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad. ##### Question 4 Lorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).\n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    \ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = \n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(...):\n    \"\"\"Compute the function F.\"\"\"\n    # A faire\n    \ndef jac_F(x):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # A faire\n\ndef newton_constrained( ):\n    # A faire"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html",
    "href": "media/teaching/SB/tp7_churn.html",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "",
    "text": "Objectives:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\nseed = 42\nrng = np.random.default_rng(seed)"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#problem-statement",
    "href": "media/teaching/SB/tp7_churn.html#problem-statement",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "1. Problem Statement",
    "text": "1. Problem Statement\nIn companies that offer services (such as a mobile phone operator), customer retention is a major challenge. The churn rate refers to the percentage of customers who decide to cancel their subscription (e.g., to switch to another provider). If the company can predict which customers are likely to churn, it can take proactive steps — such as offering additional services or special deals — to retain them.\nWe will work with a real dataset from a telecom operator (filtered and adapted from: Kaggle).\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/churn.csv\", index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\nDependents\nTechSupport\nContract\nInternetService\nCustomerID_Region\nMonthlyCharges\nMonths\nChurn\n\n\n\n\n0\nYes\nNo\nOne year\nFiber optic\nMIS-1\n78.95\n34.0\n0\n\n\n1\nYes\nYes\nTwo year\nDSL\nDAL-1\n85.95\n70.0\n0\n\n\n2\nNo\nYes\nTwo year\nFiber optic\nSAN-1\n104.00\n69.0\n0\n\n\n3\nNo\nNo internet service\nMonth-to-month\nNo\nHOU-1\n20.55\n5.0\n0\n\n\n4\nYes\nYes\nTwo year\nFiber optic\nHOU-1\n113.10\n72.0\n0\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['Dependents', 'TechSupport', 'Contract', 'InternetService',\n       'CustomerID_Region', 'MonthlyCharges', 'Months', 'Churn'],\n      dtype='object')\n\n\n\n# get the X and y variables\ny = df.Churn\nX = df.drop(\"Churn\", axis=1)\n\nCheck if the number of class instances we have:\n\ny.value_counts()\n\nChurn\n0    100\n1    100\nName: count, dtype: int64\n\n\nThis binary classification task is balanced: (in practice churn rates are significantly lower, churn prediction is very imbalanced in the real world. I made the problem easier here by resampling from the original data for the sake of simplicity). Let’s check the type of data variables we have:\n\nX.dtypes\n\nDependents            object\nTechSupport           object\nContract              object\nInternetService       object\nCustomerID_Region     object\nMonthlyCharges       float64\nMonths               float64\ndtype: object\n\n\n\nX.Contract.value_counts()\n\nContract\nMonth-to-month    133\nOne year           37\nTwo year           30\nName: count, dtype: int64"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#data-preprocessing",
    "href": "media/teaching/SB/tp7_churn.html#data-preprocessing",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "2. Data preprocessing",
    "text": "2. Data preprocessing\n\n2.1 One-hot encoding / dummy variables\nWe need to pre-process the data: turn the categorical variables to binary dummy variables. This is called one-hot encoding. Here is how it goes: - for a variable like Contract which takes Month-to-Month, One year or Two year (three categories) we can transform it to a 3 binary variables:\n\n\n\n…\nContract\n…\n\n\n\n\n…\nMonth-to-Month\n…\n\n\n…\nOne Year\n…\n\n\n…\nTwo Year\n…\n\n\n…\nMonth-to-Month\n…\n\n\n…\nTwo Year\n…\n\n\n…\nTwo Year\n…\n\n\n\n↓\n\n\n\n…\nMonth-to-Month\nOne Year\nTwo Year\n…\n\n\n\n\n…\n1\n0\n0\n…\n\n\n…\n0\n1\n0\n…\n\n\n…\n0\n0\n1\n…\n\n\n…\n1\n0\n0\n…\n\n\n…\n0\n0\n1\n…\n\n\n…\n0\n0\n1\n…\n\n\n\nThese binary variables are called dummy variables or the one-hot encoding of Contract. However you can notice that the sum of these columns will always be 1: the 3 binary variables are linearly dependent which is bad for linear models (particularly if no regularization is used), this is called a dummy variables trap. We should drop one of the columns to avoid it. Thus, a categorical variable with K categories is transformed into K-1 binary variables. We can do this with sklearn transformer object called OneHotEncoder:\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(drop='first')\nencoded_data = encoder.fit_transform(df[[\"Contract\"]])\nencoded_data\n\n&lt;200x2 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 67 stored elements in Compressed Sparse Row format&gt;\n\n\nThe output is a sparse matrix (CSR) which is a compressed form of storing matrices with lots of zeros: instead of storing all their entries, we only store in memory necessary information (for e.g the triplets (i, j, v) such that M[i, j] = v and v is not zero). Here the data is small, no need to used sparse matrices:\n\nencoder = OneHotEncoder(drop='first', sparse_output=False)\nencoded_data = encoder.fit_transform(X[[\"Contract\"]])\nencoded_data[:10]\n\narray([[1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 0.],\n       [0., 1.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 0.],\n       [0., 0.]])\n\n\nWe can apply this to all categorical variables:\n\nencoder = OneHotEncoder(drop='first', sparse_output=False)\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\", \"CustomerID_Region\"]\nencoded_data = encoder.fit_transform(X[categorical_features])\nencoded_data[:10]\n\narray([[1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0.],\n       [1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        1., 0.],\n       [0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0.],\n       [1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]])\n\n\nBut later we will have a regression coefficient for each one of these columns, how do we know which one belongs to which ? Well we can get their names from the encoder:\n\nencoder.get_feature_names_out(categorical_features)\n\narray(['Dependents_Yes', 'TechSupport_No internet service',\n       'TechSupport_Yes', 'Contract_One year', 'Contract_Two year',\n       'InternetService_Fiber optic', 'InternetService_No',\n       'CustomerID_Region_CHI-1', 'CustomerID_Region_DAL-1',\n       'CustomerID_Region_HOU-1', 'CustomerID_Region_LAX-1',\n       'CustomerID_Region_MIA-1', 'CustomerID_Region_MIS-1',\n       'CustomerID_Region_NYC-1', 'CustomerID_Region_PHL-1',\n       'CustomerID_Region_PHX-1', 'CustomerID_Region_SAN-1',\n       'CustomerID_Region_SEA-1'], dtype=object)\n\n\n\n\n2.2 Scaling numerical features\nWe also have continuous variables (numerical features): Months and MonthlyCharges. As explained in the last class, it’s important for them to have the same scale (order of magnitude) in a linear model. Scaling a variable means centering it and dividing by its standard deviation:\n\nvariable = X[\"Months\"]\nvariable = variable - variable.mean()\nvariable = variable / variable.std()\nvariable.head()\n\n0    0.326288\n1    1.850800\n2    1.808453\n3   -0.901791\n4    1.935495\nName: Months, dtype: float64\n\n\nIt is tedious to do this for each variable, the best way to do it is to use a sklearn transformer called StandardScaler:\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\nscaled_data = scaler.fit_transform(X[numeric_features])\nscaled_data[:10]\n\narray([[ 0.37846895,  0.32710679],\n       [ 0.63721955,  1.85544479],\n       [ 1.30442645,  1.81299095],\n       [-1.78025031, -0.90405438],\n       [ 1.64080222,  1.94035245],\n       [ 0.56698725, -1.03141588],\n       [ 1.37835519,  0.58182979],\n       [ 1.04937229,  0.66673745],\n       [ 0.19734354, -0.77669288],\n       [ 0.79062169, -0.73423905]])\n\n\n\n\n2.3 Merging both in one transformer:\nWe can handle both categorical and numerical features in one ColumnTransformer object:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\", \"CustomerID_Region\"]\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\n\ncategorical_transformer = OneHotEncoder(drop=\"first\", sparse_output=False)\nnumeric_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features),\n    ],\n)\n\ntransformed_data = preprocessor.fit_transform(X)\nprint(f\"The shape of the transformed data is {transformed_data.shape}\")\n\n# we construct a new pandas with the column names:\ncolumn_names = preprocessor.get_feature_names_out()\ntransformed_df = pd.DataFrame(transformed_data, columns=column_names)\ntransformed_df.head()\n\nThe shape of the transformed data is (200, 20)\n\n\n\n\n\n\n\n\n\ncat__Dependents_Yes\ncat__TechSupport_No internet service\ncat__TechSupport_Yes\ncat__Contract_One year\ncat__Contract_Two year\ncat__InternetService_Fiber optic\ncat__InternetService_No\ncat__CustomerID_Region_CHI-1\ncat__CustomerID_Region_DAL-1\ncat__CustomerID_Region_HOU-1\ncat__CustomerID_Region_LAX-1\ncat__CustomerID_Region_MIA-1\ncat__CustomerID_Region_MIS-1\ncat__CustomerID_Region_NYC-1\ncat__CustomerID_Region_PHL-1\ncat__CustomerID_Region_PHX-1\ncat__CustomerID_Region_SAN-1\ncat__CustomerID_Region_SEA-1\nnum__MonthlyCharges\nnum__Months\n\n\n\n\n0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.378469\n0.327107\n\n\n1\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.637220\n1.855445\n\n\n2\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.304426\n1.812991\n\n\n3\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.780250\n-0.904054\n\n\n4\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.640802\n1.940352\n\n\n\n\n\n\n\n\n\n2.4 Preprocessing and the train-test split\nBut when developing a machine learing model, we always follow the train-test paradigm where we split train-test data: therefore when transforming / encoding / scaling the variables we should only be using the train data. Otherwise info from the test data will be used by the model: for example, the scaling operation will use test samples to compute the mean and std. Therefore, the column transformer object should be fit on the train only, then we use the transform method on the test data:\n\nfrom sklearn.model_selection import train_test_split\n\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\", \"CustomerID_Region\"]\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\n\ncategorical_transformer = OneHotEncoder(drop=\"first\", sparse_output=False)\nnumeric_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features),\n    ],\n)\n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=42, stratify=y)\n\nX_train_processed = preprocessor.fit_transform(X_train)\n\nX_test_processed = preprocessor.transform(X_test)\n\n# we get the column names:\ncolumn_names = preprocessor.get_feature_names_out()"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#logistic-regression",
    "href": "media/teaching/SB/tp7_churn.html#logistic-regression",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "3. Logistic regression",
    "text": "3. Logistic regression\nWe can now start doing ML models. First, we use logistic regression without regularization:\n\n3.1 Without regularization\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# we can fit the logistic regression model with no regularization:\nmodel = LogisticRegression(penalty=None)\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.8357\nTest accuracy: 0.6500\n\n\nWe can see that the model isn’t quite good: 1. 83% accuracy on the training data means that the model isnt complex enough to predict labels it has already seen 2. 65% accuracy on the test data suggest a big difference between train and test: the models learns a bit of noise in the training data\nWe can extract the coefficients and visualize their values:\n\ncoef = pd.DataFrame(model.coef_, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWe can extract the change in the odd-ratios and visualize their importance in percentages (see TD6):\n\nodds_changes = (np.exp(model.coef_) - 1) * 100\ncoef = pd.DataFrame(odds_changes, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nodds_changes\n\narray([[ 184.33978368,  -73.24588216,   10.11408019,  -69.01164118,\n         -42.3604937 , 1867.04450222,  -73.24588216,  -57.30827598,\n         -67.89164809,  -27.90134379,   87.71595551,  -47.99574828,\n         -49.09364831,  -69.35050994,   10.37215112,  190.19346862,\n         -73.42643779,  875.62186833,  -54.79865072,  -77.0485994 ]])\n\n\n\nX[\"Months\"].std(), X[\"MonthlyCharges\"].std()\n\n(23.61410818827673, 27.120965039260753)\n\n\nSome odd percentage changes are off-the-charts ! 1867% increase with Optic Fiber and 875% if the customer is in the region SEA-1 ! Given that the performance of the model is poor here and the coefficient values are very large it is very likely the model has learned noise: regularization is needed.\n\n\n3.2 With L2 / L1 regularization\nWe can now try to improve the model by adding regularization. We can use the LogisticRegressionCV class which will automatically find the best regularization parameter for us:\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\n# we can fit the logistic regression model with no regularization:\nmodel = LogisticRegressionCV(Cs=np.logspace(-3, 3, 100), penalty=\"l2\")\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.7857\nTest accuracy: 0.7167\n\n\nThe test accuracy is slightly improved. The coefficients look very similar:\n\ncoef = pd.DataFrame(model.coef_, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nodds_changes = (np.exp(model.coef_) - 1) * 100\ncoef = pd.DataFrame(odds_changes, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nX[\"Months\"].std(), X[\"MonthlyCharges\"].std()\n\n(23.61410818827673, 27.120965039260753)\n\n\nThese values are more reasonable ! We can see that the most impactful features are the contract type, months, the monthly charges, internetService with Fiber optic and the SEA-1 region: 1. the longer the subscription (contract type and months) the less likely the customer churns. 2. In particular, for each additional “months std” = 23.6 months -&gt; -45% odds. 3. having a Fiber optic internet / high charges makes the customer more likely to churn 4. In particular, 26% higher odds of churning if the customer has fiber: it could perhaps more competitiveness between providers. 5. For each additional 27$ per month, the customer odds of churning increase by 23.65%. 6. Finally, being in SEA-1 increases the odds by 11%.\nWe can also try L1 regularization for sparse coefficients (feature selection):\n\nmodel = LogisticRegressionCV(Cs=np.logspace(-4, 4, 100), penalty=\"l1\", solver=\"liblinear\")\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.8429\nTest accuracy: 0.6667\n\n\n\ncoef = pd.DataFrame(model.coef_, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\ncoef = pd.DataFrame(100 * (np.exp(model.coef_) - 1), columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe model is similar to the unregularized case, we have 20 dimension with 100 observations: perhaps using feature selection is not a good idea here since the dimension is not that large compared to the number of observations: the Lasso keeps the largest coefficients and reduces the others to near 0.\nWith all these models, we can obtain the prediction probability (sigmoid) of 10 samples for e.g using:\n\nmodel.predict_proba(X_test_processed[:10])\n\narray([[0.22842486, 0.77157514],\n       [0.29932415, 0.70067585],\n       [0.22107175, 0.77892825],\n       [0.45511437, 0.54488563],\n       [0.28544249, 0.71455751],\n       [0.85739176, 0.14260824],\n       [0.67825299, 0.32174701],\n       [0.91052171, 0.08947829],\n       [0.78664728, 0.21335272],\n       [0.50537597, 0.49462403]])\n\n\nThe sigmoid probability of the model corresponds to the second column of the output above.\nIt outputs a vector of probabilites that sums to 1. We get the prediction class using the argmax or comparing the second column to 0.5:\n\nmodel.predict_proba(X_test_processed[:10]).argmax(axis=1)\n\narray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n\n\n(model.predict_proba(X_test_processed[:10])[:, 1] &gt; 0.5).astype(int)\n\narray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#bayesian-logistic-regression",
    "href": "media/teaching/SB/tp7_churn.html#bayesian-logistic-regression",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "4. Bayesian logistic regression",
    "text": "4. Bayesian logistic regression\nWe can now build a bayesian logistic regression with a Gaussian prior using pymc:\n\n4.1 Fitting the model with MCMC\n\nimport pymc as pm\nimport arviz as az\nimport seaborn as sns\n\n# Build the model\nn_mcmc_samples = 1000\ncoords = dict(var_names=column_names)\nwith pm.Model(coords=coords) as logistic_model:\n    # Priors for weights and intercept\n    sigma = pm.HalfCauchy('sigma', beta=1)\n    intercept = pm.Normal('intercept', mu=0, sigma=sigma)\n    betas = pm.Normal('betas', mu=0, sigma=sigma, shape=X_train_processed.shape[1])\n    \n    # Linear predictor\n    mu = pm.math.dot(X_train_processed, betas) + intercept\n    \n    # Likelihood (observed outcome)\n    theta = pm.math.sigmoid(mu)\n    y_obs = pm.Bernoulli('y_obs', p=theta, observed=y_train)\n    \n    # Sample from posterior\n    trace = pm.sample(n_mcmc_samples, tune=1000, return_inferencedata=True)\naz.summary(trace)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, intercept, betas]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n-0.288\n0.438\n-1.101\n0.542\n0.008\n0.007\n2776.0\n2395.0\n1.0\n\n\nbetas[0]\n0.345\n0.487\n-0.564\n1.270\n0.008\n0.007\n4078.0\n2611.0\n1.0\n\n\nbetas[1]\n-0.544\n0.733\n-1.884\n0.875\n0.012\n0.011\n3709.0\n2799.0\n1.0\n\n\nbetas[2]\n-0.066\n0.457\n-0.935\n0.788\n0.007\n0.007\n4514.0\n2947.0\n1.0\n\n\nbetas[3]\n-0.838\n0.554\n-1.978\n0.110\n0.009\n0.007\n3821.0\n3307.0\n1.0\n\n\nbetas[4]\n-0.381\n0.634\n-1.627\n0.768\n0.009\n0.009\n4762.0\n3000.0\n1.0\n\n\nbetas[5]\n1.041\n0.618\n-0.081\n2.221\n0.014\n0.010\n2181.0\n2661.0\n1.0\n\n\nbetas[6]\n-0.527\n0.751\n-1.927\n0.908\n0.012\n0.011\n3877.0\n2795.0\n1.0\n\n\nbetas[7]\n-0.058\n0.688\n-1.452\n1.156\n0.010\n0.011\n4470.0\n2692.0\n1.0\n\n\nbetas[8]\n-0.581\n0.590\n-1.675\n0.533\n0.010\n0.008\n3326.0\n3316.0\n1.0\n\n\nbetas[9]\n-0.242\n0.628\n-1.407\n0.969\n0.009\n0.010\n5203.0\n2794.0\n1.0\n\n\nbetas[10]\n0.250\n0.626\n-0.921\n1.442\n0.008\n0.011\n6243.0\n2790.0\n1.0\n\n\nbetas[11]\n-0.174\n0.596\n-1.304\n0.934\n0.008\n0.009\n5396.0\n3216.0\n1.0\n\n\nbetas[12]\n-0.254\n0.556\n-1.341\n0.760\n0.008\n0.008\n4633.0\n2792.0\n1.0\n\n\nbetas[13]\n-0.555\n0.563\n-1.572\n0.517\n0.010\n0.008\n3570.0\n2551.0\n1.0\n\n\nbetas[14]\n0.069\n0.605\n-1.032\n1.308\n0.009\n0.010\n4284.0\n2719.0\n1.0\n\n\nbetas[15]\n0.511\n0.619\n-0.663\n1.683\n0.009\n0.009\n4406.0\n2800.0\n1.0\n\n\nbetas[16]\n-0.596\n0.588\n-1.736\n0.443\n0.010\n0.008\n3500.0\n2826.0\n1.0\n\n\nbetas[17]\n0.978\n0.645\n-0.129\n2.278\n0.013\n0.009\n2661.0\n3099.0\n1.0\n\n\nbetas[18]\n0.215\n0.381\n-0.521\n0.927\n0.007\n0.005\n3335.0\n2299.0\n1.0\n\n\nbetas[19]\n-1.345\n0.328\n-1.953\n-0.729\n0.006\n0.004\n2985.0\n2600.0\n1.0\n\n\nsigma\n0.828\n0.234\n0.439\n1.262\n0.007\n0.005\n1155.0\n1620.0\n1.0\n\n\n\n\n\n\n\nNo warnings, all rhats are equal to 1, the ESS are all very large, no red flags of divergences. the MCMC chains pass all diagnostics. To intepret the coefficients, we should check the HDI of the coefficients, if they contain 0 it means that 0 is included in the 94% credible interval: therefore the coefficient is not statistically different from 0:\n\naz.plot_forest(trace)\nplt.grid()\nplt.vlines(0, 0, ymax=100, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\nNone of them are except sigma and beta[19] which corresponds to the last variable “Months”\n\ncolumn_names[19]\n\n'num__Months'\n\n\n\n100 * (np.exp(-1.352) - 1)\n\n-74.127770174036\n\n\n\n\n4.2 Making predictions\nHow do we make predictions with this model ? Well, we can use the MCMC samples (beta) to compute the sigmoid probabilities on the test data:\n\nbeta_samples = trace.posterior[\"betas\"].values.reshape(-1, 20)\nbeta_samples.shape\n\n(4000, 20)\n\n\n\nintercept_samples = trace.posterior[\"intercept\"].values.reshape(-1, 1)\nintercept_samples.shape\n\n(4000, 1)\n\n\n\nfrom scipy.special import expit as sigmoid\n\ndef get_bayes_probas(X, trace):\n    beta_samples = trace.posterior[\"betas\"].values.reshape(-1, 20)\n    # vector of size 4000 x 20\n    intercept_samples = trace.posterior[\"intercept\"].values.reshape(1, -1)\n    # vector of size 4000 x 1\n\n    # X test is of size n_samples x 20 so we transpose beta_samples to have a size 20 x 4000\n    # then we transpose the output to be 4000 x n_samples compatible with intercept_samples of size 1 x 4000\n    logits = X.dot(beta_samples.T) + intercept_samples\n    # we have a vector of size n_samples x 4000\n    return sigmoid(logits)\n\nprobas_bayes_train = get_bayes_probas(X_train_processed, trace)\nprobas_bayes_test = get_bayes_probas(X_test_processed, trace)\n\nprobas_bayes_train.shape, probas_bayes_test.shape\n\n((140, 4000), (60, 4000))\n\n\nWe have 4000 different predictions for each of the 60 test samples, we can compute the average prediction and the standard deviation to evaluate our uncertainty:\n\nmean_proba_bayes_train = probas_bayes_train.mean(axis=1)\nstd_proba_bayes_train = probas_bayes_train.std(axis=1)\n\nmean_proba_bayes_test = probas_bayes_test.mean(axis=1)\nstd_proba_bayes_test = probas_bayes_test.std(axis=1)\n\nbayes_predictions_train = (mean_proba_bayes_train &gt; 0.5).astype(int)\nbayes_predictions_test = (mean_proba_bayes_test &gt; 0.5).astype(int)\n\nprint(f\"Training accuracy: {accuracy_score(y_train, bayes_predictions_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test, bayes_predictions_test):.4f}\")\n\nTraining accuracy: 0.8071\nTest accuracy: 0.6833\n\n\nIt seems like the performance is similar or even a bit worse than the frequentist approach (Ridge). Why go through the trouble of MCMC then ? Well, because we can compute also uncertainties around those mean predictions of the MCMC samples. For example, with the frequentist approach we would get the the probability of churn is 0.8. With the bayesian approach we have 4000 probabilities of churn for each sample, assume their mean is identical: 0.8. With the 4000 MCMC samples we can also compute an HDI of those probabilities. If the HDI is too large say [0.3, 1.] then we cannot say for sure that 0.8 is statistically significant. If however the HDI is [0.7, 0.9] (it is far from 0.5) then we are more confident in our prediction.\nIn practice, the companies does not want to have many false positives (predict churn for customers who are actually satisfied and won’t leave) because it costs money (ads, promo deals to retain them…). So it might use the bayesian approach to only target the customers with predicted churn and high certainty. For the predicted churns with low certainty it may send them a satisfaction survey to be more certain."
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#how-i-manipulated-the-data",
    "href": "media/teaching/SB/tp7_churn.html#how-i-manipulated-the-data",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "5. How I manipulated the data",
    "text": "5. How I manipulated the data\nTo illustrate the regularization here I truncated data to only 200 samples (from 10K samples in the kaggle dataset) and kept only a few variables otherwise MCMC would be too slow. And I also added fake variables: the region variable is purely random, completely unrelated to the churn variable. Yet, the regression (and Lasso) found a large coefficient for one of the regions ! This is to illustrate how models with little data can learn noise and lead to wrong intepretations of the coefficients: L2 regularization (and the bayesian approach however correctly reduced their amplitudes).\nLet’s remove the region variable and see what happens. Before, we obtained with the unregularized model using the Regions:\n\nTraining accuracy: 0.8357\nTest accuracy: 0.6500\n\n\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\"]\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\n\ncategorical_transformer = OneHotEncoder(drop=\"first\", sparse_output=False)\nnumeric_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features),\n    ],\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\nX_train_processed = preprocessor.fit_transform(X_train)\n\nX_test_processed = preprocessor.transform(X_test)\n\n# we get the column names:\ncolumn_names = preprocessor.get_feature_names_out()\n\n\n# we can fit the logistic regression model with no regularization:\nmodel = LogisticRegression(penalty=None)\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.8214\nTest accuracy: 0.6833\n\n\nSlightly less train accuracy, more test accuracy: the model’s overfitting is reduced a little bit."
  },
  {
    "objectID": "data/SB/hierarchical_model.html",
    "href": "data/SB/hierarchical_model.html",
    "title": "\nTP3: Modèles Bayésiens Hiérarchiques (I)\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Modèles Bayésiens Hiérarchiques (I)\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats\n\n\n# Installation si nécessaire\n!pip install pymc arviz numpy pandas matplotlib seaborn ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\n# Configuration pour de meilleurs graphiques\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\n1. Modèle Poisson-Gamma hiérarchique\nDans le cadre de la modélisation du nombre de sinistres, il n’est pas pratique de considérer un \\(\\lambda_i\\) spécifique à chaque individu car les données individuelles contiennent très souvent très peu d’observations. Ici, on souhaite donc regrouper les assurés en utilisant leurs informations individuelles. Nous avons une variable age qui donne l’âge du conducteur en 4 catégories:\n\nage = 0 (&lt; 30 ans)\nage = 1 (entre 30 et 50 ans)\nage = 2 (entre 50 et 60 ans)\nage = 3 (supérieur à 60 ans)\n\nPour tenir compte des différences entre les catégories, on modélise chaque catégorie par un taux de sinistre spécifique \\(\\textcolor{red}{\\lambda_j}\\) avec \\(j\\in \\{0, 1, 2, 3\\}\\). Pour prendre en compte leur similarité, les \\(\\textcolor{red}{\\lambda_j}\\) sont modélisés avec une loi a priori commune \\(\\text{Gamma}(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta})\\).\n\nSi des données historiques peuvent être utilisées, alors \\(\\textcolor{purple}{\\alpha}\\) et $ $ sont choisis (constantes a priori) avec la méthode des moments (comme en TD1)\nSinon, on les modélise comme des variables aléatoires avec une loi apriori \\(\\pi\\) assez vague (grande variance, ou uniforme).\n\nLe deuxième cas définit une structure bayésienne à deux niveaux: 1. Le nombre de sinistre \\(\\textcolor{blue}{N}\\) dépend de \\(\\textcolor{red}{\\lambda_j}\\): \\(\\textcolor{blue}{N} | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\) 2. Le taux de sinistre \\(\\textcolor{red}{\\lambda_j}\\) dépend de $ $ et $ : | , (, )$ avec \\(\\textcolor{purple}{\\alpha}\\) et \\(\\textcolor{purple}{\\beta}\\) et \\(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta} \\sim \\pi\\).\nC’est un modèle hiérarchique. On considère une loi a priori Uniforme(0, 10).\nVoici les données (extraites et filtrées à partir de https://www.kaggle.com/datasets/saisatish09/insuranceclaimsdata?select=dataCar.csv)\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_age.csv\", index_col=0)\nprint(df.shape)\ndf.head()\n\nNous avons donc les données de 10205 assurés. On peut commencer par voir la taille de chaque groupe:\n\ndf.age.value_counts()\n\n\nQuestion 1: On remarque que la catégorie d’âge 1 (30-40 ans) est la plus grande avec 4610 assurés. Celle des &gt; 60 ans est la plus petite avec 1400 assurés. À quoi peut-on s’attendre concernant la qualité de l’estimation de chaque \\(\\textcolor{red}{\\lambda_j}\\) ?\nOn regarde la distribution du nombre de sinistre déclarés par assuré:\n\ndf.numclaims.value_counts()\n\n\n\nQuestion 2: On remarque que plus de 90% des assurés ne déclarent jamais de sinistres. Seulement 50/10250 ont déclaré 2 ou 3 sinistres. Quel est l’ordre de grandeur (ou fourchette de valeurs) des \\(\\textcolor{red}{\\lambda_j}\\) auquel on peut s’attendre ?\n\n\nQuestion 3: On note les nombres de sinistres de chaque groupe d’âge \\(j\\) par \\(\\textcolor{blue}{N}_1^j, \\dots, \\textcolor{blue}{N}_{n_j}^j | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\). Ainsi, d’après la distribution ci-dessus des catégories d’âge: \\(n_0 = 2377\\), \\(n_1 = 4610\\) etc… Représentez le graphe probabiliste de ce modèle et déterminez la formule de la loi a posteriori jointe en fonction des lois de l’énoncé.\n\n\nQuestion 4: Implémentez le modèle hiérarchique avec pymc et faites le diagnostic MCMC\n\n\nQuestion 5: Calculez les bonnes probabilités de type \\(\\mathbb P(\\textcolor{red}{\\lambda_j} &lt; \\textcolor{red}{\\lambda_k})\\) pour déterminer si certains groupes d’âge ont des risques différents ou non. Commenter\n\n\n\n2. Bayesian Poisson regression\nEn plus de l’âge du conducteur, nous avons également la catégorie d’âge du véhicule (veh_age) et la valeur du véhicule en ‘$’ (veh_value) (divisée par 10000). Une structure bayésienne hiérarchique n’est plus possible (à moins de diviser la variable veh_value en catégories et créer tous les croisements de catégories age x veh_age x veh_value possibles chacune avec son taux \\(\\textcolor{red}{\\lambda_j}\\)). Une meilleur approche est de considérer une regression linéaire où on prédit le taux de sinistre avec une combinaison linéaire des variables: \\(\\textcolor{red}{\\lambda} = \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\). Or \\(\\textcolor{red}{\\lambda} &gt; 0\\), ce qui n’est pas respecté ici. On utilise un modèle linéaire généralisé où c’est \\(\\log(\\textcolor{red}{\\lambda})\\) qui est expliquée:\n\\[ N | \\textcolor{red}{\\lambda} \\sim \\mathcal P(\\textcolor{red}{\\lambda})\\] \\[\\log(\\textcolor{red}{\\lambda}) =  \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\]\nAvec une loi a priori \\(\\textcolor{blue}{\\beta_0}, \\textcolor{blue}{\\beta_1}, \\textcolor{blue}{\\beta_2}, \\textcolor{blue}{\\beta_3} \\sim \\mathcal{N}(0, 1)\\).\n\nQuestion 6: On suppose que \\(\\textcolor{blue}{\\beta_1} = 0.2\\). Si toutes les variables sauf l’âge ne changent pas, quel est l’effet de passer à une catégorie d’âge supérieur (càd que l’âge passe de 0 à 1, ou 1 à 2 ou 2 à 3) sur \\(\\textcolor{red}{\\lambda}\\) ? Répondez à la question en terme de pourcentage de changement.\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_reg.csv\", index_col=0)\ndf.head()\n\n\n\nQuestion 7: Complétez le modèle bayésien ci-dessous et faites le diagonostic de convergence. En pratique, on définit \\(\\textcolor{red}{\\lambda}\\) comme l’exp de la combinaison linéaire.\n\ncoords = dict(var_name=[\"intercept\", \"age\", \"veh_age\", \"veh_value\"]) # dictionnaire qui sert à nommer les variables\nwith pm.Model(coords=coords) as reg_model:\n    # beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"var_name\") # vecteur des betas de taille 4 nommé selon \"var_name\" de coords\n    # TO DO\n    #\n    #\n    # lambda_ =\n    lambda_ = pm.Gamma(\"lambda\", 0.1, 0.1)\n    N = pm.Poisson(\"N\", mu=lambda_, observed=df[\"numclaims\"])\n    trace = pm.sample()\n\n\n\nQuestion 8: Interprétez les valeurs et HDI obtenus pour chaque coefficient de regression \\(\\beta_i\\).\n\naz.summary(trace)\n\n\n\nQuestion 9: On souhaite vérifier que le modèle fit bien les données. Pour cela on peut utiliser les échantillons MCMC (\\(\\beta\\)) pour générer des données \\(N_i\\) (pm.sample_posterior_predictive) et comparer la vraisemblance avec la distribution des données générées. Qu’en pensez-vous ?\n\nwith reg_model:\n    pm.sample_posterior_predictive(trace, extend_inferencedata=True) # ce paramètre = True fait qu'on modifie l'objet `trace` en rajoutant les samples de la predictive posterior\naz.plot_ppc(trace)\n\n\n\nQuestion 10 Now time to break it ! Pour bien cerner ce qui explique le bon fit des données de la question précédente, réduisez la complexité du modèle (le simplifier en enlevant des variables) jusqu’à ce que la fonction prédictive s’éloigne des données. Que peut-on en déduire ?\n\n\nQuestion 11: Vus les résultats obtenus, il se peut que certains coefficients soient biaisés par le choix restrictif de l’a priori Gaussien avec variance égale à 1. On considère à présent \\(\\sigma\\) comme une variable aléatoire avec un a priori gaussienne positive (tronquée, pm.HalfNormal) avec un hyperparamètre \\(\\sigma = 1\\). Implémentez ce modèle. les résultats ont-ils changé considérablement ? Interpréter.\n\n\nQuestion 12: On peut évaluer la qualité de ces modèles (et choisir le meilleur) avec le critère bayésien LOO (Leave-one-out).\nLOO consiste à évaluer la log-vraisemblance de prédiction d’un échantillon \\(i\\) après l’avoir enlevé des données, autrement dit, si on note \\(N_1, \\dots, N_n\\) les observations, alors \\(N_{-i}\\) représente toutes les données sauf \\(N_i\\), on note donc \\(N_{-i} = \\{N_1, \\dots, N_{i-1}, N_{i+1}, \\dots N_n\\}\\). La fonction de prédiction (en log-probabilité) pour des données nouvelles est dit “expected log predictive density (ELPD)”: \\[ ELPD_i = \\log p(N_i | N_{-i}) \\] Le critère LOO évalue la log-vraisemblance pour tous les échantillons (qui sont enlevés et prédits avec le reste à tour de rôle): \\[  ELPD = \\sum_{i=1}^n ELPD_i = \\sum_{i=1}^n \\log p(N_i | N_{-i}) \\] Avec la loi des probabilités totale: \\[ p(N_i | N_{-i}) = \\int p(N_i | \\beta, N_{-i}) p(\\beta | N_{-i}) \\mathrm d\\beta = \\int p(N_i | \\beta) p(\\beta | N_{-i}) \\mathrm d\\beta \\]\nCette intégrale est approchée par Importance sampling (IS) avec la loi a posteriori full \\(p(\\beta | N_1, \\dots N_n)\\) et une approximation des poids IS avec la loi de Pareto généralisée qui doit avoir des moments finies sinon IS est instable. Sa variance est finie si son paramètre (scale) \\(k &lt; 0.5\\). Sa moyenne est finie si \\(k &lt; 1\\). arviz nous donne l’estimation du ELPD ainsi que la qualité de l’estimation (k pour chaque \\(i\\)). Il faut d’abord calculer les loglikelihoods avec pm:\n\nwith reg_model:\n    pm.compute_log_likelihood(trace)\naz.plot_loo(trace)\n\nPour comparer tous les modèles vus dans ce notebook, on peut utiliser la fonction az.compare qui prend en argument un dictionnaire avec des noms des modèles en keys et les objets models en valeurs. Complétez ce code avec vos modèles et analysez, arviz trie les modèles par défaut du meilleur au pire:\n\nmodels_dict = {\"reg_model\": reg_model, ...}\naz.compare(models_dict)"
  },
  {
    "objectID": "teaching_sb.html",
    "href": "teaching_sb.html",
    "title": "Statistiques Bayésiennes",
    "section": "",
    "text": "Slides\n\nChapter 1: Bayesian thinking and bayesian models | pdf | browser\nChapter 2: Markov-Chain Monte-Carlo algorithms | pdf | browser\nChapter 3: Hierarchical models and Bayesian machine learning | pdf | browser\n\n\n\nTD/TP\n\nTD1: Bayesian models (pdf)\nTP2: MCMC diagnostics (pdf)\nTP3: Intro to PyMC (view notebook) | (download notebook)\nTP4: Hierarchical models (I): bayesian regression (view notebook) | (download notebook)\nTD5: Hierarchical models (II): Capture & recapture (pdf) (python code)\nTD6: Bayesian ML: logistic regression (pdf) (solutions pdf)\nTP7: Bayesian ML for churn prediction (view notebook) | (download notebook)\n\n\n\nTest yourself\n\nGeneral anonymous quiz\n\n\n\nFeedback\nIf you took this course, please give your anonymous feedback to improve it: google form."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Publications\nThis is list is rarely maintained, for an up-to-date list check my scholar profile.\n\nConference papers\n\nH. Janati, B. Muzellec, G. Peyré and M. Cuturi. Entropic Optimal Transport between (Unbalanced) Gaussian Measures has a Closed Form (Neurips, 2020). (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Debiased Sinkhorn barycenters (ICML 2020) (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Spatio-Temporal Alignments: Optimal transport through space and time. (AISTATS, 2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Group-level EEG / MEG source imaging via Optimal Transport: minimum Wasserstein estimates. IPMI 2019. (paper) | (Python code)\nT. Bazeille, H. Richard, H. Janati, B. Thirion, Local Optimal Transport for Functional Brain Template Estimation. IPMI 2019\nH. Janati, M. Cuturi and A. Gramfort. Wasserstein regularization for sparse multitask regression. AISTATS 2019. (paper) | (Python code)\n\n\n\nJournal papers\n\nJ. Faouzi, H. Janati. pyts: A Python package for time series classification. Journal of Machine Learning Research (2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Multi-subject MEG/EEG source imaging with sparse multi-task regression (NeuroImage, 2019) (paper)\n\n\n\nOther\n\nH. Janati, Investigating cancer resistance in a Glioblastoma cell line with gene expression data. Statistics [stat]. 2016. [Best (paper) award - ENSAE 2015/2016] (paper)\n\n\n\n\nReviewing service\n\nNeurIPS (2019, 2020, 2021, 2022, 2023)\nICML (2020, 2021, 2022, 2023, 2024)\nNeuroImage (2019, 2020)\nIEEE TNNLS (2020)\nIEEE SPL (2020)"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "INSEA\n\nStatistiques Bayésiennes (2025)\n\n\n\nÉcole Polytechnique\n\nPython for data science (Master DS for business X/HEC 2021)\nStatistical learning theory (Master data science 2022, 2023)\nOptimization algorithms for machine learning (Master data science 2023)\n\n\n\nNew York University - (Paris)\n\nIntroduction to machine learning (2022, 2023)\n\n\n\nTélécom Paris\n\nAdvanced Statistics (Cycle ingénieur 2022, 2023, 2024)\nNumerical calculus and Monte-Carlo methods (Cycle ingénieur 2022, 2023, 2024)\nAdvanced machine learning (Master spécialisé Big Data AI 2022, 2023)\nIntroduction to Machine learning (Executive education 2022, 2023, 2024)\n\n\n\nENSAE / Sorbonne Université [Teaching assistant during PhD]\n\nOptimisation différentiable (2017, 2018, 2019) - ENSAE 1A\nProbability theory (2018) - ENSAE 1A\nMonte-Carlo methods (2017, 2018, 2019) - ENSAE 2A\nProgrammation en C - La Sorbonne Université (2017)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "INSEA\nInstitut National de Statistique et d’Économie Appliquée (INSEA)\nB.P. 6217, Madinat Al Irfane,\nRabat, Morocco\nhjanati@insea.ac.ma\n\n\nTélécom Paris\n19 Place Marguerite Perey,\n91120 Palaiseau, France\nhicham.janati@telecom-paris.fr\nYou’re a student and want to share feedback anonymously ? Here’s a google form you can fill."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hicham Janati",
    "section": "",
    "text": "I’m an associate professor at Insea (Rabat) and Télécom Paris. My research and interests gravitate around machine learning with a focus on optimal transport and diffusion models. I’m passionate about exploring ideas deeply and sharing them clearly. This site [in construction] is where I document and share what I’m working on.\n\n\n\nShort Bio\nIn 2021, I obtained both my Master’s and PhD degrees from ENSAE Paris, where I worked under the supervision of Alexandre Gramfort and Marco Cuturi on optimal transport and its applications in machine learning. My thesis manuscript is available here. Following my PhD, I held a postdoctoral position at École Polytechnique, working with Rémi Flamary on deep learning for domain adaptation. I have been an Associate Professor at Télécom Paris since December 2021, and I joined INSEA in the same role in January 2025.\n\n\nNews\n\n\n\nDate\nEvent\n\n\n\n\nFeb 2025\nSpatio-Temporal Alignments accepted in Journal of Machine Learning Research (JMLR)\n\n\nJan 2025\nJoined Insea as Associate Professor\n\n\nJun 2023\nUnbalanced Co-Optimal Transport accepted at AAAI Conference\n\n\nDec 2021\nJoined Télécom Paris as Associate Professor\n\n\nApr 2021\nStarted postdoc at École Polytechnique in Rémi Flamary’s lab\n\n\nMar 2021\nDefended PhD — manuscript\n\n\nDec 2020\nPresented OT Closed-Form for Gaussians at NeurIPS 2020 (Orals, Virtual)\n\n\nJul 2020\nPresented Debiased Sinkhorn Barycenters at ICML 2020 (Spotlight, Virtual)\n\n\nApr 2020\nPresented Spatio-Temporal OT at AISTATS 2020 (Virtual)\n\n\nJun 2019\nPresented Local OT for Brain Template Estimation at IPMI 2019 (Hong Kong)\n\n\nApr 2019\nPresented Wasserstein Regularization at AISTATS 2019 (Okinawa, Japan)"
  },
  {
    "objectID": "media/teaching/SB/td3_intro_pymc.html",
    "href": "media/teaching/SB/td3_intro_pymc.html",
    "title": "\nTP3: Introduction à PyMC\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Introduction à PyMC\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats et comparer avec les statistiques fréquentistes\n\n\n%pip install pymc arviz numpy pandas matplotlib ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport arviz as az\n\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\nModèle Poisson-Gamma simple\nOn considère l’ex 1 du TD1. On observe le nombre de sinistres par année \\(N | \\lambda \\sim \\mathcal{P}(\\lambda)\\), où \\(\\lambda\\) suit une loi a priori \\(\\text{Gamma}(a, b)\\). On suppose que les données historiques mènent au choix a = 4 et b = 2. On utilise la définition de Gamma où b correspond au “rate” et non pas au scale (comme en TD), la moyenne de cette Gamma est a/b. (voir wikipedia).\nOn suppose que les 5 observations sont données par:\n\ndata = np.array([4, 0, 2, 1, 0])\nrng = np.random.default_rng(42)\nlambda_true = 2\ndata = rng.poisson(lambda_true, size=5)\n\nOn définit le modèle bayésien dans un contexte avec pymc ou on précise la distribution, le nom et les paramètres de chaque variable\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Gamma(\"lambda\", a, b) # non observée\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\nOn voit que pymc a automatiquement choisi NUTS et a simulé 4 chaînes avec 2000 échantillons chacune dont 1000 jetés (tuning / burn-in). Voyons ce que contient l’objet trace:\n\ntrace\n\nC’est un objet InferenceData du package arviz. On peut obtenir les échantillons simulés dans l’attribut posterior:\n\ntrace.posterior[\"lambda\"].data\n\n\ntrace.posterior[\"lambda\"].data.shape\n\nOn a effectivement généré 4 chaines avec 1000 échantillons chacune. On peut les visualiser:\n\nplt.figure()\nplt.plot(trace.posterior[\"lambda\"].data.T)\nplt.grid(True)\nplt.title(\"Trace plot\")\nplt.show()\n\nOu utiliser la librairie arviz directement qui donne également une estimation de la densité a posteriori:\n\naz.plot_trace(trace)\n\nOn fait le diagnostic de convergence:\n\naz.plot_autocorr(trace)\nplt.show()\n\n\naz.summary(trace)\n\n\naz.summary(trace)\n\n\nLes tracés sont bien mélangés\nRhat = 1.0 &lt; 1.01 : pas de différence significative entre les 4 chaines\nESS très larges\nAutocorrélations diminuent très rapidement\n\nCette chaîne réussit les diagnostics de convergence.\nOn peut visualiser la densité a posteriori avec l’intervalle de crédibilité HDI:\n\naz.plot_posterior(trace, hdi_prob=0.94)\n\nLes bornes de cet intervalle sont également présente dans le tableau du summary ci-dessus. On peut calcule un HDI directement:\n\naz.hdi(trace, hdi_prob=0.94).to_pandas()\n\nOn peut calculer un ESS relatif (divisé par le nombre d’échantillons):\n\naz.ess(trace, method=\"bulk\", relative=True).to_pandas()\n\nAinsi, on en déduit que 44% des échantillons “sont efficaces” pour estimer “le centre” (bulk) de la distribution\n\nQuestion 1: Augmenter le nombre de sinistres observés de 5 à 10 puis 100, comment changent les statistiques du az.summary ?\n\n\nQuestion 2: Les métriques de convergence sont-elle très différentes ? Est-ce surprenant ?\n\n\nQuestion 3: Comment peut-on interpréter le HDI obtenu ?\n\n\nQuestion 4: En utilisant le fait que la loi a priori soit conjuguée, générez des échantillons a posteriori directement (sans pymc) et comparez\nLa loi a posteriori est \\(\\text{Gamma}(a + \\sum_{i=1}^n N_i, b + n)\\)\n\nlambda_post_samples = rng.gamma(a + data.sum(), 1/(b + len(data)), size=(4, 1000))\n\nax = az.plot_posterior(trace)\nax.hist(lambda_post_samples, bins=100, density=True, alpha=0.7)\nplt.show()\n\n\ntrace_iid = az.convert_to_inference_data(dict(iid=lambda_post_samples))\naz.summary(trace_iid)\n\n\nlambda_post_samples.mean()\n\n\naz.summary(trace)\n\n\n\n\nModèle Poisson-Gamma à plusieurs conducteurs\nOn considère désormais les données de plusieurs individus avec un \\(\\lambda_i\\) différent mais un a priori commun. Chaque conducteur \\(i\\) a ses données \\(N_i^1, \\dots, N_i^m\\). Le modèle pymc s’adapte facilement en changeant le shape des paramètres:\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Gamma(\"lambda\", a, b, shape=n_drivers) # non observée, on précise le nombre de lambda\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n    pm.compute_log_likelihood(trace)\n\nL’objet trace contient désormais plusieurs variables lambda:\n\nprint(az.summary(trace))\n\n\nQuestion 4: Comparez l’estimation fréquentiste avec l’estimation bayésienne avec (a, b) = (4, 2) puis (a, b) = (10, 1).\n\ndata.mean(0)\n\n\n\nQuestion 5: Déterminez un intervalle de confiance fréquentiste de niveau 95% pour chaque \\(\\lambda_i\\).\n\\([\\bar{N} \\pm \\frac{Q_{97.5}\\hat{\\sigma}}{\\sqrt{n}}]\\) avec \\(Q_{97.5}\\) le quantile de la loi de student à n-1 degrés de liberté.\n\nfrom scipy.stats import t\n\nn = 5\nq975 = t.ppf(0.975, df=n-1)\nsigma = data.std(0)\n\ndata.mean(0) - q975 * sigma / n ** 0.5, data.mean(0) + q975 * sigma / n ** 0.5, \n\n\naz.summary(trace)\n\n\n\nQuestion 6: On reprend à présent une loi a priori Uniforme([0, 5]). Déterminez des HDI de niveau 95% pour chaque \\(\\lambda_i\\). Comment se comparent-ils aux intervalles de confiance fréquentistes ?\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Uniform(\"lambda\", 0, 5, shape=n_drivers) # non observée, on précise le nombre de lambda\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model_unif:\n    # lambda_ = pm.Gamma(\"lambda\", a, b, shape=n_drivers) # non observée, on précise le nombre de lambda\n    lambda_ = pm.Uniform(\"lambda\", 0, 100, shape=n_drivers)\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace_unif = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\n\naz.summary(trace_unif)\n\n\ndata.mean(0) - q975 * sigma / n ** 0.5, data.mean(0) + q975 * sigma / n ** 0.5,"
  },
  {
    "objectID": "media/teaching/SB/td4_hierarchical_model.html",
    "href": "media/teaching/SB/td4_hierarchical_model.html",
    "title": "\nTP3: Modèles Bayésiens Hiérarchiques (I)\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Modèles Bayésiens Hiérarchiques (I)\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats\n\n\n# Installation si nécessaire\n!pip install pymc arviz numpy pandas matplotlib seaborn ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\n# Configuration pour de meilleurs graphiques\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\n1. Modèle Poisson-Gamma hiérarchique\nDans le cadre de la modélisation du nombre de sinistres, il n’est pas pratique de considérer un \\(\\lambda_i\\) spécifique à chaque individu car les données individuelles contiennent très souvent très peu d’observations. Ici, on souhaite donc regrouper les assurés en utilisant leurs informations individuelles. Nous avons une variable age qui donne l’âge du conducteur en 4 catégories:\n\nage = 0 (&lt; 30 ans)\nage = 1 (entre 30 et 50 ans)\nage = 2 (entre 50 et 60 ans)\nage = 3 (supérieur à 60 ans)\n\nPour tenir compte des différences entre les catégories, on modélise chaque catégorie par un taux de sinistre spécifique \\(\\textcolor{red}{\\lambda_j}\\) avec \\(j\\in \\{0, 1, 2, 3\\}\\). Pour prendre en compte leur similarité, les \\(\\textcolor{red}{\\lambda_j}\\) sont modélisés avec une loi a priori commune \\(\\text{Gamma}(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta})\\).\n\nSi des données historiques peuvent être utilisées, alors \\(\\textcolor{purple}{\\alpha}\\) et $ $ sont choisis (constantes a priori) avec la méthode des moments (comme en TD1)\nSinon, on les modélise comme des variables aléatoires avec une loi apriori \\(\\pi\\) assez vague (grande variance, ou uniforme).\n\nLe deuxième cas définit une structure bayésienne à deux niveaux: 1. Le nombre de sinistre \\(\\textcolor{blue}{N}\\) dépend de \\(\\textcolor{red}{\\lambda_j}\\): \\(\\textcolor{blue}{N} | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\) 2. Le taux de sinistre \\(\\textcolor{red}{\\lambda_j}\\) dépend de $ $ et $ : | , (, )$ avec \\(\\textcolor{purple}{\\alpha}\\) et \\(\\textcolor{purple}{\\beta}\\) et \\(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta} \\sim \\pi\\).\nC’est un modèle hiérarchique. On considère une loi a priori Uniforme(0, 10).\nVoici les données (extraites et filtrées à partir de https://www.kaggle.com/datasets/saisatish09/insuranceclaimsdata?select=dataCar.csv)\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_age.csv\", index_col=0)\nprint(df.shape)\ndf.head()\n\nNous avons donc les données de 10205 assurés. On peut commencer par voir la taille de chaque groupe:\n\ndf.age.value_counts()\n\n\nQuestion 1: On remarque que la catégorie d’âge 1 (30-40 ans) est la plus grande avec 4610 assurés. Celle des &gt; 60 ans est la plus petite avec 1400 assurés. À quoi peut-on s’attendre concernant la qualité de l’estimation de chaque \\(\\textcolor{red}{\\lambda_j}\\) ?\nOn regarde la distribution du nombre de sinistre déclarés par assuré:\n\ndf.numclaims.value_counts()\n\n\n\nQuestion 2: On remarque que plus de 90% des assurés ne déclarent jamais de sinistres. Seulement 50/10250 ont déclaré 2 ou 3 sinistres. Quel est l’ordre de grandeur (ou fourchette de valeurs) des \\(\\textcolor{red}{\\lambda_j}\\) auquel on peut s’attendre ?\n\n\nQuestion 3: On note les nombres de sinistres de chaque groupe d’âge \\(j\\) par \\(\\textcolor{blue}{N}_1^j, \\dots, \\textcolor{blue}{N}_{n_j}^j | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\). Ainsi, d’après la distribution ci-dessus des catégories d’âge: \\(n_0 = 2377\\), \\(n_1 = 4610\\) etc… Représentez le graphe probabiliste de ce modèle et déterminez la formule de la loi a posteriori jointe en fonction des lois de l’énoncé.\n\n\nQuestion 4: Implémentez le modèle hiérarchique avec pymc et faites le diagnostic MCMC\n\n\nQuestion 5: Calculez les bonnes probabilités de type \\(\\mathbb P(\\textcolor{red}{\\lambda_j} &lt; \\textcolor{red}{\\lambda_k})\\) pour déterminer si certains groupes d’âge ont des risques différents ou non. Commenter\n\n\n\n2. Bayesian Poisson regression\nEn plus de l’âge du conducteur, nous avons également la catégorie d’âge du véhicule (veh_age) et la valeur du véhicule en ‘$’ (veh_value) (divisée par 10000). Une structure bayésienne hiérarchique n’est plus possible (à moins de diviser la variable veh_value en catégories et créer tous les croisements de catégories age x veh_age x veh_value possibles chacune avec son taux \\(\\textcolor{red}{\\lambda_j}\\)). Une meilleur approche est de considérer une regression linéaire où on prédit le taux de sinistre avec une combinaison linéaire des variables: \\(\\textcolor{red}{\\lambda} = \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\). Or \\(\\textcolor{red}{\\lambda} &gt; 0\\), ce qui n’est pas respecté ici. On utilise un modèle linéaire généralisé où c’est \\(\\log(\\textcolor{red}{\\lambda})\\) qui est expliquée:\n\\[ N | \\textcolor{red}{\\lambda} \\sim \\mathcal P(\\textcolor{red}{\\lambda})\\] \\[\\log(\\textcolor{red}{\\lambda}) =  \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\]\nAvec une loi a priori \\(\\textcolor{blue}{\\beta_0}, \\textcolor{blue}{\\beta_1}, \\textcolor{blue}{\\beta_2}, \\textcolor{blue}{\\beta_3} \\sim \\mathcal{N}(0, 1)\\).\n\nQuestion 6: On suppose que \\(\\textcolor{blue}{\\beta_1} = 0.2\\). Si toutes les variables sauf l’âge ne changent pas, quel est l’effet de passer à une catégorie d’âge supérieur (càd que l’âge passe de 0 à 1, ou 1 à 2 ou 2 à 3) sur \\(\\textcolor{red}{\\lambda}\\) ? Répondez à la question en terme de pourcentage de changement.\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_reg.csv\", index_col=0)\ndf.head()\n\n\n\nQuestion 7: Complétez le modèle bayésien ci-dessous et faites le diagonostic de convergence. En pratique, on définit \\(\\textcolor{red}{\\lambda}\\) comme l’exp de la combinaison linéaire.\n\ncoords = dict(var_name=[\"intercept\", \"age\", \"veh_age\", \"veh_value\"]) # dictionnaire qui sert à nommer les variables\nwith pm.Model(coords=coords) as reg_model:\n    # beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"var_name\") # vecteur des betas de taille 4 nommé selon \"var_name\" de coords\n    # TO DO\n    #\n    #\n    # lambda_ =\n    lambda_ = pm.Gamma(\"lambda\", 0.1, 0.1)\n    N = pm.Poisson(\"N\", mu=lambda_, observed=df[\"numclaims\"])\n    trace = pm.sample()\n\n\n\nQuestion 8: Interprétez les valeurs et HDI obtenus pour chaque coefficient de regression \\(\\beta_i\\).\n\naz.summary(trace)\n\n\n\nQuestion 9: On souhaite vérifier que le modèle fit bien les données. Pour cela on peut utiliser les échantillons MCMC (\\(\\beta\\)) pour générer des données \\(N_i\\) (pm.sample_posterior_predictive) et comparer la vraisemblance avec la distribution des données générées. Qu’en pensez-vous ?\n\nwith reg_model:\n    pm.sample_posterior_predictive(trace, extend_inferencedata=True) # ce paramètre = True fait qu'on modifie l'objet `trace` en rajoutant les samples de la predictive posterior\naz.plot_ppc(trace)\n\n\n\nQuestion 10 Now time to break it ! Pour bien cerner ce qui explique le bon fit des données de la question précédente, réduisez la complexité du modèle (le simplifier en enlevant des variables) jusqu’à ce que la fonction prédictive s’éloigne des données. Que peut-on en déduire ?\n\n\nQuestion 11: Vus les résultats obtenus, il se peut que certains coefficients soient biaisés par le choix restrictif de l’a priori Gaussien avec variance égale à 1. On considère à présent \\(\\sigma\\) comme une variable aléatoire avec un a priori gaussienne positive (tronquée, pm.HalfNormal) avec un hyperparamètre \\(\\sigma = 1\\). Implémentez ce modèle. les résultats ont-ils changé considérablement ? Interpréter.\n\n\nQuestion 12: On peut évaluer la qualité de ces modèles (et choisir le meilleur) avec le critère bayésien LOO (Leave-one-out).\nLOO consiste à évaluer la log-vraisemblance de prédiction d’un échantillon \\(i\\) après l’avoir enlevé des données, autrement dit, si on note \\(N_1, \\dots, N_n\\) les observations, alors \\(N_{-i}\\) représente toutes les données sauf \\(N_i\\), on note donc \\(N_{-i} = \\{N_1, \\dots, N_{i-1}, N_{i+1}, \\dots N_n\\}\\). La fonction de prédiction (en log-probabilité) pour des données nouvelles est dit “expected log predictive density (ELPD)”: \\[ ELPD_i = \\log p(N_i | N_{-i}) \\] Le critère LOO évalue la log-vraisemblance pour tous les échantillons (qui sont enlevés et prédits avec le reste à tour de rôle): \\[  ELPD = \\sum_{i=1}^n ELPD_i = \\sum_{i=1}^n \\log p(N_i | N_{-i}) \\] Avec la loi des probabilités totale: \\[ p(N_i | N_{-i}) = \\int p(N_i | \\beta, N_{-i}) p(\\beta | N_{-i}) \\mathrm d\\beta = \\int p(N_i | \\beta) p(\\beta | N_{-i}) \\mathrm d\\beta \\]\nCette intégrale est approchée par Importance sampling (IS) avec la loi a posteriori full \\(p(\\beta | N_1, \\dots N_n)\\) et une approximation des poids IS avec la loi de Pareto généralisée qui doit avoir des moments finies sinon IS est instable. Sa variance est finie si son paramètre (scale) \\(k &lt; 0.5\\). Sa moyenne est finie si \\(k &lt; 1\\). arviz nous donne l’estimation du ELPD ainsi que la qualité de l’estimation (k pour chaque \\(i\\)). Il faut d’abord calculer les loglikelihoods avec pm:\n\nwith reg_model:\n    pm.compute_log_likelihood(trace)\naz.plot_loo(trace)\n\nPour comparer tous les modèles vus dans ce notebook, on peut utiliser la fonction az.compare qui prend en argument un dictionnaire avec des noms des modèles en keys et les objets models en valeurs. Complétez ce code avec vos modèles et analysez, arviz trie les modèles par défaut du meilleur au pire:\n\nmodels_dict = {\"reg_model\": reg_model, ...}\naz.compare(models_dict)"
  },
  {
    "objectID": "media/Opti/tpnewtoncorr.html",
    "href": "media/Opti/tpnewtoncorr.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/Opti/tpnewtoncorr.html#méthode-de-newton",
    "href": "media/Opti/tpnewtoncorr.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Comme il s’agit d’un problème sans contraintes, on peut appliquer la méthode de Newton à \\(\\nabla g\\) pour résoudre \\(\\nabla g(x) = 0\\). (A priori, on converge donc vers un point critique de \\(g\\).) \n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad.\n  Pour h dans un voisinage de x, on développe \\(g(x + h) = g(x) + \\langle A^{\\top}(Ax - b) + 2\\gamma x, h\\rangle + \\frac{1}{2}h^{\\top}(A^{\\top}A + 2 \\gamma I_n) h\\). Ainsi, par identification de la partie linéaire en h: \\(\\nabla g(x) = A^{\\top}(Ax - b) + 2\\gamma x\\) Et comme \\(A^{\\top}A + 2 \\gamma I_n\\) est symmétrique: \\(\\nabla^2 g(x) = A^{\\top}A + 2 \\gamma I_n\\) \n\n\nQuestion 4\nLorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).   Il suffit d’écrire la condition d’optimalité \\(\\nabla g(x) = 0\\) qui donne exactement l’itération de Newton. \n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n  Au lieu d’inverser la hessienne, on résout un système linéaire, ce qui est 3 fois moins couteux.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    g = A.T.dot(A.dot(x) - b) + 2 * gamma * x\n    return g\n\ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n    n = len(x)\n    h = A.T.dot(A) + 2 * gamma * np.identity(n)\n    return h\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n0.00022141735630660996\n\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = x + np.linalg.solve(H, - d)\n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n  Iteration   |     g(x_k)    | max|gradient(x_k)|\n      0       | 3570.345200187844 | 291.7910294967238\n      1       | 1.0704068547572962 | 1.0200174038743626e-13\n      2       | 1.0704068547572965 | 7.507883204027621e-15\n      3       | 1.070406854757296 | 6.106226635438361e-15\n      4       | 1.0704068547572962 | 4.884981308350689e-15\n      5       | 1.070406854757296 | 4.810388198883686e-15\n      6       | 1.0704068547572962 | 6.895525817007808e-15\n      7       | 1.070406854757296 | 5.412337245047638e-15\n      8       | 1.070406854757296 | 5.224987109642143e-15\n      9       | 1.070406854757296 | 5.662137425588298e-15\n\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n  Notons la fonction objective par \\(g\\) et les contraintes linéaires par \\(h(x) = Cx - d = 0\\). Remearquez ici que h regroupe toutes les contraintes linéaires données par \\(\\langle C_i, x\\rangle - d_i = 0\\) où l’indice i dénote la ième ligne. Notons chacune de ces contraintes par \\(h_i, i=1\\dots p\\)\n\nexistence: Par continuité de h, l’ensemble des contraintes est un fermé. g est continue et clairement coercive, le minimum donc existe.\nconvexité: Comme \\(\\gamma &gt; 0\\), on a pour tout \\(x,h  \\in \\mathbb{R}^n\\): \\[ h^{\\top} \\nabla^2 g(x)h = h^{\\top}(A^{\\top}A + 2\\gamma I_n) = \\|Ah\\|^2 + 2 \\gamma \\|h\\|^2 &gt; 0\\] La hessienne de g est définie positive, donc g est strictivement convexe. La solution est donc unique.\nKKT: Les contraintes sont linéaires, elles sont donc qualifiées sur K, donc toute solution du problème vérifie KKT. Par convexité (+ qualification), KKT est aussi une condition suffisante, donc toute solution de KKT est un minimum. Par unicité, La solution de KKT est la solution du problème. Notons la \\((x, \\mu) \\in \\mathbb{R}^n \\times \\mathbb{R}^p\\) :\n$ {\n\\[\\begin{array}{l}\n            \\nabla g(x) + \\sum_{i=1}^p \\mu_i\\nabla h_i(x) = 0 \\\\\n            h(x) = 0\n         \\end{array}\\]\n. $\n\ni.e $ {\n\\[\\begin{array}{l}\n                A^{\\top}(Ax - b) + 2\\gamma x + C^{\\top}\\mu = 0 \\\\\n                Cx - d = 0\n            \\end{array}\\]\n. $ \n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n  Le problème est équivalent à son système KKT, on peut donc résoudre KKT avec la méthode de Newton appliquée à F ci-dessous pour résoudre \\(F(x, \\mu) = 0\\): \\[ F(x, \\mu) = \\left(\\nabla g(x) + \\mu \\nabla h(x), h(x)\\right) \\] La suite de Newton s’écrit donc: \\[(x_{k+1}, \\mu_{k+1}) = (x_{k}, \\mu_{k}) - J_F^{-1}(x_{k}, \\mu_{k}) F(x_{k}, \\mu_{k}) \\]\nOn a donc besoin d’écrire la Jacobienne de F. On a:\n$ F(x, ) = (A^{}(Ax - b) + 2x + C^{}, Cx - d) $\nOn écrit la matricienne Jacobienne de F par blocs:\n$ J_F(x, ) =\n\\[\\begin{pmatrix} A^{\\top}A + 2\\gamma I_n & C^{\\top} \\\\ C & 0 \\end{pmatrix}\\]\n$ \n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(x, mu):\n    \"\"\"Compute the function F.\"\"\"\n    # On note f1 et f2 les composantes de F\n    f1 = gradient_g(x) + C.T.dot(mu)\n    f2 = C.dot(x) - d\n    f = np.hstack((f1, f2))  # on concatene f1 et f2\n    return f\n    \ndef jac_F(x, mu):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # on crée une matrice de taille (n + p) x (n + p)\n    J = np.zeros((n + p, n + p))\n    J[:n, :n] = hessian_g(x)\n    J[:n, n:] = C.T\n    J[n:, :n] = C\n    \n    return J\n\ndef newton_constrained(xmu0, F=F, jac=jac_F, maxiter=10, verbose=True):\n    \"\"\"Solve constrained min g with newton method\"\"\"\n    \n    xmu = xmu0.copy()\n    x = xmu[:n]\n    mu = xmu[n:]\n    if verbose:\n        strings = [\"Iteration\", \"max(abs(F(x_k, mu_k)))\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        J = jac(x, mu)\n        f = F(x, mu)\n        \n        if verbose:\n            strings = [i, abs(f).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        xmu = xmu + np.linalg.solve(J, - f)\n        x = xmu[:n]\n        mu = xmu[n:]\n        \n    return x\n\n\nxmu0 = rnd.randn(n + p)\nx = newton_constrained(xmu0)\n\n  Iteration   | max(abs(F(x_k, mu_k)))\n      0       | 159.44416332283464\n      1       | 9.542019951958025e-14\n      2       | 7.212980213111564e-15\n      3       | 4.5449755070592346e-15\n      4       | 7.369105325949477e-15\n      5       | 7.271960811294775e-15\n      6       | 6.04572815421367e-15\n      7       | 6.25888230132432e-15\n      8       | 4.246603069191224e-15\n      9       | 6.300515664747763e-15\n\n\n\nC.dot(x) - d\n\narray([ 1.38777878e-16,  2.22044605e-16, -1.11022302e-16,  3.67761377e-16,\n       -2.22044605e-16])"
  }
]