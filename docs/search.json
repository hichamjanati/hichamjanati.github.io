[
  {
    "objectID": "media/Opti/tpnewton.html",
    "href": "media/Opti/tpnewton.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/Opti/tpnewton.html#méthode-de-newton",
    "href": "media/Opti/tpnewton.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad. ##### Question 4 Lorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).\n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    \ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = \n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(...):\n    \"\"\"Compute the function F.\"\"\"\n    # A faire\n    \ndef jac_F(x):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # A faire\n\ndef newton_constrained( ):\n    # A faire"
  },
  {
    "objectID": "media/teaching/Opti/tpnewton.html",
    "href": "media/teaching/Opti/tpnewton.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/teaching/Opti/tpnewton.html#méthode-de-newton",
    "href": "media/teaching/Opti/tpnewton.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad. ##### Question 4 Lorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).\n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    \ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = \n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(...):\n    \"\"\"Compute the function F.\"\"\"\n    # A faire\n    \ndef jac_F(x):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # A faire\n\ndef newton_constrained( ):\n    # A faire"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html",
    "href": "media/teaching/SB/tp7_churn.html",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "",
    "text": "Objectives:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\nseed = 42\nrng = np.random.default_rng(seed)"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#problem-statement",
    "href": "media/teaching/SB/tp7_churn.html#problem-statement",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "1. Problem Statement",
    "text": "1. Problem Statement\nIn companies that offer services (such as a mobile phone operator), customer retention is a major challenge. The churn rate refers to the percentage of customers who decide to cancel their subscription (e.g., to switch to another provider). If the company can predict which customers are likely to churn, it can take proactive steps — such as offering additional services or special deals — to retain them.\nWe will work with a real dataset from a telecom operator (filtered and adapted from: Kaggle).\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/churn.csv\", index_col=0)\ndf.head()\n\n\n\n\n\n\n\n\nDependents\nTechSupport\nContract\nInternetService\nCustomerID_Region\nMonthlyCharges\nMonths\nChurn\n\n\n\n\n0\nYes\nNo\nOne year\nFiber optic\nMIS-1\n78.95\n34.0\n0\n\n\n1\nYes\nYes\nTwo year\nDSL\nDAL-1\n85.95\n70.0\n0\n\n\n2\nNo\nYes\nTwo year\nFiber optic\nSAN-1\n104.00\n69.0\n0\n\n\n3\nNo\nNo internet service\nMonth-to-month\nNo\nHOU-1\n20.55\n5.0\n0\n\n\n4\nYes\nYes\nTwo year\nFiber optic\nHOU-1\n113.10\n72.0\n0\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['Dependents', 'TechSupport', 'Contract', 'InternetService',\n       'CustomerID_Region', 'MonthlyCharges', 'Months', 'Churn'],\n      dtype='object')\n\n\n\n# get the X and y variables\ny = df.Churn\nX = df.drop(\"Churn\", axis=1)\n\nCheck if the number of class instances we have:\n\ny.value_counts()\n\nChurn\n0    100\n1    100\nName: count, dtype: int64\n\n\nThis binary classification task is balanced: (in practice churn rates are significantly lower, churn prediction is very imbalanced in the real world. I made the problem easier here by resampling from the original data for the sake of simplicity). Let’s check the type of data variables we have:\n\nX.dtypes\n\nDependents            object\nTechSupport           object\nContract              object\nInternetService       object\nCustomerID_Region     object\nMonthlyCharges       float64\nMonths               float64\ndtype: object\n\n\n\nX.Contract.value_counts()\n\nContract\nMonth-to-month    133\nOne year           37\nTwo year           30\nName: count, dtype: int64"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#data-preprocessing",
    "href": "media/teaching/SB/tp7_churn.html#data-preprocessing",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "2. Data preprocessing",
    "text": "2. Data preprocessing\n\n2.1 One-hot encoding / dummy variables\nWe need to pre-process the data: turn the categorical variables to binary dummy variables. This is called one-hot encoding. Here is how it goes: - for a variable like Contract which takes Month-to-Month, One year or Two year (three categories) we can transform it to a 3 binary variables:\n\n\n\n…\nContract\n…\n\n\n\n\n…\nMonth-to-Month\n…\n\n\n…\nOne Year\n…\n\n\n…\nTwo Year\n…\n\n\n…\nMonth-to-Month\n…\n\n\n…\nTwo Year\n…\n\n\n…\nTwo Year\n…\n\n\n\n↓\n\n\n\n…\nMonth-to-Month\nOne Year\nTwo Year\n…\n\n\n\n\n…\n1\n0\n0\n…\n\n\n…\n0\n1\n0\n…\n\n\n…\n0\n0\n1\n…\n\n\n…\n1\n0\n0\n…\n\n\n…\n0\n0\n1\n…\n\n\n…\n0\n0\n1\n…\n\n\n\nThese binary variables are called dummy variables or the one-hot encoding of Contract. However you can notice that the sum of these columns will always be 1: the 3 binary variables are linearly dependent which is bad for linear models (particularly if no regularization is used), this is called a dummy variables trap. We should drop one of the columns to avoid it. Thus, a categorical variable with K categories is transformed into K-1 binary variables. We can do this with sklearn transformer object called OneHotEncoder:\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(drop='first')\nencoded_data = encoder.fit_transform(df[[\"Contract\"]])\nencoded_data\n\n&lt;200x2 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 67 stored elements in Compressed Sparse Row format&gt;\n\n\nThe output is a sparse matrix (CSR) which is a compressed form of storing matrices with lots of zeros: instead of storing all their entries, we only store in memory necessary information (for e.g the triplets (i, j, v) such that M[i, j] = v and v is not zero). Here the data is small, no need to used sparse matrices:\n\nencoder = OneHotEncoder(drop='first', sparse_output=False)\nencoded_data = encoder.fit_transform(X[[\"Contract\"]])\nencoded_data[:10]\n\narray([[1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 0.],\n       [0., 1.],\n       [0., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 0.],\n       [0., 0.]])\n\n\nWe can apply this to all categorical variables:\n\nencoder = OneHotEncoder(drop='first', sparse_output=False)\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\", \"CustomerID_Region\"]\nencoded_data = encoder.fit_transform(X[categorical_features])\nencoded_data[:10]\n\narray([[1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0.],\n       [1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        1., 0.],\n       [0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0.],\n       [1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.]])\n\n\nBut later we will have a regression coefficient for each one of these columns, how do we know which one belongs to which ? Well we can get their names from the encoder:\n\nencoder.get_feature_names_out(categorical_features)\n\narray(['Dependents_Yes', 'TechSupport_No internet service',\n       'TechSupport_Yes', 'Contract_One year', 'Contract_Two year',\n       'InternetService_Fiber optic', 'InternetService_No',\n       'CustomerID_Region_CHI-1', 'CustomerID_Region_DAL-1',\n       'CustomerID_Region_HOU-1', 'CustomerID_Region_LAX-1',\n       'CustomerID_Region_MIA-1', 'CustomerID_Region_MIS-1',\n       'CustomerID_Region_NYC-1', 'CustomerID_Region_PHL-1',\n       'CustomerID_Region_PHX-1', 'CustomerID_Region_SAN-1',\n       'CustomerID_Region_SEA-1'], dtype=object)\n\n\n\n\n2.2 Scaling numerical features\nWe also have continuous variables (numerical features): Months and MonthlyCharges. As explained in the last class, it’s important for them to have the same scale (order of magnitude) in a linear model. Scaling a variable means centering it and dividing by its standard deviation:\n\nvariable = X[\"Months\"]\nvariable = variable - variable.mean()\nvariable = variable / variable.std()\nvariable.head()\n\n0    0.326288\n1    1.850800\n2    1.808453\n3   -0.901791\n4    1.935495\nName: Months, dtype: float64\n\n\nIt is tedious to do this for each variable, the best way to do it is to use a sklearn transformer called StandardScaler:\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\nscaled_data = scaler.fit_transform(X[numeric_features])\nscaled_data[:10]\n\narray([[ 0.37846895,  0.32710679],\n       [ 0.63721955,  1.85544479],\n       [ 1.30442645,  1.81299095],\n       [-1.78025031, -0.90405438],\n       [ 1.64080222,  1.94035245],\n       [ 0.56698725, -1.03141588],\n       [ 1.37835519,  0.58182979],\n       [ 1.04937229,  0.66673745],\n       [ 0.19734354, -0.77669288],\n       [ 0.79062169, -0.73423905]])\n\n\n\n\n2.3 Merging both in one transformer:\nWe can handle both categorical and numerical features in one ColumnTransformer object:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\", \"CustomerID_Region\"]\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\n\ncategorical_transformer = OneHotEncoder(drop=\"first\", sparse_output=False)\nnumeric_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features),\n    ],\n)\n\ntransformed_data = preprocessor.fit_transform(X)\nprint(f\"The shape of the transformed data is {transformed_data.shape}\")\n\n# we construct a new pandas with the column names:\ncolumn_names = preprocessor.get_feature_names_out()\ntransformed_df = pd.DataFrame(transformed_data, columns=column_names)\ntransformed_df.head()\n\nThe shape of the transformed data is (200, 20)\n\n\n\n\n\n\n\n\n\ncat__Dependents_Yes\ncat__TechSupport_No internet service\ncat__TechSupport_Yes\ncat__Contract_One year\ncat__Contract_Two year\ncat__InternetService_Fiber optic\ncat__InternetService_No\ncat__CustomerID_Region_CHI-1\ncat__CustomerID_Region_DAL-1\ncat__CustomerID_Region_HOU-1\ncat__CustomerID_Region_LAX-1\ncat__CustomerID_Region_MIA-1\ncat__CustomerID_Region_MIS-1\ncat__CustomerID_Region_NYC-1\ncat__CustomerID_Region_PHL-1\ncat__CustomerID_Region_PHX-1\ncat__CustomerID_Region_SAN-1\ncat__CustomerID_Region_SEA-1\nnum__MonthlyCharges\nnum__Months\n\n\n\n\n0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.378469\n0.327107\n\n\n1\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.637220\n1.855445\n\n\n2\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.304426\n1.812991\n\n\n3\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.780250\n-0.904054\n\n\n4\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.640802\n1.940352\n\n\n\n\n\n\n\n\n\n2.4 Preprocessing and the train-test split\nBut when developing a machine learing model, we always follow the train-test paradigm where we split train-test data: therefore when transforming / encoding / scaling the variables we should only be using the train data. Otherwise info from the test data will be used by the model: for example, the scaling operation will use test samples to compute the mean and std. Therefore, the column transformer object should be fit on the train only, then we use the transform method on the test data:\n\nfrom sklearn.model_selection import train_test_split\n\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\", \"CustomerID_Region\"]\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\n\ncategorical_transformer = OneHotEncoder(drop=\"first\", sparse_output=False)\nnumeric_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features),\n    ],\n)\n\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=42, stratify=y)\n\nX_train_processed = preprocessor.fit_transform(X_train)\n\nX_test_processed = preprocessor.transform(X_test)\n\n# we get the column names:\ncolumn_names = preprocessor.get_feature_names_out()"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#logistic-regression",
    "href": "media/teaching/SB/tp7_churn.html#logistic-regression",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "3. Logistic regression",
    "text": "3. Logistic regression\nWe can now start doing ML models. First, we use logistic regression without regularization:\n\n3.1 Without regularization\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# we can fit the logistic regression model with no regularization:\nmodel = LogisticRegression(penalty=None)\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.8357\nTest accuracy: 0.6500\n\n\nWe can see that the model isn’t quite good: 1. 83% accuracy on the training data means that the model isnt complex enough to predict labels it has already seen 2. 65% accuracy on the test data suggest a big difference between train and test: the models learns a bit of noise in the training data\nWe can extract the coefficients and visualize their values:\n\ncoef = pd.DataFrame(model.coef_, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nWe can extract the change in the odd-ratios and visualize their importance in percentages (see TD6):\n\nodds_changes = (np.exp(model.coef_) - 1) * 100\ncoef = pd.DataFrame(odds_changes, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nodds_changes\n\narray([[ 184.33978368,  -73.24588216,   10.11408019,  -69.01164118,\n         -42.3604937 , 1867.04450222,  -73.24588216,  -57.30827598,\n         -67.89164809,  -27.90134379,   87.71595551,  -47.99574828,\n         -49.09364831,  -69.35050994,   10.37215112,  190.19346862,\n         -73.42643779,  875.62186833,  -54.79865072,  -77.0485994 ]])\n\n\n\nX[\"Months\"].std(), X[\"MonthlyCharges\"].std()\n\n(23.61410818827673, 27.120965039260753)\n\n\nSome odd percentage changes are off-the-charts ! 1867% increase with Optic Fiber and 875% if the customer is in the region SEA-1 ! Given that the performance of the model is poor here and the coefficient values are very large it is very likely the model has learned noise: regularization is needed.\n\n\n3.2 With L2 / L1 regularization\nWe can now try to improve the model by adding regularization. We can use the LogisticRegressionCV class which will automatically find the best regularization parameter for us:\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\n# we can fit the logistic regression model with no regularization:\nmodel = LogisticRegressionCV(Cs=np.logspace(-3, 3, 100), penalty=\"l2\")\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.7857\nTest accuracy: 0.7167\n\n\nThe test accuracy is slightly improved. The coefficients look very similar:\n\ncoef = pd.DataFrame(model.coef_, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nodds_changes = (np.exp(model.coef_) - 1) * 100\ncoef = pd.DataFrame(odds_changes, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nX[\"Months\"].std(), X[\"MonthlyCharges\"].std()\n\n(23.61410818827673, 27.120965039260753)\n\n\nThese values are more reasonable ! We can see that the most impactful features are the contract type, months, the monthly charges, internetService with Fiber optic and the SEA-1 region: 1. the longer the subscription (contract type and months) the less likely the customer churns. 2. In particular, for each additional “months std” = 23.6 months -&gt; -45% odds. 3. having a Fiber optic internet / high charges makes the customer more likely to churn 4. In particular, 26% higher odds of churning if the customer has fiber: it could perhaps more competitiveness between providers. 5. For each additional 27$ per month, the customer odds of churning increase by 23.65%. 6. Finally, being in SEA-1 increases the odds by 11%.\nWe can also try L1 regularization for sparse coefficients (feature selection):\n\nmodel = LogisticRegressionCV(Cs=np.logspace(-4, 4, 100), penalty=\"l1\", solver=\"liblinear\")\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.8429\nTest accuracy: 0.6667\n\n\n\ncoef = pd.DataFrame(model.coef_, columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\ncoef = pd.DataFrame(100 * (np.exp(model.coef_) - 1), columns=column_names)\ncoef.T.plot(kind=\"bar\", legend=False)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThe model is similar to the unregularized case, we have 20 dimension with 100 observations: perhaps using feature selection is not a good idea here since the dimension is not that large compared to the number of observations: the Lasso keeps the largest coefficients and reduces the others to near 0.\nWith all these models, we can obtain the prediction probability (sigmoid) of 10 samples for e.g using:\n\nmodel.predict_proba(X_test_processed[:10])\n\narray([[0.22842486, 0.77157514],\n       [0.29932415, 0.70067585],\n       [0.22107175, 0.77892825],\n       [0.45511437, 0.54488563],\n       [0.28544249, 0.71455751],\n       [0.85739176, 0.14260824],\n       [0.67825299, 0.32174701],\n       [0.91052171, 0.08947829],\n       [0.78664728, 0.21335272],\n       [0.50537597, 0.49462403]])\n\n\nThe sigmoid probability of the model corresponds to the second column of the output above.\nIt outputs a vector of probabilites that sums to 1. We get the prediction class using the argmax or comparing the second column to 0.5:\n\nmodel.predict_proba(X_test_processed[:10]).argmax(axis=1)\n\narray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n\n\n(model.predict_proba(X_test_processed[:10])[:, 1] &gt; 0.5).astype(int)\n\narray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#bayesian-logistic-regression",
    "href": "media/teaching/SB/tp7_churn.html#bayesian-logistic-regression",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "4. Bayesian logistic regression",
    "text": "4. Bayesian logistic regression\nWe can now build a bayesian logistic regression with a Gaussian prior using pymc:\n\n4.1 Fitting the model with MCMC\n\nimport pymc as pm\nimport arviz as az\nimport seaborn as sns\n\n# Build the model\nn_mcmc_samples = 1000\ncoords = dict(var_names=column_names)\nwith pm.Model(coords=coords) as logistic_model:\n    # Priors for weights and intercept\n    sigma = pm.HalfCauchy('sigma', beta=1)\n    intercept = pm.Normal('intercept', mu=0, sigma=sigma)\n    betas = pm.Normal('betas', mu=0, sigma=sigma, shape=X_train_processed.shape[1])\n    \n    # Linear predictor\n    mu = pm.math.dot(X_train_processed, betas) + intercept\n    \n    # Likelihood (observed outcome)\n    theta = pm.math.sigmoid(mu)\n    y_obs = pm.Bernoulli('y_obs', p=theta, observed=y_train)\n    \n    # Sample from posterior\n    trace = pm.sample(n_mcmc_samples, tune=1000, return_inferencedata=True)\naz.summary(trace)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, intercept, betas]\n\n\n\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nintercept\n-0.288\n0.438\n-1.101\n0.542\n0.008\n0.007\n2776.0\n2395.0\n1.0\n\n\nbetas[0]\n0.345\n0.487\n-0.564\n1.270\n0.008\n0.007\n4078.0\n2611.0\n1.0\n\n\nbetas[1]\n-0.544\n0.733\n-1.884\n0.875\n0.012\n0.011\n3709.0\n2799.0\n1.0\n\n\nbetas[2]\n-0.066\n0.457\n-0.935\n0.788\n0.007\n0.007\n4514.0\n2947.0\n1.0\n\n\nbetas[3]\n-0.838\n0.554\n-1.978\n0.110\n0.009\n0.007\n3821.0\n3307.0\n1.0\n\n\nbetas[4]\n-0.381\n0.634\n-1.627\n0.768\n0.009\n0.009\n4762.0\n3000.0\n1.0\n\n\nbetas[5]\n1.041\n0.618\n-0.081\n2.221\n0.014\n0.010\n2181.0\n2661.0\n1.0\n\n\nbetas[6]\n-0.527\n0.751\n-1.927\n0.908\n0.012\n0.011\n3877.0\n2795.0\n1.0\n\n\nbetas[7]\n-0.058\n0.688\n-1.452\n1.156\n0.010\n0.011\n4470.0\n2692.0\n1.0\n\n\nbetas[8]\n-0.581\n0.590\n-1.675\n0.533\n0.010\n0.008\n3326.0\n3316.0\n1.0\n\n\nbetas[9]\n-0.242\n0.628\n-1.407\n0.969\n0.009\n0.010\n5203.0\n2794.0\n1.0\n\n\nbetas[10]\n0.250\n0.626\n-0.921\n1.442\n0.008\n0.011\n6243.0\n2790.0\n1.0\n\n\nbetas[11]\n-0.174\n0.596\n-1.304\n0.934\n0.008\n0.009\n5396.0\n3216.0\n1.0\n\n\nbetas[12]\n-0.254\n0.556\n-1.341\n0.760\n0.008\n0.008\n4633.0\n2792.0\n1.0\n\n\nbetas[13]\n-0.555\n0.563\n-1.572\n0.517\n0.010\n0.008\n3570.0\n2551.0\n1.0\n\n\nbetas[14]\n0.069\n0.605\n-1.032\n1.308\n0.009\n0.010\n4284.0\n2719.0\n1.0\n\n\nbetas[15]\n0.511\n0.619\n-0.663\n1.683\n0.009\n0.009\n4406.0\n2800.0\n1.0\n\n\nbetas[16]\n-0.596\n0.588\n-1.736\n0.443\n0.010\n0.008\n3500.0\n2826.0\n1.0\n\n\nbetas[17]\n0.978\n0.645\n-0.129\n2.278\n0.013\n0.009\n2661.0\n3099.0\n1.0\n\n\nbetas[18]\n0.215\n0.381\n-0.521\n0.927\n0.007\n0.005\n3335.0\n2299.0\n1.0\n\n\nbetas[19]\n-1.345\n0.328\n-1.953\n-0.729\n0.006\n0.004\n2985.0\n2600.0\n1.0\n\n\nsigma\n0.828\n0.234\n0.439\n1.262\n0.007\n0.005\n1155.0\n1620.0\n1.0\n\n\n\n\n\n\n\nNo warnings, all rhats are equal to 1, the ESS are all very large, no red flags of divergences. the MCMC chains pass all diagnostics. To intepret the coefficients, we should check the HDI of the coefficients, if they contain 0 it means that 0 is included in the 94% credible interval: therefore the coefficient is not statistically different from 0:\n\naz.plot_forest(trace)\nplt.grid()\nplt.vlines(0, 0, ymax=100, color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\nNone of them are except sigma and beta[19] which corresponds to the last variable “Months”\n\ncolumn_names[19]\n\n'num__Months'\n\n\n\n100 * (np.exp(-1.352) - 1)\n\n-74.127770174036\n\n\n\n\n4.2 Making predictions\nHow do we make predictions with this model ? Well, we can use the MCMC samples (beta) to compute the sigmoid probabilities on the test data:\n\nbeta_samples = trace.posterior[\"betas\"].values.reshape(-1, 20)\nbeta_samples.shape\n\n(4000, 20)\n\n\n\nintercept_samples = trace.posterior[\"intercept\"].values.reshape(-1, 1)\nintercept_samples.shape\n\n(4000, 1)\n\n\n\nfrom scipy.special import expit as sigmoid\n\ndef get_bayes_probas(X, trace):\n    beta_samples = trace.posterior[\"betas\"].values.reshape(-1, 20)\n    # vector of size 4000 x 20\n    intercept_samples = trace.posterior[\"intercept\"].values.reshape(1, -1)\n    # vector of size 4000 x 1\n\n    # X test is of size n_samples x 20 so we transpose beta_samples to have a size 20 x 4000\n    # then we transpose the output to be 4000 x n_samples compatible with intercept_samples of size 1 x 4000\n    logits = X.dot(beta_samples.T) + intercept_samples\n    # we have a vector of size n_samples x 4000\n    return sigmoid(logits)\n\nprobas_bayes_train = get_bayes_probas(X_train_processed, trace)\nprobas_bayes_test = get_bayes_probas(X_test_processed, trace)\n\nprobas_bayes_train.shape, probas_bayes_test.shape\n\n((140, 4000), (60, 4000))\n\n\nWe have 4000 different predictions for each of the 60 test samples, we can compute the average prediction and the standard deviation to evaluate our uncertainty:\n\nmean_proba_bayes_train = probas_bayes_train.mean(axis=1)\nstd_proba_bayes_train = probas_bayes_train.std(axis=1)\n\nmean_proba_bayes_test = probas_bayes_test.mean(axis=1)\nstd_proba_bayes_test = probas_bayes_test.std(axis=1)\n\nbayes_predictions_train = (mean_proba_bayes_train &gt; 0.5).astype(int)\nbayes_predictions_test = (mean_proba_bayes_test &gt; 0.5).astype(int)\n\nprint(f\"Training accuracy: {accuracy_score(y_train, bayes_predictions_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test, bayes_predictions_test):.4f}\")\n\nTraining accuracy: 0.8071\nTest accuracy: 0.6833\n\n\nIt seems like the performance is similar or even a bit worse than the frequentist approach (Ridge). Why go through the trouble of MCMC then ? Well, because we can compute also uncertainties around those mean predictions of the MCMC samples. For example, with the frequentist approach we would get the the probability of churn is 0.8. With the bayesian approach we have 4000 probabilities of churn for each sample, assume their mean is identical: 0.8. With the 4000 MCMC samples we can also compute an HDI of those probabilities. If the HDI is too large say [0.3, 1.] then we cannot say for sure that 0.8 is statistically significant. If however the HDI is [0.7, 0.9] (it is far from 0.5) then we are more confident in our prediction.\nIn practice, the companies does not want to have many false positives (predict churn for customers who are actually satisfied and won’t leave) because it costs money (ads, promo deals to retain them…). So it might use the bayesian approach to only target the customers with predicted churn and high certainty. For the predicted churns with low certainty it may send them a satisfaction survey to be more certain."
  },
  {
    "objectID": "media/teaching/SB/tp7_churn.html#how-i-manipulated-the-data",
    "href": "media/teaching/SB/tp7_churn.html#how-i-manipulated-the-data",
    "title": "\nTP7: Bayesian logistic regression for churn prediction\n",
    "section": "5. How I manipulated the data",
    "text": "5. How I manipulated the data\nTo illustrate the regularization here I truncated data to only 200 samples (from 10K samples in the kaggle dataset) and kept only a few variables otherwise MCMC would be too slow. And I also added fake variables: the region variable is purely random, completely unrelated to the churn variable. Yet, the regression (and Lasso) found a large coefficient for one of the regions ! This is to illustrate how models with little data can learn noise and lead to wrong intepretations of the coefficients: L2 regularization (and the bayesian approach however correctly reduced their amplitudes).\nLet’s remove the region variable and see what happens. Before, we obtained with the unregularized model using the Regions:\n\nTraining accuracy: 0.8357\nTest accuracy: 0.6500\n\n\ncategorical_features = [\"Dependents\", \"TechSupport\", \"Contract\", \"InternetService\"]\nnumeric_features = [\"MonthlyCharges\", \"Months\"]\n\ncategorical_transformer = OneHotEncoder(drop=\"first\", sparse_output=False)\nnumeric_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, categorical_features),\n        ('num', numeric_transformer, numeric_features),\n    ],\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\nX_train_processed = preprocessor.fit_transform(X_train)\n\nX_test_processed = preprocessor.transform(X_test)\n\n# we get the column names:\ncolumn_names = preprocessor.get_feature_names_out()\n\n\n# we can fit the logistic regression model with no regularization:\nmodel = LogisticRegression(penalty=None)\nmodel.fit(X_train_processed, y_train.values)\n\ny_train_pred = model.predict(X_train_processed)\ny_test_pred = model.predict(X_test_processed)\n\n# Evaluate the model accuracy\nprint(f\"Training accuracy: {accuracy_score(y_train_pred, y_train):.4f}\")\nprint(f\"Test accuracy: {accuracy_score(y_test_pred, y_test):.4f}\")\n\nTraining accuracy: 0.8214\nTest accuracy: 0.6833\n\n\nSlightly less train accuracy, more test accuracy: the model’s overfitting is reduced a little bit."
  },
  {
    "objectID": "data/SB/hierarchical_model.html",
    "href": "data/SB/hierarchical_model.html",
    "title": "\nTP3: Modèles Bayésiens Hiérarchiques (I)\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Modèles Bayésiens Hiérarchiques (I)\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats\n\n\n# Installation si nécessaire\n!pip install pymc arviz numpy pandas matplotlib seaborn ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\n# Configuration pour de meilleurs graphiques\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\n1. Modèle Poisson-Gamma hiérarchique\nDans le cadre de la modélisation du nombre de sinistres, il n’est pas pratique de considérer un \\(\\lambda_i\\) spécifique à chaque individu car les données individuelles contiennent très souvent très peu d’observations. Ici, on souhaite donc regrouper les assurés en utilisant leurs informations individuelles. Nous avons une variable age qui donne l’âge du conducteur en 4 catégories:\n\nage = 0 (&lt; 30 ans)\nage = 1 (entre 30 et 50 ans)\nage = 2 (entre 50 et 60 ans)\nage = 3 (supérieur à 60 ans)\n\nPour tenir compte des différences entre les catégories, on modélise chaque catégorie par un taux de sinistre spécifique \\(\\textcolor{red}{\\lambda_j}\\) avec \\(j\\in \\{0, 1, 2, 3\\}\\). Pour prendre en compte leur similarité, les \\(\\textcolor{red}{\\lambda_j}\\) sont modélisés avec une loi a priori commune \\(\\text{Gamma}(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta})\\).\n\nSi des données historiques peuvent être utilisées, alors \\(\\textcolor{purple}{\\alpha}\\) et $ $ sont choisis (constantes a priori) avec la méthode des moments (comme en TD1)\nSinon, on les modélise comme des variables aléatoires avec une loi apriori \\(\\pi\\) assez vague (grande variance, ou uniforme).\n\nLe deuxième cas définit une structure bayésienne à deux niveaux: 1. Le nombre de sinistre \\(\\textcolor{blue}{N}\\) dépend de \\(\\textcolor{red}{\\lambda_j}\\): \\(\\textcolor{blue}{N} | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\) 2. Le taux de sinistre \\(\\textcolor{red}{\\lambda_j}\\) dépend de $ $ et $ : | , (, )$ avec \\(\\textcolor{purple}{\\alpha}\\) et \\(\\textcolor{purple}{\\beta}\\) et \\(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta} \\sim \\pi\\).\nC’est un modèle hiérarchique. On considère une loi a priori Uniforme(0, 10).\nVoici les données (extraites et filtrées à partir de https://www.kaggle.com/datasets/saisatish09/insuranceclaimsdata?select=dataCar.csv)\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_age.csv\", index_col=0)\nprint(df.shape)\ndf.head()\n\nNous avons donc les données de 10205 assurés. On peut commencer par voir la taille de chaque groupe:\n\ndf.age.value_counts()\n\n\nQuestion 1: On remarque que la catégorie d’âge 1 (30-40 ans) est la plus grande avec 4610 assurés. Celle des &gt; 60 ans est la plus petite avec 1400 assurés. À quoi peut-on s’attendre concernant la qualité de l’estimation de chaque \\(\\textcolor{red}{\\lambda_j}\\) ?\nOn regarde la distribution du nombre de sinistre déclarés par assuré:\n\ndf.numclaims.value_counts()\n\n\n\nQuestion 2: On remarque que plus de 90% des assurés ne déclarent jamais de sinistres. Seulement 50/10250 ont déclaré 2 ou 3 sinistres. Quel est l’ordre de grandeur (ou fourchette de valeurs) des \\(\\textcolor{red}{\\lambda_j}\\) auquel on peut s’attendre ?\n\n\nQuestion 3: On note les nombres de sinistres de chaque groupe d’âge \\(j\\) par \\(\\textcolor{blue}{N}_1^j, \\dots, \\textcolor{blue}{N}_{n_j}^j | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\). Ainsi, d’après la distribution ci-dessus des catégories d’âge: \\(n_0 = 2377\\), \\(n_1 = 4610\\) etc… Représentez le graphe probabiliste de ce modèle et déterminez la formule de la loi a posteriori jointe en fonction des lois de l’énoncé.\n\n\nQuestion 4: Implémentez le modèle hiérarchique avec pymc et faites le diagnostic MCMC\n\n\nQuestion 5: Calculez les bonnes probabilités de type \\(\\mathbb P(\\textcolor{red}{\\lambda_j} &lt; \\textcolor{red}{\\lambda_k})\\) pour déterminer si certains groupes d’âge ont des risques différents ou non. Commenter\n\n\n\n2. Bayesian Poisson regression\nEn plus de l’âge du conducteur, nous avons également la catégorie d’âge du véhicule (veh_age) et la valeur du véhicule en ‘$’ (veh_value) (divisée par 10000). Une structure bayésienne hiérarchique n’est plus possible (à moins de diviser la variable veh_value en catégories et créer tous les croisements de catégories age x veh_age x veh_value possibles chacune avec son taux \\(\\textcolor{red}{\\lambda_j}\\)). Une meilleur approche est de considérer une regression linéaire où on prédit le taux de sinistre avec une combinaison linéaire des variables: \\(\\textcolor{red}{\\lambda} = \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\). Or \\(\\textcolor{red}{\\lambda} &gt; 0\\), ce qui n’est pas respecté ici. On utilise un modèle linéaire généralisé où c’est \\(\\log(\\textcolor{red}{\\lambda})\\) qui est expliquée:\n\\[ N | \\textcolor{red}{\\lambda} \\sim \\mathcal P(\\textcolor{red}{\\lambda})\\] \\[\\log(\\textcolor{red}{\\lambda}) =  \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\]\nAvec une loi a priori \\(\\textcolor{blue}{\\beta_0}, \\textcolor{blue}{\\beta_1}, \\textcolor{blue}{\\beta_2}, \\textcolor{blue}{\\beta_3} \\sim \\mathcal{N}(0, 1)\\).\n\nQuestion 6: On suppose que \\(\\textcolor{blue}{\\beta_1} = 0.2\\). Si toutes les variables sauf l’âge ne changent pas, quel est l’effet de passer à une catégorie d’âge supérieur (càd que l’âge passe de 0 à 1, ou 1 à 2 ou 2 à 3) sur \\(\\textcolor{red}{\\lambda}\\) ? Répondez à la question en terme de pourcentage de changement.\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_reg.csv\", index_col=0)\ndf.head()\n\n\n\nQuestion 7: Complétez le modèle bayésien ci-dessous et faites le diagonostic de convergence. En pratique, on définit \\(\\textcolor{red}{\\lambda}\\) comme l’exp de la combinaison linéaire.\n\ncoords = dict(var_name=[\"intercept\", \"age\", \"veh_age\", \"veh_value\"]) # dictionnaire qui sert à nommer les variables\nwith pm.Model(coords=coords) as reg_model:\n    # beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"var_name\") # vecteur des betas de taille 4 nommé selon \"var_name\" de coords\n    # TO DO\n    #\n    #\n    # lambda_ =\n    lambda_ = pm.Gamma(\"lambda\", 0.1, 0.1)\n    N = pm.Poisson(\"N\", mu=lambda_, observed=df[\"numclaims\"])\n    trace = pm.sample()\n\n\n\nQuestion 8: Interprétez les valeurs et HDI obtenus pour chaque coefficient de regression \\(\\beta_i\\).\n\naz.summary(trace)\n\n\n\nQuestion 9: On souhaite vérifier que le modèle fit bien les données. Pour cela on peut utiliser les échantillons MCMC (\\(\\beta\\)) pour générer des données \\(N_i\\) (pm.sample_posterior_predictive) et comparer la vraisemblance avec la distribution des données générées. Qu’en pensez-vous ?\n\nwith reg_model:\n    pm.sample_posterior_predictive(trace, extend_inferencedata=True) # ce paramètre = True fait qu'on modifie l'objet `trace` en rajoutant les samples de la predictive posterior\naz.plot_ppc(trace)\n\n\n\nQuestion 10 Now time to break it ! Pour bien cerner ce qui explique le bon fit des données de la question précédente, réduisez la complexité du modèle (le simplifier en enlevant des variables) jusqu’à ce que la fonction prédictive s’éloigne des données. Que peut-on en déduire ?\n\n\nQuestion 11: Vus les résultats obtenus, il se peut que certains coefficients soient biaisés par le choix restrictif de l’a priori Gaussien avec variance égale à 1. On considère à présent \\(\\sigma\\) comme une variable aléatoire avec un a priori gaussienne positive (tronquée, pm.HalfNormal) avec un hyperparamètre \\(\\sigma = 1\\). Implémentez ce modèle. les résultats ont-ils changé considérablement ? Interpréter.\n\n\nQuestion 12: On peut évaluer la qualité de ces modèles (et choisir le meilleur) avec le critère bayésien LOO (Leave-one-out).\nLOO consiste à évaluer la log-vraisemblance de prédiction d’un échantillon \\(i\\) après l’avoir enlevé des données, autrement dit, si on note \\(N_1, \\dots, N_n\\) les observations, alors \\(N_{-i}\\) représente toutes les données sauf \\(N_i\\), on note donc \\(N_{-i} = \\{N_1, \\dots, N_{i-1}, N_{i+1}, \\dots N_n\\}\\). La fonction de prédiction (en log-probabilité) pour des données nouvelles est dit “expected log predictive density (ELPD)”: \\[ ELPD_i = \\log p(N_i | N_{-i}) \\] Le critère LOO évalue la log-vraisemblance pour tous les échantillons (qui sont enlevés et prédits avec le reste à tour de rôle): \\[  ELPD = \\sum_{i=1}^n ELPD_i = \\sum_{i=1}^n \\log p(N_i | N_{-i}) \\] Avec la loi des probabilités totale: \\[ p(N_i | N_{-i}) = \\int p(N_i | \\beta, N_{-i}) p(\\beta | N_{-i}) \\mathrm d\\beta = \\int p(N_i | \\beta) p(\\beta | N_{-i}) \\mathrm d\\beta \\]\nCette intégrale est approchée par Importance sampling (IS) avec la loi a posteriori full \\(p(\\beta | N_1, \\dots N_n)\\) et une approximation des poids IS avec la loi de Pareto généralisée qui doit avoir des moments finies sinon IS est instable. Sa variance est finie si son paramètre (scale) \\(k &lt; 0.5\\). Sa moyenne est finie si \\(k &lt; 1\\). arviz nous donne l’estimation du ELPD ainsi que la qualité de l’estimation (k pour chaque \\(i\\)). Il faut d’abord calculer les loglikelihoods avec pm:\n\nwith reg_model:\n    pm.compute_log_likelihood(trace)\naz.plot_loo(trace)\n\nPour comparer tous les modèles vus dans ce notebook, on peut utiliser la fonction az.compare qui prend en argument un dictionnaire avec des noms des modèles en keys et les objets models en valeurs. Complétez ce code avec vos modèles et analysez, arviz trie les modèles par défaut du meilleur au pire:\n\nmodels_dict = {\"reg_model\": reg_model, ...}\naz.compare(models_dict)"
  },
  {
    "objectID": "teaching_sb.html",
    "href": "teaching_sb.html",
    "title": "Statistiques Bayésiennes",
    "section": "",
    "text": "Slides\n\nChapter 1: Bayesian thinking and bayesian models | pdf | browser\nChapter 2: Markov-Chain Monte-Carlo algorithms | pdf | browser\nChapter 3: Hierarchical models and Bayesian machine learning | pdf | browser\n\n\n\nTD/TP\n\nTD1: Bayesian models (pdf)\nTP2: MCMC diagnostics (pdf)\nTP3: Intro to PyMC (view notebook) | (download notebook)\nTP4: Hierarchical models (I): bayesian regression (view notebook) | (download notebook)\nTD5: Hierarchical models (II): Capture & recapture (pdf) (python code)\nTD6: Bayesian ML: logistic regression (pdf) (solutions pdf)\nTP7: Bayesian ML for churn prediction (view notebook) | (download notebook)\n\n\n\nTest yourself\n\nGeneral anonymous quiz\n\n\n\nFeedback\nIf you took this course, please give your anonymous feedback to improve it: google form."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Publications\nThis is list is rarely maintained, for an up-to-date list check my scholar profile.\n\nConference papers\n\nH. Janati, B. Muzellec, G. Peyré and M. Cuturi. Entropic Optimal Transport between (Unbalanced) Gaussian Measures has a Closed Form (Neurips, 2020). (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Debiased Sinkhorn barycenters (ICML 2020) (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Spatio-Temporal Alignments: Optimal transport through space and time. (AISTATS, 2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Group-level EEG / MEG source imaging via Optimal Transport: minimum Wasserstein estimates. IPMI 2019. (paper) | (Python code)\nT. Bazeille, H. Richard, H. Janati, B. Thirion, Local Optimal Transport for Functional Brain Template Estimation. IPMI 2019\nH. Janati, M. Cuturi and A. Gramfort. Wasserstein regularization for sparse multitask regression. AISTATS 2019. (paper) | (Python code)\n\n\n\nJournal papers\n\nJ. Faouzi, H. Janati. pyts: A Python package for time series classification. Journal of Machine Learning Research (2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Multi-subject MEG/EEG source imaging with sparse multi-task regression (NeuroImage, 2019) (paper)\n\n\n\nOther\n\nH. Janati, Investigating cancer resistance in a Glioblastoma cell line with gene expression data. Statistics [stat]. 2016. [Best (paper) award - ENSAE 2015/2016] (paper)\n\n\n\n\nReviewing service\n\nNeurIPS (2019, 2020, 2021, 2022, 2023)\nICML (2020, 2021, 2022, 2023, 2024)\nNeuroImage (2019, 2020)\nIEEE TNNLS (2020)\nIEEE SPL (2020)"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "INSEA\n\nStatistiques Bayésiennes (2025)\nStatistiques Multivariées (2025)\nTechniques de réduction de dimension (2025)\n\n\n\nÉcole Polytechnique\n\nPython for data science (Master DS for business X/HEC 2021)\nStatistical learning theory (Master data science 2022, 2023)\nOptimization algorithms for machine learning (Master data science 2023)\n\n\n\nNew York University - (Paris)\n\nIntroduction to machine learning (2022, 2023)\n\n\n\nTélécom Paris\n\nAdvanced Statistics (Cycle ingénieur 2022, 2023, 2024)\nNumerical calculus and Monte-Carlo methods (Cycle ingénieur 2022, 2023, 2024)\nAdvanced machine learning (Master spécialisé Big Data AI 2022, 2023)\nIntroduction to Machine learning (Executive education 2022, 2023, 2024)\n\n\n\nENSAE / Sorbonne Université [Teaching assistant during PhD]\n\nOptimisation différentiable (2017, 2018, 2019) - ENSAE 1A\nProbability theory (2018) - ENSAE 1A\nMonte-Carlo methods (2017, 2018, 2019) - ENSAE 2A\nProgrammation en C - La Sorbonne Université (2017)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "INSEA\nInstitut National de Statistique et d’Économie Appliquée (INSEA)\nB.P. 6217, Madinat Al Irfane,\nRabat, Morocco\nhjanati@insea.ac.ma\n\n\nTélécom Paris\n19 Place Marguerite Perey,\n91120 Palaiseau, France\nhicham.janati@telecom-paris.fr\n\n\nAnyonymous Feedback\nConstructive feedback is always appreciated. Here’s a google form you can fill."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hicham Janati",
    "section": "",
    "text": "I’m an associate professor at Insea (Rabat) and Télécom Paris. My research and interests gravitate around machine learning with a focus on optimal transport and diffusion models. I’m passionate about exploring ideas deeply and sharing them clearly. This site [in construction] is where I document and share what I’m working on.\n\n\n\nShort Bio\nI obtained both my Master’s (2017) and PhD (2020) degrees from ENSAE Paris, where I worked under the supervision of Alexandre Gramfort and Marco Cuturi on optimal transport and its applications in machine learning. My thesis manuscript is available here. Following my PhD, I held a postdoctoral position at École Polytechnique, working with Rémi Flamary on deep learning for domain adaptation. I have been an Associate Professor at Télécom Paris since December 2021, and I joined INSEA in the same role in January 2025.\n\n\nNews\n\n\n\nDate\nEvent\n\n\n\n\nFeb 2025\nSpatio-Temporal Alignments accepted in Journal of Machine Learning Research (JMLR)\n\n\nJan 2025\nJoined Insea as Associate Professor\n\n\nJun 2023\nUnbalanced Co-Optimal Transport accepted at AAAI Conference\n\n\nDec 2021\nJoined Télécom Paris as Associate Professor\n\n\nApr 2021\nStarted postdoc at École Polytechnique in Rémi Flamary’s lab\n\n\nMar 2021\nDefended PhD — manuscript\n\n\nDec 2020\nPresented OT Closed-Form for Gaussians at NeurIPS 2020 (Orals, Virtual)\n\n\nJul 2020\nPresented Debiased Sinkhorn Barycenters at ICML 2020 (Spotlight, Virtual)\n\n\nApr 2020\nPresented Spatio-Temporal OT at AISTATS 2020 (Virtual)\n\n\nJun 2019\nPresented Local OT for Brain Template Estimation at IPMI 2019 (Hong Kong)\n\n\nApr 2019\nPresented Wasserstein Regularization at AISTATS 2019 (Okinawa, Japan)"
  },
  {
    "objectID": "media/teaching/SB/td3_intro_pymc.html",
    "href": "media/teaching/SB/td3_intro_pymc.html",
    "title": "\nTP3: Introduction à PyMC\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Introduction à PyMC\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats et comparer avec les statistiques fréquentistes\n\n\n%pip install pymc arviz numpy pandas matplotlib ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport arviz as az\n\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\nModèle Poisson-Gamma simple\nOn considère l’ex 1 du TD1. On observe le nombre de sinistres par année \\(N | \\lambda \\sim \\mathcal{P}(\\lambda)\\), où \\(\\lambda\\) suit une loi a priori \\(\\text{Gamma}(a, b)\\). On suppose que les données historiques mènent au choix a = 4 et b = 2. On utilise la définition de Gamma où b correspond au “rate” et non pas au scale (comme en TD), la moyenne de cette Gamma est a/b. (voir wikipedia).\nOn suppose que les 5 observations sont données par:\n\ndata = np.array([4, 0, 2, 1, 0])\nrng = np.random.default_rng(42)\nlambda_true = 2\ndata = rng.poisson(lambda_true, size=5)\n\nOn définit le modèle bayésien dans un contexte avec pymc ou on précise la distribution, le nom et les paramètres de chaque variable\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Gamma(\"lambda\", a, b) # non observée\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\nOn voit que pymc a automatiquement choisi NUTS et a simulé 4 chaînes avec 2000 échantillons chacune dont 1000 jetés (tuning / burn-in). Voyons ce que contient l’objet trace:\n\ntrace\n\nC’est un objet InferenceData du package arviz. On peut obtenir les échantillons simulés dans l’attribut posterior:\n\ntrace.posterior[\"lambda\"].data\n\n\ntrace.posterior[\"lambda\"].data.shape\n\nOn a effectivement généré 4 chaines avec 1000 échantillons chacune. On peut les visualiser:\n\nplt.figure()\nplt.plot(trace.posterior[\"lambda\"].data.T)\nplt.grid(True)\nplt.title(\"Trace plot\")\nplt.show()\n\nOu utiliser la librairie arviz directement qui donne également une estimation de la densité a posteriori:\n\naz.plot_trace(trace)\n\nOn fait le diagnostic de convergence:\n\naz.plot_autocorr(trace)\nplt.show()\n\n\naz.summary(trace)\n\n\naz.summary(trace)\n\n\nLes tracés sont bien mélangés\nRhat = 1.0 &lt; 1.01 : pas de différence significative entre les 4 chaines\nESS très larges\nAutocorrélations diminuent très rapidement\n\nCette chaîne réussit les diagnostics de convergence.\nOn peut visualiser la densité a posteriori avec l’intervalle de crédibilité HDI:\n\naz.plot_posterior(trace, hdi_prob=0.94)\n\nLes bornes de cet intervalle sont également présente dans le tableau du summary ci-dessus. On peut calcule un HDI directement:\n\naz.hdi(trace, hdi_prob=0.94).to_pandas()\n\nOn peut calculer un ESS relatif (divisé par le nombre d’échantillons):\n\naz.ess(trace, method=\"bulk\", relative=True).to_pandas()\n\nAinsi, on en déduit que 44% des échantillons “sont efficaces” pour estimer “le centre” (bulk) de la distribution\n\nQuestion 1: Augmenter le nombre de sinistres observés de 5 à 10 puis 100, comment changent les statistiques du az.summary ?\n\n\nQuestion 2: Les métriques de convergence sont-elle très différentes ? Est-ce surprenant ?\n\n\nQuestion 3: Comment peut-on interpréter le HDI obtenu ?\n\n\nQuestion 4: En utilisant le fait que la loi a priori soit conjuguée, générez des échantillons a posteriori directement (sans pymc) et comparez\nLa loi a posteriori est \\(\\text{Gamma}(a + \\sum_{i=1}^n N_i, b + n)\\)\n\nlambda_post_samples = rng.gamma(a + data.sum(), 1/(b + len(data)), size=(4, 1000))\n\nax = az.plot_posterior(trace)\nax.hist(lambda_post_samples, bins=100, density=True, alpha=0.7)\nplt.show()\n\n\ntrace_iid = az.convert_to_inference_data(dict(iid=lambda_post_samples))\naz.summary(trace_iid)\n\n\nlambda_post_samples.mean()\n\n\naz.summary(trace)\n\n\n\n\nModèle Poisson-Gamma à plusieurs conducteurs\nOn considère désormais les données de plusieurs individus avec un \\(\\lambda_i\\) différent mais un a priori commun. Chaque conducteur \\(i\\) a ses données \\(N_i^1, \\dots, N_i^m\\). Le modèle pymc s’adapte facilement en changeant le shape des paramètres:\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Gamma(\"lambda\", a, b, shape=n_drivers) # non observée, on précise le nombre de lambda\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n    pm.compute_log_likelihood(trace)\n\nL’objet trace contient désormais plusieurs variables lambda:\n\nprint(az.summary(trace))\n\n\nQuestion 4: Comparez l’estimation fréquentiste avec l’estimation bayésienne avec (a, b) = (4, 2) puis (a, b) = (10, 1).\n\ndata.mean(0)\n\n\n\nQuestion 5: Déterminez un intervalle de confiance fréquentiste de niveau 95% pour chaque \\(\\lambda_i\\).\n\\([\\bar{N} \\pm \\frac{Q_{97.5}\\hat{\\sigma}}{\\sqrt{n}}]\\) avec \\(Q_{97.5}\\) le quantile de la loi de student à n-1 degrés de liberté.\n\nfrom scipy.stats import t\n\nn = 5\nq975 = t.ppf(0.975, df=n-1)\nsigma = data.std(0)\n\ndata.mean(0) - q975 * sigma / n ** 0.5, data.mean(0) + q975 * sigma / n ** 0.5, \n\n\naz.summary(trace)\n\n\n\nQuestion 6: On reprend à présent une loi a priori Uniforme([0, 5]). Déterminez des HDI de niveau 95% pour chaque \\(\\lambda_i\\). Comment se comparent-ils aux intervalles de confiance fréquentistes ?\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Uniform(\"lambda\", 0, 5, shape=n_drivers) # non observée, on précise le nombre de lambda\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model_unif:\n    # lambda_ = pm.Gamma(\"lambda\", a, b, shape=n_drivers) # non observée, on précise le nombre de lambda\n    lambda_ = pm.Uniform(\"lambda\", 0, 100, shape=n_drivers)\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace_unif = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\n\naz.summary(trace_unif)\n\n\ndata.mean(0) - q975 * sigma / n ** 0.5, data.mean(0) + q975 * sigma / n ** 0.5,"
  },
  {
    "objectID": "media/teaching/SB/td4_hierarchical_model.html",
    "href": "media/teaching/SB/td4_hierarchical_model.html",
    "title": "\nTP3: Modèles Bayésiens Hiérarchiques (I)\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Modèles Bayésiens Hiérarchiques (I)\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats\n\n\n# Installation si nécessaire\n!pip install pymc arviz numpy pandas matplotlib seaborn ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\n# Configuration pour de meilleurs graphiques\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\n1. Modèle Poisson-Gamma hiérarchique\nDans le cadre de la modélisation du nombre de sinistres, il n’est pas pratique de considérer un \\(\\lambda_i\\) spécifique à chaque individu car les données individuelles contiennent très souvent très peu d’observations. Ici, on souhaite donc regrouper les assurés en utilisant leurs informations individuelles. Nous avons une variable age qui donne l’âge du conducteur en 4 catégories:\n\nage = 0 (&lt; 30 ans)\nage = 1 (entre 30 et 50 ans)\nage = 2 (entre 50 et 60 ans)\nage = 3 (supérieur à 60 ans)\n\nPour tenir compte des différences entre les catégories, on modélise chaque catégorie par un taux de sinistre spécifique \\(\\textcolor{red}{\\lambda_j}\\) avec \\(j\\in \\{0, 1, 2, 3\\}\\). Pour prendre en compte leur similarité, les \\(\\textcolor{red}{\\lambda_j}\\) sont modélisés avec une loi a priori commune \\(\\text{Gamma}(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta})\\).\n\nSi des données historiques peuvent être utilisées, alors \\(\\textcolor{purple}{\\alpha}\\) et $ $ sont choisis (constantes a priori) avec la méthode des moments (comme en TD1)\nSinon, on les modélise comme des variables aléatoires avec une loi apriori \\(\\pi\\) assez vague (grande variance, ou uniforme).\n\nLe deuxième cas définit une structure bayésienne à deux niveaux: 1. Le nombre de sinistre \\(\\textcolor{blue}{N}\\) dépend de \\(\\textcolor{red}{\\lambda_j}\\): \\(\\textcolor{blue}{N} | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\) 2. Le taux de sinistre \\(\\textcolor{red}{\\lambda_j}\\) dépend de $ $ et $ : | , (, )$ avec \\(\\textcolor{purple}{\\alpha}\\) et \\(\\textcolor{purple}{\\beta}\\) et \\(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta} \\sim \\pi\\).\nC’est un modèle hiérarchique. On considère une loi a priori Uniforme(0, 10).\nVoici les données (extraites et filtrées à partir de https://www.kaggle.com/datasets/saisatish09/insuranceclaimsdata?select=dataCar.csv)\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_age.csv\", index_col=0)\nprint(df.shape)\ndf.head()\n\nNous avons donc les données de 10205 assurés. On peut commencer par voir la taille de chaque groupe:\n\ndf.age.value_counts()\n\n\nQuestion 1: On remarque que la catégorie d’âge 1 (30-40 ans) est la plus grande avec 4610 assurés. Celle des &gt; 60 ans est la plus petite avec 1400 assurés. À quoi peut-on s’attendre concernant la qualité de l’estimation de chaque \\(\\textcolor{red}{\\lambda_j}\\) ?\nOn regarde la distribution du nombre de sinistre déclarés par assuré:\n\ndf.numclaims.value_counts()\n\n\n\nQuestion 2: On remarque que plus de 90% des assurés ne déclarent jamais de sinistres. Seulement 50/10250 ont déclaré 2 ou 3 sinistres. Quel est l’ordre de grandeur (ou fourchette de valeurs) des \\(\\textcolor{red}{\\lambda_j}\\) auquel on peut s’attendre ?\n\n\nQuestion 3: On note les nombres de sinistres de chaque groupe d’âge \\(j\\) par \\(\\textcolor{blue}{N}_1^j, \\dots, \\textcolor{blue}{N}_{n_j}^j | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\). Ainsi, d’après la distribution ci-dessus des catégories d’âge: \\(n_0 = 2377\\), \\(n_1 = 4610\\) etc… Représentez le graphe probabiliste de ce modèle et déterminez la formule de la loi a posteriori jointe en fonction des lois de l’énoncé.\n\n\nQuestion 4: Implémentez le modèle hiérarchique avec pymc et faites le diagnostic MCMC\n\n\nQuestion 5: Calculez les bonnes probabilités de type \\(\\mathbb P(\\textcolor{red}{\\lambda_j} &lt; \\textcolor{red}{\\lambda_k})\\) pour déterminer si certains groupes d’âge ont des risques différents ou non. Commenter\n\n\n\n2. Bayesian Poisson regression\nEn plus de l’âge du conducteur, nous avons également la catégorie d’âge du véhicule (veh_age) et la valeur du véhicule en ‘$’ (veh_value) (divisée par 10000). Une structure bayésienne hiérarchique n’est plus possible (à moins de diviser la variable veh_value en catégories et créer tous les croisements de catégories age x veh_age x veh_value possibles chacune avec son taux \\(\\textcolor{red}{\\lambda_j}\\)). Une meilleur approche est de considérer une regression linéaire où on prédit le taux de sinistre avec une combinaison linéaire des variables: \\(\\textcolor{red}{\\lambda} = \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\). Or \\(\\textcolor{red}{\\lambda} &gt; 0\\), ce qui n’est pas respecté ici. On utilise un modèle linéaire généralisé où c’est \\(\\log(\\textcolor{red}{\\lambda})\\) qui est expliquée:\n\\[ N | \\textcolor{red}{\\lambda} \\sim \\mathcal P(\\textcolor{red}{\\lambda})\\] \\[\\log(\\textcolor{red}{\\lambda}) =  \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\]\nAvec une loi a priori \\(\\textcolor{blue}{\\beta_0}, \\textcolor{blue}{\\beta_1}, \\textcolor{blue}{\\beta_2}, \\textcolor{blue}{\\beta_3} \\sim \\mathcal{N}(0, 1)\\).\n\nQuestion 6: On suppose que \\(\\textcolor{blue}{\\beta_1} = 0.2\\). Si toutes les variables sauf l’âge ne changent pas, quel est l’effet de passer à une catégorie d’âge supérieur (càd que l’âge passe de 0 à 1, ou 1 à 2 ou 2 à 3) sur \\(\\textcolor{red}{\\lambda}\\) ? Répondez à la question en terme de pourcentage de changement.\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_reg.csv\", index_col=0)\ndf.head()\n\n\n\nQuestion 7: Complétez le modèle bayésien ci-dessous et faites le diagonostic de convergence. En pratique, on définit \\(\\textcolor{red}{\\lambda}\\) comme l’exp de la combinaison linéaire.\n\ncoords = dict(var_name=[\"intercept\", \"age\", \"veh_age\", \"veh_value\"]) # dictionnaire qui sert à nommer les variables\nwith pm.Model(coords=coords) as reg_model:\n    # beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"var_name\") # vecteur des betas de taille 4 nommé selon \"var_name\" de coords\n    # TO DO\n    #\n    #\n    # lambda_ =\n    lambda_ = pm.Gamma(\"lambda\", 0.1, 0.1)\n    N = pm.Poisson(\"N\", mu=lambda_, observed=df[\"numclaims\"])\n    trace = pm.sample()\n\n\n\nQuestion 8: Interprétez les valeurs et HDI obtenus pour chaque coefficient de regression \\(\\beta_i\\).\n\naz.summary(trace)\n\n\n\nQuestion 9: On souhaite vérifier que le modèle fit bien les données. Pour cela on peut utiliser les échantillons MCMC (\\(\\beta\\)) pour générer des données \\(N_i\\) (pm.sample_posterior_predictive) et comparer la vraisemblance avec la distribution des données générées. Qu’en pensez-vous ?\n\nwith reg_model:\n    pm.sample_posterior_predictive(trace, extend_inferencedata=True) # ce paramètre = True fait qu'on modifie l'objet `trace` en rajoutant les samples de la predictive posterior\naz.plot_ppc(trace)\n\n\n\nQuestion 10 Now time to break it ! Pour bien cerner ce qui explique le bon fit des données de la question précédente, réduisez la complexité du modèle (le simplifier en enlevant des variables) jusqu’à ce que la fonction prédictive s’éloigne des données. Que peut-on en déduire ?\n\n\nQuestion 11: Vus les résultats obtenus, il se peut que certains coefficients soient biaisés par le choix restrictif de l’a priori Gaussien avec variance égale à 1. On considère à présent \\(\\sigma\\) comme une variable aléatoire avec un a priori gaussienne positive (tronquée, pm.HalfNormal) avec un hyperparamètre \\(\\sigma = 1\\). Implémentez ce modèle. les résultats ont-ils changé considérablement ? Interpréter.\n\n\nQuestion 12: On peut évaluer la qualité de ces modèles (et choisir le meilleur) avec le critère bayésien LOO (Leave-one-out).\nLOO consiste à évaluer la log-vraisemblance de prédiction d’un échantillon \\(i\\) après l’avoir enlevé des données, autrement dit, si on note \\(N_1, \\dots, N_n\\) les observations, alors \\(N_{-i}\\) représente toutes les données sauf \\(N_i\\), on note donc \\(N_{-i} = \\{N_1, \\dots, N_{i-1}, N_{i+1}, \\dots N_n\\}\\). La fonction de prédiction (en log-probabilité) pour des données nouvelles est dit “expected log predictive density (ELPD)”: \\[ ELPD_i = \\log p(N_i | N_{-i}) \\] Le critère LOO évalue la log-vraisemblance pour tous les échantillons (qui sont enlevés et prédits avec le reste à tour de rôle): \\[  ELPD = \\sum_{i=1}^n ELPD_i = \\sum_{i=1}^n \\log p(N_i | N_{-i}) \\] Avec la loi des probabilités totale: \\[ p(N_i | N_{-i}) = \\int p(N_i | \\beta, N_{-i}) p(\\beta | N_{-i}) \\mathrm d\\beta = \\int p(N_i | \\beta) p(\\beta | N_{-i}) \\mathrm d\\beta \\]\nCette intégrale est approchée par Importance sampling (IS) avec la loi a posteriori full \\(p(\\beta | N_1, \\dots N_n)\\) et une approximation des poids IS avec la loi de Pareto généralisée qui doit avoir des moments finies sinon IS est instable. Sa variance est finie si son paramètre (scale) \\(k &lt; 0.5\\). Sa moyenne est finie si \\(k &lt; 1\\). arviz nous donne l’estimation du ELPD ainsi que la qualité de l’estimation (k pour chaque \\(i\\)). Il faut d’abord calculer les loglikelihoods avec pm:\n\nwith reg_model:\n    pm.compute_log_likelihood(trace)\naz.plot_loo(trace)\n\nPour comparer tous les modèles vus dans ce notebook, on peut utiliser la fonction az.compare qui prend en argument un dictionnaire avec des noms des modèles en keys et les objets models en valeurs. Complétez ce code avec vos modèles et analysez, arviz trie les modèles par défaut du meilleur au pire:\n\nmodels_dict = {\"reg_model\": reg_model, ...}\naz.compare(models_dict)"
  },
  {
    "objectID": "media/Opti/tpnewtoncorr.html",
    "href": "media/Opti/tpnewtoncorr.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/Opti/tpnewtoncorr.html#méthode-de-newton",
    "href": "media/Opti/tpnewtoncorr.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Comme il s’agit d’un problème sans contraintes, on peut appliquer la méthode de Newton à \\(\\nabla g\\) pour résoudre \\(\\nabla g(x) = 0\\). (A priori, on converge donc vers un point critique de \\(g\\).) \n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad.\n  Pour h dans un voisinage de x, on développe \\(g(x + h) = g(x) + \\langle A^{\\top}(Ax - b) + 2\\gamma x, h\\rangle + \\frac{1}{2}h^{\\top}(A^{\\top}A + 2 \\gamma I_n) h\\). Ainsi, par identification de la partie linéaire en h: \\(\\nabla g(x) = A^{\\top}(Ax - b) + 2\\gamma x\\) Et comme \\(A^{\\top}A + 2 \\gamma I_n\\) est symmétrique: \\(\\nabla^2 g(x) = A^{\\top}A + 2 \\gamma I_n\\) \n\n\nQuestion 4\nLorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).   Il suffit d’écrire la condition d’optimalité \\(\\nabla g(x) = 0\\) qui donne exactement l’itération de Newton. \n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n  Au lieu d’inverser la hessienne, on résout un système linéaire, ce qui est 3 fois moins couteux.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    g = A.T.dot(A.dot(x) - b) + 2 * gamma * x\n    return g\n\ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n    n = len(x)\n    h = A.T.dot(A) + 2 * gamma * np.identity(n)\n    return h\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n0.00022141735630660996\n\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = x + np.linalg.solve(H, - d)\n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n  Iteration   |     g(x_k)    | max|gradient(x_k)|\n      0       | 3570.345200187844 | 291.7910294967238\n      1       | 1.0704068547572962 | 1.0200174038743626e-13\n      2       | 1.0704068547572965 | 7.507883204027621e-15\n      3       | 1.070406854757296 | 6.106226635438361e-15\n      4       | 1.0704068547572962 | 4.884981308350689e-15\n      5       | 1.070406854757296 | 4.810388198883686e-15\n      6       | 1.0704068547572962 | 6.895525817007808e-15\n      7       | 1.070406854757296 | 5.412337245047638e-15\n      8       | 1.070406854757296 | 5.224987109642143e-15\n      9       | 1.070406854757296 | 5.662137425588298e-15\n\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n  Notons la fonction objective par \\(g\\) et les contraintes linéaires par \\(h(x) = Cx - d = 0\\). Remearquez ici que h regroupe toutes les contraintes linéaires données par \\(\\langle C_i, x\\rangle - d_i = 0\\) où l’indice i dénote la ième ligne. Notons chacune de ces contraintes par \\(h_i, i=1\\dots p\\)\n\nexistence: Par continuité de h, l’ensemble des contraintes est un fermé. g est continue et clairement coercive, le minimum donc existe.\nconvexité: Comme \\(\\gamma &gt; 0\\), on a pour tout \\(x,h  \\in \\mathbb{R}^n\\): \\[ h^{\\top} \\nabla^2 g(x)h = h^{\\top}(A^{\\top}A + 2\\gamma I_n) = \\|Ah\\|^2 + 2 \\gamma \\|h\\|^2 &gt; 0\\] La hessienne de g est définie positive, donc g est strictivement convexe. La solution est donc unique.\nKKT: Les contraintes sont linéaires, elles sont donc qualifiées sur K, donc toute solution du problème vérifie KKT. Par convexité (+ qualification), KKT est aussi une condition suffisante, donc toute solution de KKT est un minimum. Par unicité, La solution de KKT est la solution du problème. Notons la \\((x, \\mu) \\in \\mathbb{R}^n \\times \\mathbb{R}^p\\) :\n$ {\n\\[\\begin{array}{l}\n            \\nabla g(x) + \\sum_{i=1}^p \\mu_i\\nabla h_i(x) = 0 \\\\\n            h(x) = 0\n         \\end{array}\\]\n. $\n\ni.e $ {\n\\[\\begin{array}{l}\n                A^{\\top}(Ax - b) + 2\\gamma x + C^{\\top}\\mu = 0 \\\\\n                Cx - d = 0\n            \\end{array}\\]\n. $ \n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n  Le problème est équivalent à son système KKT, on peut donc résoudre KKT avec la méthode de Newton appliquée à F ci-dessous pour résoudre \\(F(x, \\mu) = 0\\): \\[ F(x, \\mu) = \\left(\\nabla g(x) + \\mu \\nabla h(x), h(x)\\right) \\] La suite de Newton s’écrit donc: \\[(x_{k+1}, \\mu_{k+1}) = (x_{k}, \\mu_{k}) - J_F^{-1}(x_{k}, \\mu_{k}) F(x_{k}, \\mu_{k}) \\]\nOn a donc besoin d’écrire la Jacobienne de F. On a:\n$ F(x, ) = (A^{}(Ax - b) + 2x + C^{}, Cx - d) $\nOn écrit la matricienne Jacobienne de F par blocs:\n$ J_F(x, ) =\n\\[\\begin{pmatrix} A^{\\top}A + 2\\gamma I_n & C^{\\top} \\\\ C & 0 \\end{pmatrix}\\]\n$ \n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(x, mu):\n    \"\"\"Compute the function F.\"\"\"\n    # On note f1 et f2 les composantes de F\n    f1 = gradient_g(x) + C.T.dot(mu)\n    f2 = C.dot(x) - d\n    f = np.hstack((f1, f2))  # on concatene f1 et f2\n    return f\n    \ndef jac_F(x, mu):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # on crée une matrice de taille (n + p) x (n + p)\n    J = np.zeros((n + p, n + p))\n    J[:n, :n] = hessian_g(x)\n    J[:n, n:] = C.T\n    J[n:, :n] = C\n    \n    return J\n\ndef newton_constrained(xmu0, F=F, jac=jac_F, maxiter=10, verbose=True):\n    \"\"\"Solve constrained min g with newton method\"\"\"\n    \n    xmu = xmu0.copy()\n    x = xmu[:n]\n    mu = xmu[n:]\n    if verbose:\n        strings = [\"Iteration\", \"max(abs(F(x_k, mu_k)))\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        J = jac(x, mu)\n        f = F(x, mu)\n        \n        if verbose:\n            strings = [i, abs(f).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        xmu = xmu + np.linalg.solve(J, - f)\n        x = xmu[:n]\n        mu = xmu[n:]\n        \n    return x\n\n\nxmu0 = rnd.randn(n + p)\nx = newton_constrained(xmu0)\n\n  Iteration   | max(abs(F(x_k, mu_k)))\n      0       | 159.44416332283464\n      1       | 9.542019951958025e-14\n      2       | 7.212980213111564e-15\n      3       | 4.5449755070592346e-15\n      4       | 7.369105325949477e-15\n      5       | 7.271960811294775e-15\n      6       | 6.04572815421367e-15\n      7       | 6.25888230132432e-15\n      8       | 4.246603069191224e-15\n      9       | 6.300515664747763e-15\n\n\n\nC.dot(x) - d\n\narray([ 1.38777878e-16,  2.22044605e-16, -1.11022302e-16,  3.67761377e-16,\n       -2.22044605e-16])"
  },
  {
    "objectID": "teaching_smv.html",
    "href": "teaching_smv.html",
    "title": "Statistiques Multivariées",
    "section": "",
    "text": "Slides\n\nCours complet pdf\n\n\n\nProjet\n\nDeadline du rendu (voir instructions dans le notebook): Dimanche 7 Décembre 2025 23:59\nDeadline pour former les groupes: Dimanche 16 Novembre 2025 23:59: spreadsheet\nnotebook + data github\n\n\n\nTest yourself\n\nAnonymous quiz 1\nAnonymous quiz 2\nAnonymous quiz 3\n\n\n\nFeedback\nIf you took this course, please give your anonymous feedback to improve it: google form."
  },
  {
    "objectID": "teaching_trd.html",
    "href": "teaching_trd.html",
    "title": "Techniques de réduction de dimension",
    "section": "",
    "text": "Slides\n\nIntroduction - Linear algebra [revisited] pdf\nPrincipal components analysis (PCA) pdf\nMDS, Isomap, t-SNE pdf\nIntro to learning representations pdf\n\n\n\nTD\n\nTD 1 - intro to PCA pdf\nTD 2 - PCA and SVD pdf\nTD 3 - MDS, Isomap, t-SNE pdf\n\n\n\nTP\n\nTP 1 – Python basics: mutability pitfalls view notebook | download notebook\nTP 2 – Scientific computing with Python and NumPy view notebook | download notebook\nTP 3 – Linear algebra and PCA with numpy view notebook | download notebook\nTP 4 – MDS, Isomap, t-SNE view notebook | download notebook\nTP 5 – Intro to learning with pytorch view notebook | download notebook\n\n\n\n\nYour feedback\nPlease take a minute to fill this google form."
  },
  {
    "objectID": "media/teaching/TRD/introduction_python.html#variables-names-and-identity",
    "href": "media/teaching/TRD/introduction_python.html#variables-names-and-identity",
    "title": "\nTP 1: Introduction to Python for data science\n",
    "section": "1 - Variables, names and identity",
    "text": "1 - Variables, names and identity\nIn Python, variable names are references to objects that live in memory. The identity of an object refers to its unique location in memory.\n\n1.1 The identity of a variable:\n\nQuestion 1: Predict the output of:\n\na = [1, 2, 3]\nb = a\n\nprint(f\"a = {a}, b = {b}\")\n\nprint(f\"a == b ? {a == b}\")\n\nThe variables a and b do not “contain” the list. Instead, they are names pointing to the same object. We can confirm this by running the id function which returns the memory location:\n\nprint(f\"id(a) = {id(a)}, id(b) = {id(b)}\")\n\n\n\n\n1.2 Comparing with == or is:\nEquality (==) checks if two objects have the same value\nIdentity (is) checks if two names refer to the same object\n\nQuestion 2: Predict the output of:\n\na = [1, 2, 3]\nb = [1, 2, 3]\n\nprint(f\"a = {a}, b = {b}\")\n\nprint(f\"a == b ? {a == b}\")\nprint(f\"a is b ? {a is b}\")\n\n\nprint(f\"id(a) = {id(a)}, id(b) = {id(b)}\")\n\n\n\n\nQuestion 3\nWhat does the following code change internally ?\n\na = 10\na = 20"
  },
  {
    "objectID": "media/teaching/TRD/introduction_python.html#mutable-immutable-types",
    "href": "media/teaching/TRD/introduction_python.html#mutable-immutable-types",
    "title": "\nTP 1: Introduction to Python for data science\n",
    "section": "1.3 Mutable / immutable types",
    "text": "1.3 Mutable / immutable types\nSome python types are immutable: once they’re created, their values cannot be changed without creating a new object: - ints, floats, booleans, strings, tuples.\nOther can be changed: - dicts, lists, set\nAnalyse what happens in these two snippets of code:\n\na = 10\na = a + 5\n\nThe operation x = x + 5 appears to change the value of x, but it actually computes 10 + 5, saves it in a new physical location and attaches the name a to it.\n\nx = [5, 4]\nx.append(1)\n\nThe operation x.append(1) is said to “modify” (mutate) the object “in-place”: it changes the value of the memory location.\nTry this with strings:\n\ns = \"something\"\ns = s.upper()\n\n\nQuestion 4: what is the output of the following cells ? analyse with id\n\na = [1, 2, 3]\nb = a\n\na.append(15)\n\nprint(f\"a = {a}, b = {b}\")\n\n\na = [1, 2, 3]\nb = a\n\na = a + [15]\n\nprint(f\"a = {a}, b = {b}\")\n\n\na = [1, 2, 3]\nb = a\na += [15]\n\nprint(f\"a = {a}, b = {b}\")\n\n\na = [2, 3, 4]\nb = a\n\nb[0] = 1000\n\nThe operation += is also said to be in-place: when manipulating lists, always be careful to what you’re actually changing !\n\n\nQuestion 5: What about slicing ?\n\na = [0, 1, 2, 3, 4, 5, 6]\nb = a[:4]\n\nb[0] = 1000\n\nLet’s say I want to create a copy b of a list a in a different physical location (so that changingb won’t affect a) how can I do it, based on the previous question ?\n\n\nQuestion 6: try these operations seen on list to tuples:\n\na = (1, 2, 3)\nb = a\na = (15, 2)\n\n\na = (1, 2, 3)\nb = a * 2\nprint(f\"a = {a}, b = {b}\")\n\nWhat if we put a mutable type inside an immutable type ?\n\na = (1, [1, 2, 3])\nb = a\na[1].append(1000)\nprint(f\"a = {a}, b = {b}\")\n\nWe want to create a copy of a list using a slice like before, but the list contains another list. Would it work ?\n\na = [1, 2, 3, [4, 5, 6]]\nb = a[:]\nb[3].append(1000)\nprint(f\"a = {a}, b = {b}\")\n\n\nThe slice copies the references of the elements of the list but NOT recursively: it will not go through the elements of the elements (if any are lists). Such a recursive copy is called a deepcopy which can be done by the package copy:\n\nimport copy\na = [1, 2, 3, [4, 5, 6]]\nb = copy.deepcopy(a)\nb[3].append(1000)\nprint(f\"a = {a}, b = {b}\")"
  },
  {
    "objectID": "media/teaching/TRD/introduction_python.html#what-happens-inside-a-function-call",
    "href": "media/teaching/TRD/introduction_python.html#what-happens-inside-a-function-call",
    "title": "\nTP 1: Introduction to Python for data science\n",
    "section": "1.4 What happens inside a function call",
    "text": "1.4 What happens inside a function call\n\nQuestion 7: Call this function with different types, does the function make a copy of the object ?\n\ndef return_self(x):\n    print(f\"inside function: x = {x}, id(x) = {id(x)}\" )\n    return x\n\n\nQuestion 8: Same for this one which manipulates an immutable int. Is the returned object the same ?\n\ndef add_one(x):\n    print(f\"inside function: x = {x}, id(x) = {id(x)}\" )\n    x = x + 1\n    print(f\"inside function after increment: x = {x}, id(x) = {id(x)}\" )\n    return x\n\na = 10\nb = add_one(a)\n\n\n\nQuestion 9: What about mutable types like lists ?\n\ndef append_one(x):\n    print(f\"inside function before append: x = {x}, id(x) = {id(x)}\" )\n    x.append(1)\n    print(f\"inside function after append: x = {x}, id(x) = {id(x)}\" )\n    return x\n\na = [1]\nb = append_one(a)\n\nThe logic we discovered earlier happens inside the function, Python never makes copies when passing arguments to functions: the references of the objects are passed as if the code is executed outside the function. The local variables are “local names” attached to existing objects.\n\n\n\nQuestion 10: What about this one which manipulates a list ?\n\ndef change_list(x):\n    x = [4, 5, 6]\n    return x\n\n\n\nQuestion 11: Explain this behavior\n\ndef change_list(x=[]):\n    x.append(1)\n    return x\n\nchange_list()\nchange_list()\nchange_list()\n\n\n\n1.5 Variables scopes\nWhen Python looks up a name, it searches in this order, the LEGB order:\n\nLocal – inside the current function (including function arguments).\nEnclosing – outer function scopes (for nested functions, function inside a function).\nGlobal – module-level names.\nBuilt-in – stuff like len, print, int, def.\n\n\n\nQuestion 12: What is the output of the following cells ?\n\ndef replace_list(lst):\n    lst = [99, 100]\n\nx = [1, 2, 3]\nreplace_list(x)\nprint(x)\n\n\nx = 10\n\ndef outer():\n    x = 20 \n\n    def inner():\n        x = 30\n        print(x)\n\n    inner()\n\nouter()"
  },
  {
    "objectID": "media/teaching/TRD/introduction_python.html#numpy-vs-built-in-python",
    "href": "media/teaching/TRD/introduction_python.html#numpy-vs-built-in-python",
    "title": "\nTP 1: Introduction to Python for data science\n",
    "section": "2.2 Numpy vs built-in Python",
    "text": "2.2 Numpy vs built-in Python\nCalculating a sum with a loop. You can “time” the operation with the magic command %%time.\n\n%%time\n\nN = 10_000_000 # underscores in whole numbers are ignored by Python\n\nnumbers = []  # create an empty list\nfor ii in range(N):\n    numbers.append(ii) # add the number to the list\ntotal = sum(numbers)\n\nprint(f\"The total is {total}\")\n\nThe %%time at the beginning of the cell above is an example of magic commands. It allows you to measure the time the processor (CPU) took to execute the entire cell. Magic commands with a single percent sign apply only to a single line.\n\n%time print(\"The total is\", sum([ii for ii in range(N)]))\n\nAlways prefer list comprehensions !\nLists in python can contain any mix of types, to run the sum above, Python needs to do type checks at each step before computing the +. Moreover, everything in Python is an object, even integers which have their own attributes and methods, making built-in types even slower. Numpy allows to circumvent this limitation by creating arrays where elements have the same type.\n\nimport numpy as np\n\nx = np.arange(5)\n\nWe can check its type:\n\nx.dtype\n\n\nx = 2 * np.ones(1).astype(int)\nx\n\n\nQuestion 14: Using x above, run the following cells, explain\n\ny = x ** 62\ny\n\n\ny = x ** 63\ny\n\nBut using built-in integers:\n\ny = 2 ** 62\ny\n\n\ny = 2 ** 63\ny\n\nBack to the sum computation. With numpy, we can run it in one line:\n\n%time print(np.arange(N).sum())\n\nAs you can see, NumPy is a lot faster than list comprehensions, and the code is much shorter. NumPy should always be preferred over Python’s native lists when working solely with numbers and matrices. NumPy vectorizes operations: instead of going through elements one by one as in a for loop, the operations are applied simultaneously in the language C.\nThe dot product between two arrays ( x ) and ( y ) of length ( n ) is defined as:\n\\[\n\\langle x, y \\rangle = x^{\\top} y = \\sum_{i=1}^n x_i y_i.\n\\]\n\n\nQuestion 15:\nComplete the following cells to compare the speed of dot products using native Python loops and the numpy.dot operation:\n\n%%time\n\nN = 10_000_000\nx = np.random.randn(N)  # creates a list of random numbers following the Gaussian distribution\ny = np.random.randn(N)\n\nresult_loops = 0\n\n## To do \n\n\n\n%%time\nresult_numpy =  ## to do \n\n\nNumpy slicing\nNumPy offers a simple way to select subsets of an array, called slicing. To obtain a slice from the 3rd to the 5th element, for example:\n\nx = np.arange(10)\nprint(\"All the array: \", x)\nprint(\"A slice: \", x[2:5])\n\nRemember that Python starts indexing at 0 and that a slice [start:end] includes start but excludes end. If start is omitted, it is set to 0, and if end is omitted, it corresponds to the last index of the array. To select the first 6 elements:\n\nx[:6]\n\nTo exclude the last element, you can use negative indices. For example:\n\nx[:-1]\n\n\nx[:-3]\n\nSlices can also have a third parameter called step. So far, we have omitted this argument, which defaults to 1. Starting from 0, we can select the even indices by using a step of 2:\n\nx[0:10:2]\n\nWe can omit the start and end arguments since they don’t do anything in this case:\n\nx[::2]\n\n\n\n\nQuestion 16:\nCreate a slice that selects the odd numbers in reverse order using a single slice:\n\n\nQuestion 17:\narray numpy array mutable ? What happens if you modify a full slice like we did with lists using x[:] ?"
  },
  {
    "objectID": "media/teaching/TRD/introduction_python.html#scientific-computing-with-numpy",
    "href": "media/teaching/TRD/introduction_python.html#scientific-computing-with-numpy",
    "title": "\nTP 1: Introduction to Python for data science\n",
    "section": "2.3 Scientific computing with Numpy",
    "text": "2.3 Scientific computing with Numpy\nThe shape of a numpy array is by default not a column and not a row: just an array. To make it a row or a column, we must add an “imaginary” axis to change its shape:\n\na = np.array([1, 2, 3])\na.shape\n\n\n\nb = a[np.newaxis, :]\nb.shape\n\n\nc = a[:, np.newaxis]\nc.shape\n\nThis is equivalent to a reshape:\n\nb = a.reshape(3, 1)\nb.shape\n\n\nQuestion 18:\nWe have two vectors \\(a\\) and \\(b\\). We would like to compute the distance matrix \\(D_{ij} = (a_i - b_j)^2\\). Implement this using built-in Python and Numpy. Which is faster ?\n\na = np.random.randn(10)\nb = np.random.randn(20)\nD = np.zeros((10, 10))\n\n\n\nQuestion 19:\nWe have two datasets \\(A\\) and \\(B\\) both of dimension 5. We would like to compute the distance matrix \\(D_{ij} = \\|A_i - B_j\\|^2\\). Implement this using built-in Python and Numpy. Look up the function np.linalg.norm.\n\n\nQuestion 20:\nGenerate a random dataset of 100 samples in 2 dimensions. Center the data, compute the empirical covariance and diagonalize with np.linalg.eig or np.linalg.eigh. Which one should you prefer ?"
  },
  {
    "objectID": "media/teaching/TRD/tp1_intro.html#variables-names-and-identity",
    "href": "media/teaching/TRD/tp1_intro.html#variables-names-and-identity",
    "title": "\nTP 1: Python basics: types, references and mutability\n",
    "section": "1 - Variables, names and identity",
    "text": "1 - Variables, names and identity\nIn Python, variable names are references to objects that live in memory. The identity of an object refers to its unique location in memory.\n\n1.1 The identity of a variable:\n\nQuestion 1: Predict the output of:\n\na = [1, 2, 3]\nb = a\n\nprint(f\"a = {a}, b = {b}\")\n\nprint(f\"a == b ? {a == b}\")\n\nThe variables a and b do not “contain” the list. Instead, they are names pointing to the same object. We can confirm this by running the id function which returns the memory location:\n\nprint(f\"id(a) = {id(a)}, id(b) = {id(b)}\")\n\n\n\n\n1.2 Comparing with == or is:\nEquality (==) checks if two objects have the same value\nIdentity (is) checks if two names refer to the same object\n\nQuestion 2: Predict the output of:\n\na = [1, 2, 3]\nb = [1, 2, 3]\n\nprint(f\"a = {a}, b = {b}\")\n\nprint(f\"a == b ? {a == b}\")\nprint(f\"a is b ? {a is b}\")\n\n\nprint(f\"id(a) = {id(a)}, id(b) = {id(b)}\")\n\n\n\n\nQuestion 3\nWhat does the following code change internally ?\n\na = 10\na = 20"
  },
  {
    "objectID": "media/teaching/TRD/tp1_intro.html#mutable-immutable-types",
    "href": "media/teaching/TRD/tp1_intro.html#mutable-immutable-types",
    "title": "\nTP 1: Python basics: types, references and mutability\n",
    "section": "1.3 Mutable / immutable types",
    "text": "1.3 Mutable / immutable types\nSome python types are immutable: once they’re created, their values cannot be changed without creating a new object: - ints, floats, booleans, strings, tuples.\nOther can be changed: - dicts, lists, set\nAnalyse what happens in these two snippets of code:\n\na = 10\na = a + 5\n\nThe operation x = x + 5 appears to change the value of x, but it actually computes 10 + 5, saves it in a new physical location and attaches the name a to it.\n\nx = [5, 4]\nx.append(1)\n\nThe operation x.append(1) is said to “modify” (mutate) the object “in-place”: it changes the value of the memory location.\nTry this with strings:\n\ns = \"something\"\ns = s.upper()\n\n\nQuestion 4: what is the output of the following cells ? analyse with id\n\na = [1, 2, 3]\nb = a\n\na.append(15)\n\nprint(f\"a = {a}, b = {b}\")\n\n\na = [1, 2, 3]\nb = a\n\na = a + [15]\n\nprint(f\"a = {a}, b = {b}\")\n\n\na = [1, 2, 3]\nb = a\na += [15]\n\nprint(f\"a = {a}, b = {b}\")\n\n\na = [2, 3, 4]\nb = a\n\nb[0] = 1000\n\nThe operation += is also said to be in-place: when manipulating lists, always be careful to what you’re actually changing !\n\n\nQuestion 5: What about slicing ?\n\na = [0, 1, 2, 3, 4, 5, 6]\nb = a[:4]\n\nb[0] = 1000\n\nLet’s say I want to create a copy b of a list a in a different physical location (so that changingb won’t affect a) how can I do it, based on the previous question ?\n\n\nQuestion 6: try these operations seen on list to tuples:\n\na = (1, 2, 3)\nb = a\na = (15, 2)\n\n\na = (1, 2, 3)\nb = a * 2\nprint(f\"a = {a}, b = {b}\")\n\nWhat if we put a mutable type inside an immutable type ?\n\na = (1, [1, 2, 3])\nb = a\na[1].append(1000)\nprint(f\"a = {a}, b = {b}\")\n\nWe want to create a copy of a list using a slice like before, but the list contains another list. Would it work ?\n\na = [1, 2, 3, [4, 5, 6]]\nb = a[:]\nb[3].append(1000)\nprint(f\"a = {a}, b = {b}\")\n\n\nThe slice copies the references of the elements of the list but NOT recursively: it will not go through the elements of the elements (if any are lists). Such a recursive copy is called a deepcopy which can be done by the package copy:\n\nimport copy\na = [1, 2, 3, [4, 5, 6]]\nb = copy.deepcopy(a)\nb[3].append(1000)\nprint(f\"a = {a}, b = {b}\")"
  },
  {
    "objectID": "media/teaching/TRD/tp1_intro.html#what-happens-inside-a-function-call",
    "href": "media/teaching/TRD/tp1_intro.html#what-happens-inside-a-function-call",
    "title": "\nTP 1: Python basics: types, references and mutability\n",
    "section": "1.4 What happens inside a function call",
    "text": "1.4 What happens inside a function call\n\nQuestion 7: Call this function with different types, does the function make a copy of the object ?\n\ndef return_self(x):\n    print(f\"inside function: x = {x}, id(x) = {id(x)}\" )\n    return x\n\n\nQuestion 8: Same for this one which manipulates an immutable int. Is the returned object the same ?\n\ndef add_one(x):\n    print(f\"inside function: x = {x}, id(x) = {id(x)}\" )\n    x = x + 1\n    print(f\"inside function after increment: x = {x}, id(x) = {id(x)}\" )\n    return x\n\na = 10\nb = add_one(a)\n\n\n\nQuestion 9: What about mutable types like lists ?\n\ndef append_one(x):\n    print(f\"inside function before append: x = {x}, id(x) = {id(x)}\" )\n    x.append(1)\n    print(f\"inside function after append: x = {x}, id(x) = {id(x)}\" )\n    return x\n\na = [1]\nb = append_one(a)\n\nThe logic we discovered earlier happens inside the function, Python never makes copies when passing arguments to functions: the references of the objects are passed as if the code is executed outside the function. The local variables are “local names” attached to existing objects.\n\n\n\nQuestion 10: What about this one which manipulates a list ?\n\ndef change_list(x):\n    x = [4, 5, 6]\n    return x\n\n\n\nQuestion 11: Explain this behavior\n\ndef change_list(x=[]):\n    x.append(1)\n    return x\n\nchange_list()\nchange_list()\nchange_list()\n\n\n\n1.5 Variables scopes\nWhen Python looks up a name, it searches in this order, the LEGB order:\n\nLocal – inside the current function (including function arguments).\nEnclosing – outer function scopes (for nested functions, function inside a function).\nGlobal – module-level names.\nBuilt-in – stuff like len, print, int, def.\n\n\n\nQuestion 12: What is the output of the following cells ?\n\ndef replace_list(lst):\n    lst = [99, 100]\n\nx = [1, 2, 3]\nreplace_list(x)\nprint(x)\n\n\nx = 10\n\ndef outer():\n    x = 20 \n\n    def inner():\n        x = 30\n        print(x)\n\n    inner()\n\nouter()"
  },
  {
    "objectID": "media/teaching/TRD/tp3_pca.html",
    "href": "media/teaching/TRD/tp3_pca.html",
    "title": "\nTP 3: Numpy: Linear algebra and PCA\n",
    "section": "",
    "text": "INSEA             Techniques de réduction de dimension - 2025 \n\n\nTP 3: Numpy: Linear algebra and PCA\n\n            Author: Hicham Janati \n\n\nHow to follow this lab:\n\nThe goal is to understand AND retain in the long term: resist copy-pasting, prefer typing manually.\nGetting stuck while programming is completely normal: search online, use documentation, or use AI.\nWhen prompting the AI, you must be specific. Explain that your goal is to learn, not to get an instant solution no matter what. Ask for short, explained answers with alternatives.\nNEVER ASK THE AI TO PRODUCE MORE THAN ONE LINE OF CODE!\nAdopt the Solve-It method: always try to solve a question or predict the output of code before running it. Learning happens when you confirm your understanding—and even more when you’re wrong and surprised.\n\n\n\n1 - Eigenvalues, eigenvectors, singular values, singular vectors\nEigenvalues and eigenvectors can be computed using the np.linalg.eig function. Example with a symmetric matrix:\n\nimport numpy as np\nA = np.random.randn(5, 5)\nA = A + A.T\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n\nQuestion 1\nVerify the eigen property \\(Ax = \\lambda x\\) for all eigenvalues in a vectorized form. Do these eigenvectors form an orthogonal matrix ?\n\n\nQuestion 2\nIf the matrix is Hermitian (Hermitian is equivalent to Symmetric if the matrix is real), np.linalg.eigh must be prefered, it is more stable and faster. Compare their output and computation time for large matrices.\nThe SVD can be computed for any rectangular matrix using np.linalg.svd\n\nA = np.random.randn(5, 3)\nU, S, V_t = np.linalg.svd(A)\n\n\n\nQuestion 3:\nVerify the SVD formula \\(A = USV^\\top\\).\n\n\nQuestion 4:\nWhat is the link between the singular values of A and the eigenvalues of \\(A^\\top A\\) ? Verify that.\n\n\nQuestion 5:\nWe can define several different norms of a rectangular matrix of rank r. In the theoretical exercises, we have demonstrated that they can all be obtained using the singular values: - The usual Frobenius norm: \\(\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2} = \\sqrt{\\sum_{k=1}^r \\sigma_k^2}\\) - The spectral norm (also known as operator norm or L2 norm): \\(\\|A\\|_{op} = \\max_{x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2} = \\max_k {\\sigma_k}\\) - The nuclear norm (also known as Trace norm) \\(\\|A\\|_{*} = \\sum_{k} \\sigma_k\\) Verify how to compute each of these norms using the svd and directly via np.linalg.norm by specifying the correct ord argument. Check the table in the Notes for the different ord values: https://numpy.org/devdocs/reference/generated/numpy.linalg.norm.html\n\nA = np.random.randn(5, 3)\n\n\n\n\n2 - PCA with Python\nThe goal of this section is to make your own implementation of PCA with numpy. We will first be working with random data.\n\nX = np.random.randn(100, 5)\n\n\nQuestion 6:\nCenter the data and compute the empirical covariance matrix \\(\\Sigma\\) of \\(X\\).\n\n\nQuestion 7:\nCompute the principal components of the PCA using the spectral decomposition of \\(\\Sigma\\) and project the data on the first k components\n\nk = 5\n\n\n\nQuestion 8:\nHow could you have obtained this projection without computing \\(\\Sigma\\) ?\n\n\nQuestion 9:\nPackage this logic in a function that takes a dataset, a number of components and returns the eigenvalues and eigenvectors\n\ndef compute_pca(dataset, n_components):\n    # todo\n    return eigenvalues, eigenvectors\n\n\n\nQuestion 10:\nCompute the explained variance ratio of each component and visualize with matplotlib the scree plot. The plotting code is given below:\n\nfrom matplotlib import pyplot as plt\n\npercentages = # todo\n\nplt.figure(figsize=(10, 5))\nplt.plot(percentages, marker='o')\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Scree Plot')\nplt.show()\n\n\n\n\nQuestion 11:\nUsing np.cumsum make this plot visualize the cumulated explained variance percentage.\n\n\nQuestion 12:\nHow can we obtain the PCA projection in 2D of the data ? Visualize the obtained data in 2d, here’s the matplotlib code for that:\n\npca_2d_projection = # todo\n\nplt.figure(figsize=(10, 5))\nplt.scatter(pca_2d_projection[:, 0], pca_2d_projection[:, 1], marker='o')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Projection in 2D')\n\n\n\nQuestion 13:\nWe would like to project a new data point on the PCA space. Complete the following function that does this.\n\ndef project_on_pca(data, eigenvalues, eigenvectors, n_components):\n    # todo\n    return projection\n\n\n\n\n3. Introduction to Object oriented programming (OOP) in Python:\nThe goal of this section is to nicely package the PCA code into a clean abstraction using class. A class is a way to define a new object with attributes. Here’s an example. We create a “Player” character in some 2D video game which can move around. Each character is defined with: - its speed (fixed) - its coordinates (x, y) that change during the game and are initially set to (0, 0).\nA Class needs a constructor function called __init__ that sets attributes speed, x and y. The self argument is the object itself. The __init__ function is called when we create a new instance of the object player.\n\nclass Player():\n    def __init__(self, speed):\n        self.speed = speed\n        self.x = 0\n        self.y = 0\n\nWe can create a variable of type player with speed=5:\n\np1 = Player(5) # here __init__ is called with speed=5\n\nprint(f\"p1.speed: {p1.speed} | p1.x: {p1.x} | p1.y: {p1.y}\")\n\nWe can modify the attributes of the object:\n\np1.x += 10\nprint(f\"p1.x: {p1.x}\")\n\nBut that is not a good practice ! To keep things clean, we never change attributes directly, instead we implement functions inside class called methods. We add a move method that changes the coordinates of the player given a direction and duration:\n\nclass Player():\n    def __init__(self, speed):\n        self.speed = speed\n        self.x = 0\n        self.y = 0\n    \n    def move(self, direction_x, direction_y, dt=1):\n        # directions must be -1, 0 or 1\n        direction_x = np.sign(direction_x)\n        direction_y = np.sign(direction_y)\n        self.x += direction_x * self.speed * dt\n        self.y += direction_y * self.speed * dt\n\np1 = Player(3)\np1.move(direction_x=1, direction_y=-1, dt=5)\nprint(f\"p1.x: {p1.x} | p1.y: {p1.y}\")\n\nWe would like to apply this to create our own PCA class. A PCA class is defined with its number of components and can be applied to any data. ### Question 14: Complete the following code using your previous implementation\n\nclass MyPCA():\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.eigenvalues = None\n        self.eigenvectors = None\n        # add attributes if needed\n\n    def computePCA(self, data):\n        # todo\n\n    def applyPCA(self, new_data):\n        # todo\n    \n    def plot_scree_plot(self):\n        # todo\n\n    def plot_cumulated_variance(self):\n        # todo\n\n    def plot_pca_projection(self, new_data):\n\n\nYou can now create your own PCA object. Test its functions on random data:\n\nX = np.random.randn(100, 5)\n\npca = MyPCA(2)\n\n# todo \n\n\nQuestion 15\nCompare the PCA you obtained with scikit-learn’s implementation which can be computed using:\n\nfrom sklearn.decomposition import PCA\ndata = np.random.randn(100, 5)\n\n\npca_sklearn = PCA(n_components=2)\npca_sklearn.fit(data)\nexplained_variance_ratio = pca_sklearn.explained_variance_ratio_\nprojected_data = pca_sklearn.transform(data)\n\n\n\n\n\n4 - PCA for Machine learning\nPCA is mostly used as a preprocessing step if the dimension is very high. We will investigate this using synthetic data. The following code creates a dataset of variables (X) and binary classes (y) that we want to predict using k-NN. To keep this simple, we keep the number of observations (samples) fixed n_samples=100 and only change the dimension (n_features):\n\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=200, n_features=300, n_informative=10)\n\nprint(X.shape)\nprint(y.shape)\nprint(f\"the first 5 y are: {y[:5]}\")\n\nTo test the performance of k-NN, we first split the data into a training and a test set. The train set is where we assume y is known. On the other hand, the test set is used to evaluate the model: we predict the labels y with the model, and evaluate its accuracy using y_test.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nWe can fit a k-NN classifier with 5 neighbors and predict the labels of X_test:\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nprint(f\"the first 5 y_pred are: {y_pred[:5]}\")\nprint(f\"the first 5 y_test are: {y_test[:5]}\")\n\n\nQuestion 16:\nWrite a function with numpy that computes the average accuracy comparing y_test with y_true. Compute the accuracy of the predictions on the test data.\n\ndef accuracy(y, y_true):\n    # todo \n\n\n\nQuestion 17:\nApply PCA keeping only 20 components, then fit the k-NN model on the transformed data. Is the performance better or worse ?"
  },
  {
    "objectID": "media/teaching/TRD/tp2_numpy.html#matrix-inverse-and-solving-linear-systems",
    "href": "media/teaching/TRD/tp2_numpy.html#matrix-inverse-and-solving-linear-systems",
    "title": "\nTP 2: Numpy: introduction to scientific computing\n",
    "section": "2.5 Matrix inverse and solving linear systems",
    "text": "2.5 Matrix inverse and solving linear systems\nWe can solve linear systems like \\(Ax = b\\) and find the unknown \\(x\\) using:\n\nA = np.random.randn(10, 10)\nb = np.random.randn(10)\n\nx = np.linalg.solve(A, b)\nprint(x)\n\n[-1.23075777 -1.38216739  0.156997    1.2443776  -0.94236743 -0.47399296\n -1.59190301 -1.57523578 -0.78723647  0.70608696]\n\n\nTo evaluate the accuracy, never compare floats exactly with == since floats are approximations (Section 1). Instead we use approximate comparisons:\n\nabs(A.dot(x) - b).max()\n\nnp.float64(1.4988010832439613e-15)\n\n\nwhich are used in the comparisons functions of numpy like:\n\nnp.allclose(A.dot(x), b)\n\nTrue\n\n\nWe can also compute the solution by inverting the matrix:\n\nAinv = np.linalg.inv(A)\nx2 = Ainv.dot(b)\n\nabs(x2 - x).max()\n\nnp.float64(1.2212453270876722e-15)\n\n\nBoth solutions are identical.\n\nQuestion 15\nUsing different sizes of A and b, compare the time it takes to solve the linear system vs inverting the matrix.\n\n\nQuestion 16\nCompare both solutions with this matrix A and vector b, how can you explain this ?\n\ngrid = np.arange(5)\nA = np.arange(25).reshape(5, 5) / 25\nb = np.linspace(0, 1, 5)\nA[0] = A[1:].sum(axis=0) + 1e-10\n\nx = np.linalg.solve(A, b)\nx2 = np.linalg.inv(A).dot(b)\nabs(x2 - x).max()\n\nnp.float64(1.5)\n\n\n\n\nQuestion 17:\nYou need to implement a formula involving a matrix inverse like: \\[ y = \\| A^\\top B^{-1} z \\| \\] What is the best implementation ?\n\nA = np.random.randn(10, 10)\nB = np.random.randn(10, 10)\nz = np.random.randn(10)\n\ny = # todo"
  },
  {
    "objectID": "media/teaching/TRD/tp4_tsne.html",
    "href": "media/teaching/TRD/tp4_tsne.html",
    "title": "\nTP 4: PCA and non-linear visualization methods\n",
    "section": "",
    "text": "INSEA             Techniques de réduction de dimension - 2025 \n\n\nTP 4: PCA and non-linear visualization methods\n\n Lundi 14 Décembre  2025            Author: Hicham Janati \n\n\nPart 1\nThe following code reads a subset of image data of handwritten digits. The data matrix X contains the images: each row is an 8x8 image. The vector y contains the true digit for each image. In ML, we call y the labels/targets (or “étiquettes” en français).\n\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom matplotlib import pyplot as plt\n\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\nprint(f\"La taille des données est {X.shape} et celle des labels est {y.shape}\")\nprint(f\"La première image ressemble à:\")\nplt.figure(figsize=(3, 3))\nplt.imshow(X[0].reshape(8, 8), cmap=\"Greys\")\nplt.axis(\"off\")\nplt.title(f\"Le label de cette image est {y[0]}\")\nplt.show()\n\n\nQuestion 1\nVisualize the first 8 images of the dataset in a single figure, with their true label shown as the title for each image.\n\n\nQuestion 2\nPrepare the data to perform a principal component analysis and compute the covariance matrix.\n\n\nQuestion 3\nDetermine the principal axes and their variances, then visualize the 2-dimensional projection using a plt.scatter, including the percentage of variance explained by each principal axis. What can you conclude from this?\n\n\nQuestion 4\nIn the 2D projection figure, color each point according to its true label to see whether PCA made it possible to separate the digits into separate clusters. Check the arguments of plt.scatter by running plt.scatter?:\n\nplt.scatter?\n\n\n\nQuestion 5\nVisualize the scree plot (the curve of the cumulative percentage of explained variance as a function of the principal component index, in decreasing order of importance) for this PCA. What do you think about it?\n\n\n\nPart 2: Non-linear methods\nMDS, Isomap and TSNE are implemented in scikit-learn’s manifold module:\n\nfrom sklearn.manifold import Isomap, TSNE, MDS, ClassicalMDS\nfrom sklearn.decomposition import PCA\n\nYou can use the following to run MDS for example:\n\nmds = ClassicalMDS(n_components=2)\nX_mds = mds.fit_transform(X)\nprint(X_mds.shape)\n\n\nQuestion 6:\nCompare the PCA projections with classical MDS both visually and computationally. Are they equivalent ?\n\n\nQuestion 7:\nNow using MDS (metric) MDS, is the visualization similar ? Compare with PCA and Classical MDS.\n\n\nQuestion 8:\nRun isomap and play around with its main arguments (n_neighbors). Check the Isomap Documentation.\n\n\nQuestion 9:\nRun TSNE and play around with perplexity. Check the TSNE documentation\n\n\nQuestion 10:\nInstall umap (pip install umap-learn) and run it on the same dataset. The package follows the same API (logic) of scikit-learn with .fit_transform\n\nfrom umap import UMAP\n\nu = UMAP(n_components=2)\n\n# to do \n\n\n\n\nPart 3: SVHN dataset\nNow we move on to a more complex dataset: the SVHN (Street view house numbers). You can download it here:\n\nimport os, requests\nimport numpy as np\nurl = \"https://www.dropbox.com/scl/fi/5u0s8hdv7wyzh8rhndi2o/small_svhn.npz?rlkey=l0zsmchiymz8qmdjb16koijqa&dl=1\"\n\nlocal_path = \"small_svhn.npz\"\n\nif not os.path.exists(local_path):\n    print(\"Downloading small_svhn.npz...\")\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(local_path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\nelse:\n    print(\"Using cached small_svhn.npz\")\n\ndata = np.load(local_path)\n\nX = np.load(\"small_svhn.npz\")[\"X\"]\ny = np.load(\"small_svhn.npz\")[\"y\"]\n\nprint(f\"The shape of the dataset is {X.shape}\")\nprint(f\"The first labels are {y[:10]}\")\n\nUsing cached small_svhn.npz\nThe shape of the dataset is (3662, 32, 32)\nThe first labels are [1 1 0 2 6 4 2 5 9 0]\n\n\n\nQuestion 11:\nVisualize the first images and their labels. What do you notice ?\n\n\nQuestion 12:\nApply PCA, t-SNE and UMAP on this data. Was the result expected ?\n\n\n\nChallenge\nLes images SVHN ont une très grande variabilité ce qui rend la séparation des clusters difficile. Le défi est de chercher (une ou des transformations) à appliquer à ces images afin d’extraire des “features” qui simplifient la tâche de t-SNE. Par exemple, on peut jouer sur le contraste, les filtres, la taille des images etc. On note la transformation appliquée à ces images par \\(\\phi\\): \\[ \\phi: \\quad image \\in \\mathbb R^{32 \\times 32} \\mapsto \\phi(image) \\in \\mathbb R^{k} \\]\nà vous de définir la transformation \\(\\phi\\) adéquate telle que t-SNE appliquée sur \\(\\phi(images)\\) distingue au mieux les clusters. Pour définir la transformation \\(\\phi\\) vous pouvez utiliser n’importe quelle librairie Python à condition de respecter les règles suivantes: - \\(\\phi\\) ne dépend que des images \\(X\\) et n’utilise pas les labels \\(y\\) - Comprendre exactement ce que fait la fonction \\(\\phi\\)\nPour vous expliquer l’évaluation de votre rendu, on commence avec une fonction \\(\\phi\\) naive qui ne fait que reshape chaque image 32 x 32 en un vecteur de taille 1024. On utilise la fonction transform_images qui calcule \\(phi(x_i)\\) pour chaque image \\(x_i\\) et renvoie le dataset transformé en dimension \\(k\\):\n\n\ndef phi(image):\n    # TODO: define the transformation $\\phi$\n    return image.flatten()\n\ndef transform_images(X):\n    out = []\n    for image in X:\n        phi_x = phi(image)\n        out.append(phi_x)\n    return np.array(out)\n\n\nVous serez évalués sur le temps pris par la transformation également:\n\nfrom time import time\n\nt0 = time()\nX_transformed = transform_images(X)\nt1 = time()\n\nprint(f\"Temps pris pour la transformation du dataset: {t1 - t0}s | Nouvelle dimension des données: {X_transformed.shape}\")\n\nTemps pris pour la transformation du dataset: 0.022612810134887695s | Nouvelle dimension des données: (3662, 1024)\n\n\nOn applique t_SNE sur ces données transformées. Afin de garder la reproducibilité du code et de pouvoir comparer avec les autres soumissions de façon équitable, ne changez pas le random_state=42:\n\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pyplot as plt\n\n\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_transformed)\n\nplt.figure(figsize=(4, 4))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=\"tab10\")\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nPour évaluer la qualité des clusters, on évalue simplement la performance d’un k-NN sur la représentation t-SNE. En pratique, on ne fait jamais ça, ceci est un simple critère mesurable afin de comparer vos méthodes. Éxécutez mais Ne changez pas le code suivant:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_tsne, y, test_size=0.2, random_state=42, stratify=y)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nscore = knn.score(X_test, y_test)\nprint(f\"Score du k-NN: {score}\")\n\nScore du k-NN: 0.29740791268758526"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#loading-and-preparing-the-data",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#loading-and-preparing-the-data",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.1 Loading and preparing the data",
    "text": "1.1 Loading and preparing the data\nLet’s start by loading the digits dataset and preparing it for binary classification:\n\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom scipy.optimize import check_grad\n\n# Load the digits dataset\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\nprint(f\"Dataset shape: X={X.shape}, y={y.shape}\")\nprint(f\"Unique labels: {np.unique(y)}\")\n\n\nQuestion 1:\nFilter the dataset to keep only digits 0 and 1. Then split the data into training and test sets (use 80% for training, 20% for test). What are the shapes of your training and test sets?\n\n\nQuestion 2:\nNormalize the features by subtracting the mean and dividing by the standard deviation. Why is this important? Apply the normalization to both training and test sets, but compute the mean and standard deviation only from the training set."
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#the-logistic-regression-model",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#the-logistic-regression-model",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.2 The logistic regression model",
    "text": "1.2 The logistic regression model\nLogistic regression models the probability that a sample belongs to class 1 using the sigmoid function:\n\\[P(y=1 | x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}\\]\nwhere \\(w\\) is the weight vector (including the bias term) and \\(\\sigma\\) is the sigmoid function.\n\nQuestion 3:\nImplement the sigmoid function. What is the range of its output? What happens when the input is very large (positive or negative)?\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid function: sigma(z) = 1 / (1 + exp(-z))\n    \n    Args:\n        z: Input (can be a scalar or array)\n    \n    Returns:\n        Sigmoid of z\n    \"\"\"\n    # TODO: implement the sigmoid function\n\n\n\n\nQuestion 4:\nImplement a function that computes the predictions (probabilities) for a given weight vector \\(w\\) and input data \\(X\\). The function should return probabilities for each sample.\n\ndef predict_proba(X, w):\n    \"\"\"\n    Compute the probability P(y=1 | x) for each sample in X.\n    \n    Args:\n        X: Input data (n_samples, n_features)\n        w: Weight vector (n_features,)\n    \n    Returns:\n        Probabilities (n_samples,)\n    \"\"\"\n\n# test it with random weights"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#the-loss-function",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#the-loss-function",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.3 The loss function",
    "text": "1.3 The loss function\nFor binary classification, we use the binary cross-entropy loss (also called log loss):\n\\[L(w) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\sigma(w^T x_i)) + (1-y_i) \\log(1-\\sigma(w^T x_i)) \\right]\\]\nThis loss function penalizes confident wrong predictions more than uncertain ones.\n\nQuestion 5:\nImplement the loss function. What happens if \\(\\sigma(w^T x_i) = 0\\) when \\(y_i = 1\\)? How can we avoid numerical issues?\n\ndef compute_loss(X, y, w):\n    \"\"\"\n    Compute the binary cross-entropy loss.\n    \n    Args:\n        X: Input data (n_samples, n_features)\n        y: True labels (n_samples,)\n        w: Weight vector (n_features,)\n    \n    Returns:\n        Loss value (scalar)\n    \"\"\"\n\n    return loss\n\n# Test the loss function"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#computing-the-gradient",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#computing-the-gradient",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.4 Computing the gradient",
    "text": "1.4 Computing the gradient\nTo minimize the loss function using gradient descent, we need to compute its gradient with respect to the weights \\(w\\). Compute the gradient of the binary cross-entropy loss with pen and paper then implement the gradient function. Verify that the output has the same shape as the weight vector \\(w\\).\n\ndef compute_gradient(X, y, w):\n    \"\"\"\n    Compute the gradient of the loss with respect to w.\n    \n    Args:\n        X: Input data (n_samples, n_features)\n        y: True labels (n_samples,)\n        w: Weight vector (n_features,)\n    \n    Returns:\n        Gradient vector (n_features,)\n    \"\"\"\n\n    return gradient\n\n# Test the gradient"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#gradient-checking",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#gradient-checking",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.5 Gradient checking",
    "text": "1.5 Gradient checking\nBefore implementing gradient descent, it’s crucial to verify that our gradient computation is correct. We can do this using numerical differentiation and comparing it with our analytical gradient.\n\nQuestion 7:\nWe can use scipy.optimize.check_grad to verify that your gradient implementation of the loss is correct. Here’s an example with the function: \\[ x \\mapsto f(x, a, b) = a \\|x\\|^2 + b^\\top x \\] its gradient is given by: \\[ \\nabla_x f(x, a, b) = 2 a x + b\\]\n\n# Wrapper functions for check_grad\nimport numpy as np\nfrom scipy.optimize import check_grad\n\ndim = 10\na = 5\nb = np.random.randn(dim)\n\ndef f(x, a, b):\n    return a * np.linalg.norm(x)**2 + b @ x\n\ndef grad_f(x, a, b):\n    return 2 * a * x + b\n\n\ndef loss_wrapper(x):\n    return f(x, a, b)\n\ndef grad_wrapper(x):\n    return grad_f(x, a, b)\n\nx_check = np.random.randn(dim)\n\nerror = check_grad(loss_wrapper, grad_wrapper, x_check)\n\nprint(f\"Gradient check error: {error:.2e}\")"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#gradient-descent",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#gradient-descent",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.6 Gradient descent",
    "text": "1.6 Gradient descent\nNow that we’ve verified our gradient, we can implement gradient descent to minimize the loss function. The update rule is:\n\\[w_{t+1} = w_t - \\alpha \\nabla_w L(w_t)\\]\nwhere \\(\\alpha\\) is the learning rate.\n\nQuestion 8:\nImplement gradient descent. The function should: 1. Initialize weights (you can use zeros or small random values) 2. For each iteration: - Compute the gradient - Update the weights - Optionally store the loss for visualization 3. Return the final weights and the history of losses\nVisualize the curve of the loss as function of the iterations\n\ndef gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, verbose=True):\n    \"\"\"\n    Perform gradient descent to minimize the loss.\n    \n    Args:\n        X: Input data (n_samples, n_features)\n        y: True labels (n_samples,)\n        learning_rate: Step size for gradient descent\n        n_iterations: Number of iterations\n        verbose: Whether to print progress\n    \n    Returns:\n        w: Final weight vector\n        loss_history: List of loss values at each iteration\n    \"\"\""
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#effect-of-learning-rate",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#effect-of-learning-rate",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.7 Effect of learning rate",
    "text": "1.7 Effect of learning rate\nThe learning rate is a crucial hyperparameter. If it’s too small, convergence is slow. If it’s too large, the algorithm might diverge or oscillate.\n\nQuestion 10:\nRun gradient descent with different learning rates (e.g., 0.001, 0.01, 0.1, 1.0) and compare: - The convergence speed - The final loss value - Whether the algorithm converges or diverges\nVisualize the loss curves for different learning rates on the same plot and explain what you observe."
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#evaluating-on-test-data",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#evaluating-on-test-data",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "1.8 Evaluating on test data",
    "text": "1.8 Evaluating on test data\nNow that we’ve trained our model, we need to evaluate its performance on unseen test data. This is crucial to assess whether our model generalizes well.\n\nQuestion 12:\nImplement a function that: 1. Computes predictions (probabilities) for test data 2. Converts probabilities to binary predictions (threshold = 0.5) 3. Computes the accuracy: (number of correct predictions) / (total number of samples)\nWhat is the accuracy on the training set? On the test set? Are they similar?\n\ndef predict(X, w, threshold=0.5):\n    \"\"\"\n    Make binary predictions.\n    \n    Args:\n        X: Input data (n_samples, n_features)\n        w: Weight vector (n_features,)\n        threshold: Decision threshold\n    \n    Returns:\n        Binary predictions (n_samples,)\n    \"\"\"\n\ndef compute_accuracy(X, y, w):\n    \"\"\"\n    Compute the accuracy of predictions.\n    \n    Args:\n        X: Input data (n_samples, n_features)\n        y: True labels (n_samples,)\n        w: Weight vector (n_features,)\n    \n    Returns:\n        Accuracy (scalar between 0 and 1)\n    \"\"\""
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#introduction-to-pytorch",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#introduction-to-pytorch",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "2.1 Introduction to PyTorch",
    "text": "2.1 Introduction to PyTorch\nPyTorch is a deep learning framework that provides automatic differentiation (autograd). This means we don’t need to manually compute gradients—PyTorch tracks operations and can compute gradients automatically.\nLet’s start by importing PyTorch and understanding tensors:\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# Create a simple tensor\nx = torch.tensor([1.0, 2.0, 3.0])\nprint(f\"Tensor x: {x}\")\nprint(f\"Tensor shape: {x.shape}\")\nprint(f\"Tensor dtype: {x.dtype}\")"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#automatic-differentiation-autograd",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#automatic-differentiation-autograd",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "2.2 Automatic Differentiation (Autograd)",
    "text": "2.2 Automatic Differentiation (Autograd)\nThe key feature of PyTorch is automatic differentiation. When we create a tensor with requires_grad=True, PyTorch tracks all operations on it and can compute gradients automatically.\n\nQuestion 14:\nRun the following code and explain what happens. What is the difference between requires_grad=True and requires_grad=False?\n\n# Create a tensor that requires gradient computation\nx = torch.tensor([2.0], requires_grad=True)\nprint(f\"x: {x}\")\nprint(f\"x.requires_grad: {x.requires_grad}\")\n\n# Define a simple function: y = x^2\ny = x ** 2\nprint(f\"y: {y}\")\nprint(f\"y.requires_grad: {y.requires_grad}\")\nprint(f\"y.grad_fn: {y.grad_fn}\")  # This shows the operation that created y\n\n# Compute the gradient\ny.backward()  # This computes dy/dx\nprint(f\"x.grad: {x.grad}\")  # Should be 2*x = 4.0\n\n\n\nQuestion 15:\nTry a more complex function: \\(z = x^2 + 2xy + y^2\\) where \\(x=2\\) and \\(y=3\\). Compute \\(\\frac{\\partial z}{\\partial x}\\) and \\(\\frac{\\partial z}{\\partial y}\\) using PyTorch’s autograd. Verify manually that the gradients are correct."
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#building-a-neural-network",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#building-a-neural-network",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "2.3 Building a Neural Network",
    "text": "2.3 Building a Neural Network\nNow let’s build a neural network with one hidden layer. We’ll use PyTorch’s nn.Module class, which provides a convenient way to define neural networks.\nOur network will have: - Input layer: 64 features (8x8 image flattened) - Hidden layer: 32 neurons with ReLU activation - Output layer: 10 neurons (one for each digit 0-9) with softmax\n\nQuestion 17:\nWe create a neural network class that inherits from nn.Module. What is the purpose of the __init__ and forward methods?\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size=64, hidden_size=32, output_size=10):\n        \"\"\"\n        Initialize the neural network.\n        \n        Args:\n            input_size: Number of input features\n            hidden_size: Number of neurons in the hidden layer\n            output_size: Number of output classes\n        \"\"\"\n        super(NeuralNetwork, self).__init__()\n        \n        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden\n        self.fc2 = nn.Linear(hidden_size, output_size)  # Hidden to output\n        self.relu = nn.ReLU()  # Activation function\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the network.\n        \n        Args:\n            x: Input tensor (batch_size, input_size)\n        \n        Returns:\n            Output tensor (batch_size, output_size)\n        \"\"\"\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Create an instance of the network\nmodel = NeuralNetwork(input_size=64, hidden_size=32, output_size=10)\nprint(model)\n\n# Test with a random input\nx_test = torch.randn(5, 64)  # Batch of 5 samples\noutput = model(x_test)\nprint(f\"\\nInput shape: {x_test.shape}\")\nprint(f\"Output shape: {output.shape}\")"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#preparing-the-data",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#preparing-the-data",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "2.4 Preparing the Data",
    "text": "2.4 Preparing the Data\nBefore training, we need to convert our NumPy arrays to PyTorch tensors and create data loaders for efficient batch processing.\n\nQuestion 18:\nWe convert the digits dataset to PyTorch tensors. Use all 10 classes (not just 0 and 1). What is the difference between torch.tensor() and torch.from_numpy()?\n\n# Load full digits dataset (all 10 classes)\ndigits = load_digits()\nX_full = digits.data\ny_full = digits.target\n\n# Split into train and test\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n    X_full, y_full, test_size=0.2, random_state=42\n)\n\n# Normalize\nmean_train_full = X_train_full.mean(axis=0)\nstd_train_full = X_train_full.std(axis=0)\nX_train_full_norm = (X_train_full - mean_train_full) / (std_train_full + 1e-8)\nX_test_full_norm = (X_test_full - mean_train_full) / (std_train_full + 1e-8)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.from_numpy(X_train_full_norm).float()\ny_train_tensor = torch.from_numpy(y_train_full).long()\nX_test_tensor = torch.from_numpy(X_test_full_norm).float()\ny_test_tensor = torch.from_numpy(y_test_full).long()\n\nprint(f\"Training set: {X_train_tensor.shape}, {y_train_tensor.shape}\")\nprint(f\"Test set: {X_test_tensor.shape}, {y_test_tensor.shape}\")\n\n# Create data loaders for batch processing\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\nNumber of batches in training set: {len(train_loader)}\")\nprint(f\"Batch size: {batch_size}\")"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#training-the-model",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#training-the-model",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "2.5 Training the Model",
    "text": "2.5 Training the Model\nNow we’ll train the neural network. The training loop involves: 1. Forward pass: compute predictions 2. Compute loss 3. Backward pass: compute gradients 4. Update weights using an optimizer\nWe implement the training loop. Using: - Cross-entropy loss (nn.CrossEntropyLoss) - Stochastic gradient descent optimizer (optim.SGD) - Learning rate of 0.01\nTrain for 100 epochs and print the loss every 10 epochs (iterations).\n\n# Create model, loss function, and optimizer\nmodel = NeuralNetwork(input_size=64, hidden_size=32, output_size=10)\ncriterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\noptimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n\n# Training loop\nn_epochs = 100\ntrain_losses = []\n\nfor epoch in range(n_epochs):\n    epoch_loss = 0.0\n    n_batches = 0\n    \n    # Iterate over batches\n    for batch_X, batch_y in train_loader:\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(batch_X)\n        \n        # Compute loss\n        loss = criterion(outputs, batch_y)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n    \n    avg_loss = epoch_loss / n_batches\n    train_losses.append(avg_loss)\n    \n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {avg_loss:.4f}\")\n\n# Plot training loss\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.grid(True)\nplt.show()\n\n\nQuestion 20:\nEvaluate the model on the test set. Compute the accuracy. How does it compare to the training accuracy?\n\ndef evaluate_model(model, data_loader):\n    \"\"\"\n    Evaluate the model on a dataset.\n    \n    Returns:\n        accuracy: Classification accuracy\n    \"\"\"\n    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for batch_X, batch_y in data_loader:\n            outputs = model(batch_X)\n            _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n            total += batch_y.size(0)\n            correct += (predicted == batch_y).sum().item()\n    \n    accuracy = correct / total\n    return accuracy\n\n# Evaluate on training and test sets\ntrain_accuracy = evaluate_model(model, train_loader)\ntest_accuracy = evaluate_model(model, test_loader)\n\nprint(f\"Training accuracy: {train_accuracy:.4f}\")\nprint(f\"Test accuracy: {test_accuracy:.4f}\")"
  },
  {
    "objectID": "media/teaching/TRD/tp5_intro_pytorch.html#extracting-hidden-layer-representations",
    "href": "media/teaching/TRD/tp5_intro_pytorch.html#extracting-hidden-layer-representations",
    "title": "\nTP 5: Optimization and supervised representations\n",
    "section": "2.6 Extracting Hidden Layer Representations",
    "text": "2.6 Extracting Hidden Layer Representations\nOne of the powerful aspects of neural networks is that the hidden layers learn useful representations of the input data. These representations can be used for visualization, transfer learning, or other tasks.\n\nQuestion 21:\nModify the forward method to also return the hidden layer activations. Then extract the hidden layer representations for all test samples and visualize them using PCA (project to 2D). Color the points by their true labels. Do the hidden representations separate the classes well?\n\n\nQuestion 22:\nCompare the PCA visualization of the hidden representations with a PCA visualization of the original input data. Which one separates the classes better? What does this tell you about what the neural network learned?"
  }
]