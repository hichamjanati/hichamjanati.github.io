{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Optimisation Différentiable \n",
    "<h3 align=\"right\"> ENSAE, Avril 2018 </h3>\n",
    "\n",
    "\n",
    "<h4 align=\"right\"> Hicham Janati </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de Newton\n",
    "### 1.1 Introduction\n",
    "Soit $f$ une fonction de classe $\\mathcal{C}^1$ de $\\mathbb{R}^n$ dans $\\mathbb{R}^n$. Le but de la méthode de Newton est de résoudre $f(x) = 0$.\n",
    "Soit $x_0$ dans $\\mathbb{R}^n$. L'approximation de premier ordre de $f$ dans un voisinage de $x_0$ est donnée par:\n",
    "$$ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) $$\n",
    "Oû $J$ est la matrice Jacobienne de f.\n",
    "Annuler la tangeante $\\hat{f}$ donne $x = x_0 - J^{-1}f(x_0)$ et obtient donc la suite d'itérés:\n",
    "$$x_k = x_{k-1} - J^{-1}f(x_{k-1})$$\n",
    "\n",
    "\n",
    "#### Question 1\n",
    "Soit $x^{\\star}$ un zéro de $f$. Supposons que $J(x^{\\star})$ est inversible et que $f$ est de classe $\\mathcal{C}^2$. Montrez que la méthode de Newton a une convergence localement quadratique i.e qu'il existe une boule B centrée en $x^{\\star}$ telle que pour tout $x_0$ dans B, il existe $\\alpha > 0$ telle que la suite de Newton vérifie: $$ \\|x_{k+1} - x^{\\star}\\| < \\alpha \\|x_{k} - x^{\\star}\\|^2 $$\n",
    "###### indication: Écrire l'approximation de deuxième ordre de f avec reste intégral.\n",
    "\n",
    "---------\n",
    "\n",
    "Deux remarques importantes:\n",
    "- Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de $x_0$.\n",
    "- $J$ doit être inversible.\n",
    "\n",
    "---------\n",
    "\n",
    "### 1.2 Optimisation sans contraintes\n",
    "Soit $g$ une fonction de classe $\\mathcal{C}^2$ de $\\mathbb{R}^n$ dans $\\mathbb{R}$.\n",
    "\n",
    "##### Question 2 \n",
    "Adapter la méthode de Newton pour résoudre $\\min_{x \\in \\mathbb{R}^n} g(x)$.\n",
    "\n",
    " <br> <font color=\"green\"> Comme il s'agit d'un problème sans contraintes, on peut appliquer la méthode de Newton à $\\nabla g$ pour résoudre $\\nabla g(x) = 0$. (A priori, on converge donc vers un point critique de $g$.) </font>\n",
    " \n",
    " <br> <font color=\"blue\"> Pour les questions 3-4-5,  On prend $g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2$, avec $A \\in \\mathcal{M}_{m, n}(\\mathbb{R})$, $b \\in \\mathbb{R}^m $ et $\\gamma \\geq 0 $</font>\n",
    "\n",
    "##### Question 3\n",
    "Donner le gradient et la hessienne de g et complétez les fonctions `gradient` et `hessian` ci-dessous.\n",
    "Vérifiez votre gradient avec l'approximation numérique donnée par ```scipy.optimize.check_grad```.\n",
    "\n",
    "<br> <font color=\"green\"> Pour h dans un voisinage de x, on développe $g(x + h) = g(x) + \\langle A^{\\top}(Ax - b) + 2\\gamma x, h\\rangle + \\frac{1}{2}h^{\\top}(A^{\\top}A + 2 \\gamma I_n) h$.\n",
    " Ainsi, par identification de la partie linéaire en h: $\\nabla g(x) = A^{\\top}(Ax - b) + 2\\gamma x$\n",
    " Et comme $A^{\\top}A + 2 \\gamma I_n$ est symmétrique: $\\nabla^2 g(x) = A^{\\top}A + 2 \\gamma I_n$\n",
    " </font> \n",
    "   \n",
    "##### Question 4\n",
    "Lorsque $\\gamma > 0$, montrez que la méthode de Newton converge en une itération indépendemment de $x_0$.\n",
    "<br> <font color=\"green\"> Il suffit d'écrire la condition d'optimalité $\\nabla g(x) = 0$ qui donne exactement l'itération de Newton. \n",
    " </font> \n",
    " \n",
    "##### Question 5\n",
    "\n",
    "Complétez la fonction `newton` ci-dessous pour résoudre (2). Calculer l'inverse de la hessienne est très couteux (complexité $O(n^3)$), comment peut-on y remédier ?\n",
    "Vérifiez le point (4) numériquement.\n",
    "\n",
    "<br> <font color=\"green\"> \n",
    "    Au lieu d'inverser la hessienne, on résout un système linéaire, ce qui est 3 fois moins couteux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 1729 # Seed du générateur aléatoire\n",
    "m, n = 50, 100\n",
    "rnd = np.random.RandomState(seed) # générateur aléatoire\n",
    "A = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\n",
    "b = rnd.randn(m) # on génére b aléatoirement également \n",
    "gamma = 1.\n",
    "\n",
    "def g(x):\n",
    "    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n",
    "    Ax = A.dot(x)\n",
    "    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n",
    "    return gx\n",
    "\n",
    "def gradient_g(x):\n",
    "    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n",
    "    # A faire\n",
    "    g = A.T.dot(A.dot(x) - b) + 2 * gamma * x\n",
    "    return g\n",
    "\n",
    "def hessian_g(x):\n",
    "    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n",
    "    # A faire\n",
    "    n = len(x)\n",
    "    h = A.T.dot(A) + 2 * gamma * np.identity(n)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy `scipy.optimize.check_grad`. \n",
    "Exécutez ```scipy.optimize.check_grad?``` pour obtenir la documentation de la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00022141735630660996"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "x_test = rnd.randn(n) # point où on veut évaluer le gradient\n",
    "check_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n",
    "    \"\"\"Solve min g with newton method\"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    if verbose:\n",
    "        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n",
    "        strings = [s.center(13) for s in strings]\n",
    "        strings = \" | \".join(strings)\n",
    "        print(strings)\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        H = hessian(x)\n",
    "        d = gradient(x)\n",
    "        \n",
    "        if verbose:\n",
    "            obj = g(x)\n",
    "            strings = [i, obj, abs(d).max()] # On affiche des trucs \n",
    "            strings = [str(s).center(13) for s in strings]\n",
    "            strings = \" | \".join(strings)\n",
    "            print(strings)\n",
    "        \n",
    "        # A faire\n",
    "        x = x + np.linalg.solve(H, - d)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration   |     g(x_k)    | max|gradient(x_k)|\n",
      "      0       | 3570.345200187844 | 291.7910294967238\n",
      "      1       | 1.0704068547572962 | 1.0200174038743626e-13\n",
      "      2       | 1.0704068547572965 | 7.507883204027621e-15\n",
      "      3       | 1.070406854757296 | 6.106226635438361e-15\n",
      "      4       | 1.0704068547572962 | 4.884981308350689e-15\n",
      "      5       | 1.070406854757296 | 4.810388198883686e-15\n",
      "      6       | 1.0704068547572962 | 6.895525817007808e-15\n",
      "      7       | 1.070406854757296 | 5.412337245047638e-15\n",
      "      8       | 1.070406854757296 | 5.224987109642143e-15\n",
      "      9       | 1.070406854757296 | 5.662137425588298e-15\n"
     ]
    }
   ],
   "source": [
    "x0 = rnd.randn(n)\n",
    "x = newton(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Optimisation avec contraintes d'égalité\n",
    "\n",
    "On s'intéresse à présent au problème avec contrainte linéaire: $$ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 $$\n",
    "\n",
    "##### Question 6\n",
    "Donnez (en justifiant) le système KKT du problème.\n",
    "\n",
    "<br> <font color=\"green\"> \n",
    "    Notons la fonction objective par $g$ et les contraintes linéaires par $h(x) = Cx - d = 0$.\n",
    "    Remearquez ici que h regroupe toutes les contraintes linéaires données par $\\langle C_i, x\\rangle - d_i = 0$ où l'indice i dénote la ième ligne. Notons chacune de ces contraintes par $h_i, i=1\\dots p$\n",
    "\n",
    "1) existence:\n",
    "    Par continuité de h, l'ensemble des contraintes est un fermé. g est continue et clairement coercive, le minimum donc existe. \n",
    "    \n",
    "2) convexité: \n",
    "    Comme $\\gamma > 0$, on a pour tout $x,h  \\in \\mathbb{R}^n$: $$ h^{\\top} \\nabla^2 g(x)h = h^{\\top}(A^{\\top}A + 2\\gamma I_n) = \\|Ah\\|^2 + 2 \\gamma \\|h\\|^2 > 0$$ Donc g est strictivement convexe.\n",
    "    La solution est donc unique. \n",
    "    \n",
    "3) KKT:\n",
    "    Les contraintes sont linéaires, elle est donc qualifié sur K, donc toute solution du problème vérifie KKT.\n",
    "    Par convexité (+ qualification), KKT est aussi suffisante, donc toute solution de KKT est un minimum. \n",
    "    Par unicité, La solution de KKT est la solution du problème. Notons la $(x, \\mu) \\in \\mathbb{R}^n \\times \\mathbb{R}^p$ :\n",
    "    \n",
    "   $$ \\left\\{ \\begin{array}{l}\n",
    "               \\nabla g(x) + \\sum_{i=1}^p \\mu_i\\nabla h_i(x) = 0 \\\\\n",
    "               h(x) = 0\n",
    "            \\end{array}\n",
    "            \\right.  $$\n",
    "   i.e \n",
    "      $$ \\left\\{ \\begin{array}{l}\n",
    "                A^{\\top}(Ax - b) + 2\\gamma x + C^{\\top}\\mu = 0 \\\\\n",
    "                Cx - d = 0\n",
    "            \\end{array}\n",
    "            \\right.  $$\n",
    "</font>\n",
    "\n",
    "##### Question 7\n",
    "Expliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n",
    "\n",
    "<br> <font color=\"green\"> \n",
    "Le problème est équivalent à son système KKT, on peut donc résoudre KKT avec la méthode de Newton appliquée à F ci-dessous pour résoudre $F(x, \\mu) = 0$:\n",
    "    $$ F(x, \\mu) = \\left(\\nabla g(x) + \\mu \\nabla h(x), h(x)\\right) $$ \n",
    "La suite de Newton s'écrit donc: $$(x_{k+1}, \\mu_{k+1}) = (x_{k}, \\mu_{k}) - J_F^{-1}(x_{k}, \\mu_{k}) F(x_{k}, \\mu_{k}) $$\n",
    "\n",
    "On a donc besoin d'écrire la Jacobienne de F. \n",
    "On a:\n",
    "$$ F(x, \\mu) = \\left(A^{\\top}(Ax - b) + 2\\gamma x + C^{\\top}\\mu, Cx - d\\right) $$\n",
    "On écrit la matricienne Jacobienne de F par blocs: \n",
    "\n",
    "$$ J_F(x, \\mu) = \\begin{pmatrix} A^{\\top}A + 2\\gamma I_n & C^{\\top} \\\\ C & 0 \\end{pmatrix} $$\n",
    " </font>\n",
    "##### Question 8\n",
    "Implémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n",
    "\n",
    "##### Question 9\n",
    "Implémentez la version de newton adaptée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 5 # nombre de contraintes\n",
    "C = rnd.randn(p, n)\n",
    "d = rnd.randn(p)\n",
    "\n",
    "def F(x, mu):\n",
    "    \"\"\"Compute the function F.\"\"\"\n",
    "    # On note f1 et f2 les composantes de F\n",
    "    f1 = gradient_g(x) + C.T.dot(mu)\n",
    "    f2 = C.dot(x) - d\n",
    "    f = np.hstack((f1, f2))  # on concatene f1 et f2\n",
    "    return f\n",
    "    \n",
    "def jac_F(x, mu):\n",
    "    \"\"\"Compute the jacobian of F.\"\"\"\n",
    "    # on crée une matrice de taille (n + p) x (n + p)\n",
    "    J = np.zeros((n + p, n + p))\n",
    "    J[:n, :n] = hessian_g(x)\n",
    "    J[:n, n:] = C.T\n",
    "    J[n:, :n] = C\n",
    "    \n",
    "    return J\n",
    "\n",
    "def newton_constrained(xmu0, F=F, jac=jac_F, maxiter=10, verbose=True):\n",
    "    \"\"\"Solve constrained min g with newton method\"\"\"\n",
    "    \n",
    "    xmu = xmu0.copy()\n",
    "    x = xmu[:n]\n",
    "    mu = xmu[n:]\n",
    "    if verbose:\n",
    "        strings = [\"Iteration\", \"max(abs(F(x_k, mu_k)))\"]\n",
    "        strings = [s.center(13) for s in strings]\n",
    "        strings = \" | \".join(strings)\n",
    "        print(strings)\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        J = jac(x, mu)\n",
    "        f = F(x, mu)\n",
    "        \n",
    "        if verbose:\n",
    "            strings = [i, abs(f).max()] # On affiche des trucs \n",
    "            strings = [str(s).center(13) for s in strings]\n",
    "            strings = \" | \".join(strings)\n",
    "            print(strings)\n",
    "        \n",
    "        # A faire\n",
    "        xmu = xmu + np.linalg.solve(J, - f)\n",
    "        x = xmu[:n]\n",
    "        mu = xmu[n:]\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration   | max(abs(F(x_k, mu_k)))\n",
      "      0       | 159.44416332283464\n",
      "      1       | 9.542019951958025e-14\n",
      "      2       | 7.212980213111564e-15\n",
      "      3       | 4.5449755070592346e-15\n",
      "      4       | 7.369105325949477e-15\n",
      "      5       | 7.271960811294775e-15\n",
      "      6       | 6.04572815421367e-15\n",
      "      7       | 6.25888230132432e-15\n",
      "      8       | 4.246603069191224e-15\n",
      "      9       | 6.300515664747763e-15\n"
     ]
    }
   ],
   "source": [
    "xmu0 = rnd.randn(n + p)\n",
    "x = newton_constrained(xmu0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.38777878e-16,  2.22044605e-16, -1.11022302e-16,  3.67761377e-16,\n",
       "       -2.22044605e-16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.dot(x) - d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
