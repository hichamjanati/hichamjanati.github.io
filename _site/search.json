[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "teaching_sb.html",
    "href": "teaching_sb.html",
    "title": "Statistiques Bayésiennes",
    "section": "",
    "text": "Slides\n\nChapter 1: Bayesian thinking and bayesian models | pdf | browser\nChapter 2: Markov-Chain Monte-Carlo algorithms | pdf | browser\nChapter 3: Hierarchical models and Bayesian machine learning | pdf | browser\n\n\n\nTD/TP\n\nTD1: Bayesian models (pdf)\nTP2: MCMC diagnostics (pdf)\nTP3: Intro to PyMC (notebook)\nTP4: Hierarchical models (I): bayesian regression (notebook)\nTD5: Hierarchical models (II): Capture & recapture (pdf) (python code)\nTD6: Bayesian ML: logistic regression (pdf) (solutions pdf)\nTP7: Bayesian ML: Logistic regression (notebook in progress)\n\n\n\nAre you ready for the exam ?\n\nGeneral anonymous quiz\n\n\n\nFeedback\nIf you took this course, please give your anonymous feedback to improve it: google form."
  },
  {
    "objectID": "teaching_sb.html#slides",
    "href": "teaching_sb.html#slides",
    "title": "Statistiques Bayésiennes",
    "section": "",
    "text": "Chapitre 1: pdf (no animation) animated html\nChapitre 2: pdf (no animation) animated html\nChapitre 3: pdf (no animation) animated html"
  },
  {
    "objectID": "teaching_sb.html#tdtp",
    "href": "teaching_sb.html#tdtp",
    "title": "Statistiques Bayésiennes",
    "section": "TD/TP",
    "text": "TD/TP\n\nTD1: Bayesian models (pdf)\nTP2: MCMC diagnostics (Gibbs) (pdf)\nTP3: Intro to PyMC (notebook)\nTP4: Hierarchical models and bayesian regression (notebook)\nTD5: Capture Recapture model in ecology (pdf) (python code)\nTD6: Logistic regression frequentist vs bayesian (pdf) (solutions pdf)\nTP7: Logistic regression (notebook in progress)"
  },
  {
    "objectID": "teaching_sb.html#open-anonymous-tests",
    "href": "teaching_sb.html#open-anonymous-tests",
    "title": "Statistiques Bayésiennes",
    "section": "Open anonymous tests",
    "text": "Open anonymous tests\n\nGeneral quiz"
  },
  {
    "objectID": "teaching_sb.html#feedback",
    "href": "teaching_sb.html#feedback",
    "title": "Statistiques Bayésiennes",
    "section": "Feedback",
    "text": "Feedback\nAnonymous feedback is always welcome and very much appreciated: fill the form."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Publications\nThis is list is rarely maintained, for an up-to-date list check my scholar profile.\n\nConference papers\n\nH. Janati, B. Muzellec, G. Peyré and M. Cuturi. Entropic Optimal Transport between (Unbalanced) Gaussian Measures has a Closed Form (Neurips, 2020). (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Debiased Sinkhorn barycenters (ICML 2020) (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Spatio-Temporal Alignments: Optimal transport through space and time. (AISTATS, 2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Group-level EEG / MEG source imaging via Optimal Transport: minimum Wasserstein estimates. IPMI 2019. (paper) | (Python code)\nT. Bazeille, H. Richard, H. Janati, B. Thirion, Local Optimal Transport for Functional Brain Template Estimation. IPMI 2019\nH. Janati, M. Cuturi and A. Gramfort. Wasserstein regularization for sparse multitask regression. AISTATS 2019. (paper) | (Python code)\n\n\n\nJournal papers\n\nJ. Faouzi, H. Janati. pyts: A Python package for time series classification. Journal of Machine Learning Research (2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Multi-subject MEG/EEG source imaging with sparse multi-task regression (NeuroImage, 2019) (paper)\n\n\n\nOther\n\nH. Janati, Investigating cancer resistance in a Glioblastoma cell line with gene expression data. Statistics [stat]. 2016. [Best (paper) award - ENSAE 2015/2016] (paper)\n\n\n\n\nReviewing service\n\nNeurIPS (2019, 2020, 2021, 2022, 2023)\nICML (2020, 2021, 2022, 2023, 2024)\nNeuroImage (2019, 2020)\nIEEE TNNLS (2020)\nIEEE SPL (2020)"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "This is list is rarely maintained, for an up-to-date list check my scholar profile.\n\n\n\nH. Janati, B. Muzellec, G. Peyré and M. Cuturi. Entropic Optimal Transport between (Unbalanced) Gaussian Measures has a Closed Form (Neurips, 2020). (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Debiased Sinkhorn barycenters (ICML 2020) (paper) | (Python code)\nH. Janati, M. Cuturi and A. Gramfort. Spatio-Temporal Alignments: Optimal transport through space and time. (AISTATS, 2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Group-level EEG / MEG source imaging via Optimal Transport: minimum Wasserstein estimates. IPMI 2019. (paper) | (Python code)\nT. Bazeille, H. Richard, H. Janati, B. Thirion, Local Optimal Transport for Functional Brain Template Estimation. IPMI 2019\nH. Janati, M. Cuturi and A. Gramfort. Wasserstein regularization for sparse multitask regression. AISTATS 2019. (paper) | (Python code)\n\n\n\n\n\nJ. Faouzi, H. Janati. pyts: A Python package for time series classification. Journal of Machine Learning Research (2020) (paper) | (Python code)\nH. Janati, T. Bazeille, B. Thirion, M. Cuturi and A. Gramfort. Multi-subject MEG/EEG source imaging with sparse multi-task regression (NeuroImage, 2019) (paper)\n\n\n\n\n\nH. Janati, Investigating cancer resistance in a Glioblastoma cell line with gene expression data. Statistics [stat]. 2016. [Best (paper) award - ENSAE 2015/2016] (paper)"
  },
  {
    "objectID": "research.html#reviewing-service",
    "href": "research.html#reviewing-service",
    "title": "Research",
    "section": "Reviewing service",
    "text": "Reviewing service\n\nNeurIPS (2019, 2020, 2021, 2022, 2023)\nICML (2020, 2021, 2022, 2023, 2024)\nNeuroImage (2019, 2020)\nIEEE TNNLS (2020)\nIEEE SPL (2020)"
  },
  {
    "objectID": "teaching.html#école-polytechnique",
    "href": "teaching.html#école-polytechnique",
    "title": "Teaching",
    "section": "École Polytechnique",
    "text": "École Polytechnique\n\nPython for data science (Master DS for business X/HEC 2021)\nStatistical learning theory (Master data science 2022, 2023)\nOptimization algorithms for machine learning (Master data science 2023)"
  },
  {
    "objectID": "teaching.html#new-york-university",
    "href": "teaching.html#new-york-university",
    "title": "Teaching",
    "section": "New York University",
    "text": "New York University\n\nIntroduction to machine learning (2022, 2023)"
  },
  {
    "objectID": "teaching.html#télécom-paris",
    "href": "teaching.html#télécom-paris",
    "title": "Teaching",
    "section": "Télécom Paris",
    "text": "Télécom Paris\n\nAdvanced Statistics (Cycle ingénieur 2022, 2023, 2024)\nNumerical calculus and Monte-Carlo methods (Cycle ingénieur 2022, 2023, 2024)\nAdvanced machine learning (Master spécialisé Big Data AI 2022, 2023)\nIntroduction to Machine learning (Executive education 2022, 2023, 2024)"
  },
  {
    "objectID": "teaching.html#ensae-sorbonne-université-teaching-assistant-during-phd",
    "href": "teaching.html#ensae-sorbonne-université-teaching-assistant-during-phd",
    "title": "Teaching",
    "section": "ENSAE / Sorbonne Université [Teaching assistant during PhD]",
    "text": "ENSAE / Sorbonne Université [Teaching assistant during PhD]\n\nOptimisation différentiable (2017, 2018, 2019) - ENSAE 1A\nProbability theory (2018) - ENSAE 1A\nMonte-Carlo methods (2017, 2018, 2019) - ENSAE 2A\nProgrammation en C - La Sorbonne Université (2017)"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "INSEA\nInstitut National de Statistique et d’Économie Appliquée (INSEA)\nB.P. 6217, Madinat Al Irfane,\nRabat, Morocco\nhjanati@insea.ac.ma\n\n\nTélécom Paris\n19 Place Marguerite Perey,\n91120 Palaiseau, France\nhicham.janati@telecom-paris.fr\nYou’re a student and want to share feedback anonymously ? Here’s a google form you can fill."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hicham Janati",
    "section": "",
    "text": "I’m an associate professor at Insea (Rabat) and Télécom Paris. My research and interests gravitate around machine learning with a focus on optimal transport and diffusion models. I’m passionate about exploring ideas deeply and sharing them clearly. This site [in construction] is where I document and share what I’m working on.\n\n\n\nShort Bio\nIn 2021, I obtained both my Master’s and PhD degrees from ENSAE Paris, where I worked under the supervision of Alexandre Gramfort and Marco Cuturi on optimal transport and its applications in machine learning. My thesis manuscript is available here. Following my PhD, I held a postdoctoral position at École Polytechnique, working with Rémi Flamary on deep learning for domain adaptation. I have been an Associate Professor at Télécom Paris since December 2021, and I joined INSEA in the same role in January 2025.\n\n\nNews\n\n\n\nDate\nEvent\n\n\n\n\nFeb 2025\nSpatio-Temporal Alignments accepted in Journal of Machine Learning Research (JMLR)\n\n\nJan 2025\nJoined Insea as Associate Professor\n\n\nJun 2023\nUnbalanced Co-Optimal Transport accepted at AAAI Conference\n\n\nDec 2021\nJoined Télécom Paris as Associate Professor\n\n\nApr 2021\nStarted postdoc at École Polytechnique in Rémi Flamary’s lab\n\n\nMar 2021\nDefended PhD — manuscript\n\n\nDec 2020\nPresented OT Closed-Form for Gaussians at NeurIPS 2020 (Orals, Virtual)\n\n\nJul 2020\nPresented Debiased Sinkhorn Barycenters at ICML 2020 (Spotlight, Virtual)\n\n\nApr 2020\nPresented Spatio-Temporal OT at AISTATS 2020 (Virtual)\n\n\nJun 2019\nPresented Local OT for Brain Template Estimation at IPMI 2019 (Hong Kong)\n\n\nApr 2019\nPresented Wasserstein Regularization at AISTATS 2019 (Okinawa, Japan)"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Hicham Janati",
    "section": "News",
    "text": "News\n\nFebruary 2025 — Spatio-Temporal Alignments accepted for publication in the Journal of Machine Learning Research (JMLR).\n\nJanuary 2025 — Appointed as Associate Professor at INSEA.\n\nJune 2023 — Unbalanced Co-Optimal Transport accepted at the AAAI Conference on Artificial Intelligence.\n\nDecember 2021 — Joined Télécom Paris as Associate Professor.\n\nApril 2021 — Started a postdoctoral position at CMAP, École Polytechnique, in Rémi Flamary’s lab.\n\nMarch 2021 — Successfully defended my PhD. Manuscript.\n\nDecember 2020 — Presented OT Closed-Form for Gaussians at the NeurIPS 2020 (Orals, Virtual).\n\nJuly 2020 — Presented Debiased Sinkhorn Barycenters at ICML 2020 (Spotlight, Virtual).\n\nApril 2020 — Presented Spatio-Temporal OT at AISTATS 2020 (Posters, Virtual).\nJune 2019 - Presented Local OT for functional brain template estimation at IPMI 2019 (Posters, Hong Kong)\nApril 2019 - Presented Wasserstein regularization for sparse multi-task regression at AISTATS 2019 (Okinawa, Japan)"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Hicham Janati",
    "section": "Bio",
    "text": "Bio\nI obtained my PhD at Inria in the Parietal team, working under supervision of Alexandre Gramfort and Marco Cuturi. My thesis manuscript can be found here."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "INSEA\n\nStatistiques Bayésiennes (2025)\n\n\n\nÉcole Polytechnique\n\nPython for data science (Master DS for business X/HEC 2021)\nStatistical learning theory (Master data science 2022, 2023)\nOptimization algorithms for machine learning (Master data science 2023)\n\n\n\nNew York University - (Paris)\n\nIntroduction to machine learning (2022, 2023)\n\n\n\nTélécom Paris\n\nAdvanced Statistics (Cycle ingénieur 2022, 2023, 2024)\nNumerical calculus and Monte-Carlo methods (Cycle ingénieur 2022, 2023, 2024)\nAdvanced machine learning (Master spécialisé Big Data AI 2022, 2023)\nIntroduction to Machine learning (Executive education 2022, 2023, 2024)\n\n\n\nENSAE / Sorbonne Université [Teaching assistant during PhD]\n\nOptimisation différentiable (2017, 2018, 2019) - ENSAE 1A\nProbability theory (2018) - ENSAE 1A\nMonte-Carlo methods (2017, 2018, 2019) - ENSAE 2A\nProgrammation en C - La Sorbonne Université (2017)"
  },
  {
    "objectID": "media/Opti/tpnewton.html",
    "href": "media/Opti/tpnewton.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/Opti/tpnewton.html#méthode-de-newton",
    "href": "media/Opti/tpnewton.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad. ##### Question 4 Lorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).\n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    \ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = \n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(...):\n    \"\"\"Compute the function F.\"\"\"\n    # A faire\n    \ndef jac_F(x):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # A faire\n\ndef newton_constrained( ):\n    # A faire"
  },
  {
    "objectID": "media/teaching/Opti/tpnewton.html",
    "href": "media/teaching/Opti/tpnewton.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/teaching/Opti/tpnewton.html#méthode-de-newton",
    "href": "media/teaching/Opti/tpnewton.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad. ##### Question 4 Lorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).\n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    \ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = \n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(...):\n    \"\"\"Compute the function F.\"\"\"\n    # A faire\n    \ndef jac_F(x):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # A faire\n\ndef newton_constrained( ):\n    # A faire"
  },
  {
    "objectID": "media/teaching/SB/td3_intro_pymc.html",
    "href": "media/teaching/SB/td3_intro_pymc.html",
    "title": "\nTP3: Introduction à PyMC\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Introduction à PyMC\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats et comparer avec les statistiques fréquentistes\n\n\n%pip install pymc arviz numpy pandas matplotlib ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pymc as pm\nimport arviz as az\n\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\nModèle Poisson-Gamma simple\nOn considère l’ex 1 du TD1. On observe le nombre de sinistres par année \\(N | \\lambda \\sim \\mathcal{P}(\\lambda)\\), où \\(\\lambda\\) suit une loi a priori \\(\\text{Gamma}(a, b)\\). On suppose que les données historiques mènent au choix a = 4 et b = 2. On utilise la définition de Gamma où b correspond au “rate” et non pas au scale (comme en TD), la moyenne de cette Gamma est a/b. (voir wikipedia).\nOn suppose que les 5 observations sont données par:\n\ndata = np.array([4, 0, 2, 1, 0])\nrng = np.random.default_rng(42)\nlambda_true = 2\ndata = rng.poisson(lambda_true, size=5)\n\nOn définit le modèle bayésien dans un contexte avec pymc ou on précise la distribution, le nom et les paramètres de chaque variable\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Gamma(\"lambda\", a, b) # non observée\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\nOn voit que pymc a automatiquement choisi NUTS et a simulé 4 chaînes avec 2000 échantillons chacune dont 1000 jetés (tuning / burn-in). Voyons ce que contient l’objet trace:\n\ntrace\n\nC’est un objet InferenceData du package arviz. On peut obtenir les échantillons simulés dans l’attribut posterior:\n\ntrace.posterior[\"lambda\"].data\n\n\ntrace.posterior[\"lambda\"].data.shape\n\nOn a effectivement généré 4 chaines avec 1000 échantillons chacune. On peut les visualiser:\n\nplt.figure()\nplt.plot(trace.posterior[\"lambda\"].data.T)\nplt.grid(True)\nplt.title(\"Trace plot\")\nplt.show()\n\nOu utiliser la librairie arviz directement qui donne également une estimation de la densité a posteriori:\n\naz.plot_trace(trace)\n\nOn fait le diagnostic de convergence:\n\naz.plot_autocorr(trace)\nplt.show()\n\n\naz.summary(trace)\n\n\naz.summary(trace)\n\n\nLes tracés sont bien mélangés\nRhat = 1.0 &lt; 1.01 : pas de différence significative entre les 4 chaines\nESS très larges\nAutocorrélations diminuent très rapidement\n\nCette chaîne réussit les diagnostics de convergence.\nOn peut visualiser la densité a posteriori avec l’intervalle de crédibilité HDI:\n\naz.plot_posterior(trace, hdi_prob=0.94)\n\nLes bornes de cet intervalle sont également présente dans le tableau du summary ci-dessus. On peut calcule un HDI directement:\n\naz.hdi(trace, hdi_prob=0.94).to_pandas()\n\nOn peut calculer un ESS relatif (divisé par le nombre d’échantillons):\n\naz.ess(trace, method=\"bulk\", relative=True).to_pandas()\n\nAinsi, on en déduit que 44% des échantillons “sont efficaces” pour estimer “le centre” (bulk) de la distribution\n\nQuestion 1: Augmenter le nombre de sinistres observés de 5 à 10 puis 100, comment changent les statistiques du az.summary ?\n\n\nQuestion 2: Les métriques de convergence sont-elle très différentes ? Est-ce surprenant ?\n\n\nQuestion 3: Comment peut-on interpréter le HDI obtenu ?\n\n\nQuestion 4: En utilisant le fait que la loi a priori soit conjuguée, générez des échantillons a posteriori directement (sans pymc) et comparez\nLa loi a posteriori est \\(\\text{Gamma}(a + \\sum_{i=1}^n N_i, b + n)\\)\n\nlambda_post_samples = rng.gamma(a + data.sum(), 1/(b + len(data)), size=(4, 1000))\n\nax = az.plot_posterior(trace)\nax.hist(lambda_post_samples, bins=100, density=True, alpha=0.7)\nplt.show()\n\n\ntrace_iid = az.convert_to_inference_data(dict(iid=lambda_post_samples))\naz.summary(trace_iid)\n\n\nlambda_post_samples.mean()\n\n\naz.summary(trace)\n\n\n\n\nModèle Poisson-Gamma à plusieurs conducteurs\nOn considère désormais les données de plusieurs individus avec un \\(\\lambda_i\\) différent mais un a priori commun. Chaque conducteur \\(i\\) a ses données \\(N_i^1, \\dots, N_i^m\\). Le modèle pymc s’adapte facilement en changeant le shape des paramètres:\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Gamma(\"lambda\", a, b, shape=n_drivers) # non observée, on précise le nombre de lambda\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n    pm.compute_log_likelihood(trace)\n\nL’objet trace contient désormais plusieurs variables lambda:\n\nprint(az.summary(trace))\n\n\nQuestion 4: Comparez l’estimation fréquentiste avec l’estimation bayésienne avec (a, b) = (4, 2) puis (a, b) = (10, 1).\n\ndata.mean(0)\n\n\n\nQuestion 5: Déterminez un intervalle de confiance fréquentiste de niveau 95% pour chaque \\(\\lambda_i\\).\n\\([\\bar{N} \\pm \\frac{Q_{97.5}\\hat{\\sigma}}{\\sqrt{n}}]\\) avec \\(Q_{97.5}\\) le quantile de la loi de student à n-1 degrés de liberté.\n\nfrom scipy.stats import t\n\nn = 5\nq975 = t.ppf(0.975, df=n-1)\nsigma = data.std(0)\n\ndata.mean(0) - q975 * sigma / n ** 0.5, data.mean(0) + q975 * sigma / n ** 0.5, \n\n\naz.summary(trace)\n\n\n\nQuestion 6: On reprend à présent une loi a priori Uniforme([0, 5]). Déterminez des HDI de niveau 95% pour chaque \\(\\lambda_i\\). Comment se comparent-ils aux intervalles de confiance fréquentistes ?\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model:\n    lambda_ = pm.Uniform(\"lambda\", 0, 5, shape=n_drivers) # non observée, on précise le nombre de lambda\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\n\ndata = np.array([[4, 0, 0],\n                [0, 2, 1],\n                [2, 2, 0],\n                [1, 3, 0],\n                [0, 0, 1]])\n\n# donnéees de 3 conducteurs\n\nn_drivers = data.shape[1]\n\na = 4\nb = 2\nwith pm.Model() as model_unif:\n    # lambda_ = pm.Gamma(\"lambda\", a, b, shape=n_drivers) # non observée, on précise le nombre de lambda\n    lambda_ = pm.Uniform(\"lambda\", 0, 100, shape=n_drivers)\n    # le shape des données par défaut est n_observation x n_features, pymc associe chaque lambda_i a une colonne de data\n    N_obs = pm.Poisson(\"N_obs\", mu=lambda_, observed=data, ) # observée\n    trace_unif = pm.sample(1000) # on simule une chaine MCMC de la loi a posteriori\n\n\naz.summary(trace_unif)\n\n\ndata.mean(0) - q975 * sigma / n ** 0.5, data.mean(0) + q975 * sigma / n ** 0.5,"
  },
  {
    "objectID": "media/teaching/SB/td4_hierarchical_model.html",
    "href": "media/teaching/SB/td4_hierarchical_model.html",
    "title": "\nTP3: Modèles Bayésiens Hiérarchiques (I)\n",
    "section": "",
    "text": "Insea 2025             Statistiques Bayésiennes \n\n\nTP3: Modèles Bayésiens Hiérarchiques (I)\n\n                 Author: Hicham Janati \n\n\nObjectifs:\n\nDécouvrir la librairie PyMC\nImplémenter les premiers modèles bayésiens et faire le diagnostic de convergence\nInterpréter les résultats\n\n\n# Installation si nécessaire\n!pip install pymc arviz numpy pandas matplotlib seaborn ipywidgets\n\nOn importe les librariries et un crée un générateur aléatoire:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\n# Configuration pour de meilleurs graphiques\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_context('notebook')\n\nseed = 42\nrng = np.random.default_rng(seed)\n\n\n\n1. Modèle Poisson-Gamma hiérarchique\nDans le cadre de la modélisation du nombre de sinistres, il n’est pas pratique de considérer un \\(\\lambda_i\\) spécifique à chaque individu car les données individuelles contiennent très souvent très peu d’observations. Ici, on souhaite donc regrouper les assurés en utilisant leurs informations individuelles. Nous avons une variable age qui donne l’âge du conducteur en 4 catégories:\n\nage = 0 (&lt; 30 ans)\nage = 1 (entre 30 et 50 ans)\nage = 2 (entre 50 et 60 ans)\nage = 3 (supérieur à 60 ans)\n\nPour tenir compte des différences entre les catégories, on modélise chaque catégorie par un taux de sinistre spécifique \\(\\textcolor{red}{\\lambda_j}\\) avec \\(j\\in \\{0, 1, 2, 3\\}\\). Pour prendre en compte leur similarité, les \\(\\textcolor{red}{\\lambda_j}\\) sont modélisés avec une loi a priori commune \\(\\text{Gamma}(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta})\\).\n\nSi des données historiques peuvent être utilisées, alors \\(\\textcolor{purple}{\\alpha}\\) et $ $ sont choisis (constantes a priori) avec la méthode des moments (comme en TD1)\nSinon, on les modélise comme des variables aléatoires avec une loi apriori \\(\\pi\\) assez vague (grande variance, ou uniforme).\n\nLe deuxième cas définit une structure bayésienne à deux niveaux: 1. Le nombre de sinistre \\(\\textcolor{blue}{N}\\) dépend de \\(\\textcolor{red}{\\lambda_j}\\): \\(\\textcolor{blue}{N} | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\) 2. Le taux de sinistre \\(\\textcolor{red}{\\lambda_j}\\) dépend de $ $ et $ : | , (, )$ avec \\(\\textcolor{purple}{\\alpha}\\) et \\(\\textcolor{purple}{\\beta}\\) et \\(\\textcolor{purple}{\\alpha},  \\textcolor{purple}{\\beta} \\sim \\pi\\).\nC’est un modèle hiérarchique. On considère une loi a priori Uniforme(0, 10).\nVoici les données (extraites et filtrées à partir de https://www.kaggle.com/datasets/saisatish09/insuranceclaimsdata?select=dataCar.csv)\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_age.csv\", index_col=0)\nprint(df.shape)\ndf.head()\n\nNous avons donc les données de 10205 assurés. On peut commencer par voir la taille de chaque groupe:\n\ndf.age.value_counts()\n\n\nQuestion 1: On remarque que la catégorie d’âge 1 (30-40 ans) est la plus grande avec 4610 assurés. Celle des &gt; 60 ans est la plus petite avec 1400 assurés. À quoi peut-on s’attendre concernant la qualité de l’estimation de chaque \\(\\textcolor{red}{\\lambda_j}\\) ?\nOn regarde la distribution du nombre de sinistre déclarés par assuré:\n\ndf.numclaims.value_counts()\n\n\n\nQuestion 2: On remarque que plus de 90% des assurés ne déclarent jamais de sinistres. Seulement 50/10250 ont déclaré 2 ou 3 sinistres. Quel est l’ordre de grandeur (ou fourchette de valeurs) des \\(\\textcolor{red}{\\lambda_j}\\) auquel on peut s’attendre ?\n\n\nQuestion 3: On note les nombres de sinistres de chaque groupe d’âge \\(j\\) par \\(\\textcolor{blue}{N}_1^j, \\dots, \\textcolor{blue}{N}_{n_j}^j | \\textcolor{red}{\\lambda_j} \\sim \\mathcal{P}(\\textcolor{red}{\\lambda_j})\\). Ainsi, d’après la distribution ci-dessus des catégories d’âge: \\(n_0 = 2377\\), \\(n_1 = 4610\\) etc… Représentez le graphe probabiliste de ce modèle et déterminez la formule de la loi a posteriori jointe en fonction des lois de l’énoncé.\n\n\nQuestion 4: Implémentez le modèle hiérarchique avec pymc et faites le diagnostic MCMC\n\n\nQuestion 5: Calculez les bonnes probabilités de type \\(\\mathbb P(\\textcolor{red}{\\lambda_j} &lt; \\textcolor{red}{\\lambda_k})\\) pour déterminer si certains groupes d’âge ont des risques différents ou non. Commenter\n\n\n\n2. Bayesian Poisson regression\nEn plus de l’âge du conducteur, nous avons également la catégorie d’âge du véhicule (veh_age) et la valeur du véhicule en ‘$’ (veh_value) (divisée par 10000). Une structure bayésienne hiérarchique n’est plus possible (à moins de diviser la variable veh_value en catégories et créer tous les croisements de catégories age x veh_age x veh_value possibles chacune avec son taux \\(\\textcolor{red}{\\lambda_j}\\)). Une meilleur approche est de considérer une regression linéaire où on prédit le taux de sinistre avec une combinaison linéaire des variables: \\(\\textcolor{red}{\\lambda} = \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\). Or \\(\\textcolor{red}{\\lambda} &gt; 0\\), ce qui n’est pas respecté ici. On utilise un modèle linéaire généralisé où c’est \\(\\log(\\textcolor{red}{\\lambda})\\) qui est expliquée:\n\\[ N | \\textcolor{red}{\\lambda} \\sim \\mathcal P(\\textcolor{red}{\\lambda})\\] \\[\\log(\\textcolor{red}{\\lambda}) =  \\textcolor{blue}{\\beta_0} + \\textcolor{blue}{\\beta_1} \\text{age} + \\textcolor{blue}{\\beta_2} \\text{veh\\_age} + \\textcolor{blue}{\\beta_3} \\text{veh\\_value}\\]\nAvec une loi a priori \\(\\textcolor{blue}{\\beta_0}, \\textcolor{blue}{\\beta_1}, \\textcolor{blue}{\\beta_2}, \\textcolor{blue}{\\beta_3} \\sim \\mathcal{N}(0, 1)\\).\n\nQuestion 6: On suppose que \\(\\textcolor{blue}{\\beta_1} = 0.2\\). Si toutes les variables sauf l’âge ne changent pas, quel est l’effet de passer à une catégorie d’âge supérieur (càd que l’âge passe de 0 à 1, ou 1 à 2 ou 2 à 3) sur \\(\\textcolor{red}{\\lambda}\\) ? Répondez à la question en terme de pourcentage de changement.\n\ndf = pd.read_csv(\"http://hichamjanati.github.io/data/claims_reg.csv\", index_col=0)\ndf.head()\n\n\n\nQuestion 7: Complétez le modèle bayésien ci-dessous et faites le diagonostic de convergence. En pratique, on définit \\(\\textcolor{red}{\\lambda}\\) comme l’exp de la combinaison linéaire.\n\ncoords = dict(var_name=[\"intercept\", \"age\", \"veh_age\", \"veh_value\"]) # dictionnaire qui sert à nommer les variables\nwith pm.Model(coords=coords) as reg_model:\n    # beta = pm.Normal(\"beta\", mu=0, sigma=1, dims=\"var_name\") # vecteur des betas de taille 4 nommé selon \"var_name\" de coords\n    # TO DO\n    #\n    #\n    # lambda_ =\n    lambda_ = pm.Gamma(\"lambda\", 0.1, 0.1)\n    N = pm.Poisson(\"N\", mu=lambda_, observed=df[\"numclaims\"])\n    trace = pm.sample()\n\n\n\nQuestion 8: Interprétez les valeurs et HDI obtenus pour chaque coefficient de regression \\(\\beta_i\\).\n\naz.summary(trace)\n\n\n\nQuestion 9: On souhaite vérifier que le modèle fit bien les données. Pour cela on peut utiliser les échantillons MCMC (\\(\\beta\\)) pour générer des données \\(N_i\\) (pm.sample_posterior_predictive) et comparer la vraisemblance avec la distribution des données générées. Qu’en pensez-vous ?\n\nwith reg_model:\n    pm.sample_posterior_predictive(trace, extend_inferencedata=True) # ce paramètre = True fait qu'on modifie l'objet `trace` en rajoutant les samples de la predictive posterior\naz.plot_ppc(trace)\n\n\n\nQuestion 10 Now time to break it ! Pour bien cerner ce qui explique le bon fit des données de la question précédente, réduisez la complexité du modèle (le simplifier en enlevant des variables) jusqu’à ce que la fonction prédictive s’éloigne des données. Que peut-on en déduire ?\n\n\nQuestion 11: Vus les résultats obtenus, il se peut que certains coefficients soient biaisés par le choix restrictif de l’a priori Gaussien avec variance égale à 1. On considère à présent \\(\\sigma\\) comme une variable aléatoire avec un a priori gaussienne positive (tronquée, pm.HalfNormal) avec un hyperparamètre \\(\\sigma = 1\\). Implémentez ce modèle. les résultats ont-ils changé considérablement ? Interpréter.\n\n\nQuestion 12: On peut évaluer la qualité de ces modèles (et choisir le meilleur) avec le critère bayésien LOO (Leave-one-out).\nLOO consiste à évaluer la log-vraisemblance de prédiction d’un échantillon \\(i\\) après l’avoir enlevé des données, autrement dit, si on note \\(N_1, \\dots, N_n\\) les observations, alors \\(N_{-i}\\) représente toutes les données sauf \\(N_i\\), on note donc \\(N_{-i} = \\{N_1, \\dots, N_{i-1}, N_{i+1}, \\dots N_n\\}\\). La fonction de prédiction (en log-probabilité) pour des données nouvelles est dit “expected log predictive density (ELPD)”: \\[ ELPD_i = \\log p(N_i | N_{-i}) \\] Le critère LOO évalue la log-vraisemblance pour tous les échantillons (qui sont enlevés et prédits avec le reste à tour de rôle): \\[  ELPD = \\sum_{i=1}^n ELPD_i = \\sum_{i=1}^n \\log p(N_i | N_{-i}) \\] Avec la loi des probabilités totale: \\[ p(N_i | N_{-i}) = \\int p(N_i | \\beta, N_{-i}) p(\\beta | N_{-i}) \\mathrm d\\beta = \\int p(N_i | \\beta) p(\\beta | N_{-i}) \\mathrm d\\beta \\]\nCette intégrale est approchée par Importance sampling (IS) avec la loi a posteriori full \\(p(\\beta | N_1, \\dots N_n)\\) et une approximation des poids IS avec la loi de Pareto généralisée qui doit avoir des moments finies sinon IS est instable. Sa variance est finie si son paramètre (scale) \\(k &lt; 0.5\\). Sa moyenne est finie si \\(k &lt; 1\\). arviz nous donne l’estimation du ELPD ainsi que la qualité de l’estimation (k pour chaque \\(i\\)). Il faut d’abord calculer les loglikelihoods avec pm:\n\nwith reg_model:\n    pm.compute_log_likelihood(trace)\naz.plot_loo(trace)\n\nPour comparer tous les modèles vus dans ce notebook, on peut utiliser la fonction az.compare qui prend en argument un dictionnaire avec des noms des modèles en keys et les objets models en valeurs. Complétez ce code avec vos modèles et analysez, arviz trie les modèles par défaut du meilleur au pire:\n\nmodels_dict = {\"reg_model\": reg_model, ...}\naz.compare(models_dict)"
  },
  {
    "objectID": "media/Opti/tpnewtoncorr.html",
    "href": "media/Opti/tpnewtoncorr.html",
    "title": "TP Optimisation Différentiable",
    "section": "",
    "text": "ENSAE, Avril 2018\nimport numpy as np\nimport scipy\nfrom matplotlib import pyplot"
  },
  {
    "objectID": "media/Opti/tpnewtoncorr.html#méthode-de-newton",
    "href": "media/Opti/tpnewtoncorr.html#méthode-de-newton",
    "title": "TP Optimisation Différentiable",
    "section": "Méthode de Newton",
    "text": "Méthode de Newton\n\n1.1 Introduction\nSoit \\(f\\) une fonction de classe \\(\\mathcal{C}^1\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}^n\\). Le but de la méthode de Newton est de résoudre \\(f(x) = 0\\). Soit \\(x_0\\) dans \\(\\mathbb{R}^n\\). L’approximation de premier ordre de \\(f\\) dans un voisinage de \\(x_0\\) est donnée par: \\[ \\hat{f}(x) = f(x_0) + J(x_0)(x - x_0) \\] Oû \\(J\\) est la matrice Jacobienne de f. Annuler la tangeante \\(\\hat{f}\\) donne \\(x = x_0 - J^{-1}f(x_0)\\) et obtient donc la suite d’itérés: \\[x_k = x_{k-1} - J^{-1}f(x_{k-1})\\]\n\nQuestion 1\nSoit \\(x^{\\star}\\) un zéro de \\(f\\). Supposons que \\(J(x^{\\star})\\) est inversible et que \\(f\\) est de classe \\(\\mathcal{C}^2\\). Montrez que la méthode de Newton a une convergence localement quadratique i.e qu’il existe une boule B centrée en \\(x^{\\star}\\) telle que pour tout \\(x_0\\) dans B, il existe \\(\\alpha &gt; 0\\) telle que la suite de Newton vérifie: \\[ \\|x_{k+1} - x^{\\star}\\| &lt; \\alpha \\|x_{k} - x^{\\star}\\|^2 \\] ###### indication: Écrire l’approximation de deuxième ordre de f avec reste intégral.\n\nDeux remarques importantes: - Newton est basée sur une approximation locale. La solution obtenue dépend donc du choix de \\(x_0\\). - \\(J\\) doit être inversible.\n\n\n\n\n1.2 Optimisation sans contraintes\nSoit \\(g\\) une fonction de classe \\(\\mathcal{C}^2\\) de \\(\\mathbb{R}^n\\) dans \\(\\mathbb{R}\\).\n\nQuestion 2\nAdapter la méthode de Newton pour résoudre \\(\\min_{x \\in \\mathbb{R}^n} g(x)\\).\n  Comme il s’agit d’un problème sans contraintes, on peut appliquer la méthode de Newton à \\(\\nabla g\\) pour résoudre \\(\\nabla g(x) = 0\\). (A priori, on converge donc vers un point critique de \\(g\\).) \n  Pour les questions 3-4-5, On prend \\(g: x \\mapsto \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x\\|^2\\), avec \\(A \\in \\mathcal{M}_{m, n}(\\mathbb{R})\\), $b ^m $ et $ $\n\n\nQuestion 3\nDonner le gradient et la hessienne de g et complétez les fonctions gradient et hessian ci-dessous. Vérifiez votre gradient avec l’approximation numérique donnée par scipy.optimize.check_grad.\n  Pour h dans un voisinage de x, on développe \\(g(x + h) = g(x) + \\langle A^{\\top}(Ax - b) + 2\\gamma x, h\\rangle + \\frac{1}{2}h^{\\top}(A^{\\top}A + 2 \\gamma I_n) h\\). Ainsi, par identification de la partie linéaire en h: \\(\\nabla g(x) = A^{\\top}(Ax - b) + 2\\gamma x\\) Et comme \\(A^{\\top}A + 2 \\gamma I_n\\) est symmétrique: \\(\\nabla^2 g(x) = A^{\\top}A + 2 \\gamma I_n\\) \n\n\nQuestion 4\nLorsque \\(\\gamma &gt; 0\\), montrez que la méthode de Newton converge en une itération indépendemment de \\(x_0\\).   Il suffit d’écrire la condition d’optimalité \\(\\nabla g(x) = 0\\) qui donne exactement l’itération de Newton. \n\n\nQuestion 5\nComplétez la fonction newton ci-dessous pour résoudre (2). Calculer l’inverse de la hessienne est très couteux (complexité \\(O(n^3)\\)), comment peut-on y remédier ? Vérifiez le point (4) numériquement.\n  Au lieu d’inverser la hessienne, on résout un système linéaire, ce qui est 3 fois moins couteux.\n\nseed = 1729 # Seed du générateur aléatoire\nm, n = 50, 100\nrnd = np.random.RandomState(seed) # générateur aléatoire\nA = rnd.randn(m, n) # une matrice avec des entrées aléatoires gaussiennes\nb = rnd.randn(m) # on génére b aléatoirement également \ngamma = 1.\n\ndef g(x):\n    \"\"\"Compute the objective function g at a given x in R^n.\"\"\"\n    Ax = A.dot(x)\n    gx = 0.5 * np.linalg.norm(Ax - b) ** 2 + gamma * np.linalg.norm(x) ** 2\n    return gx\n\ndef gradient_g(x):\n    \"\"\"Compute the gradient of g at a given x in R^n.\"\"\"\n    # A faire\n    g = A.T.dot(A.dot(x) - b) + 2 * gamma * x\n    return g\n\ndef hessian_g(x):\n    \"\"\"Compute the hessian of g at a given x in R^n.\"\"\"\n    # A faire\n    n = len(x)\n    h = A.T.dot(A) + 2 * gamma * np.identity(n)\n    return h\n\nVous pouvez vérifier que votre gradient est bon en utilisant la fonction de scipy scipy.optimize.check_grad. Exécutez scipy.optimize.check_grad? pour obtenir la documentation de la fonction.\n\nfrom scipy.optimize import check_grad\nx_test = rnd.randn(n) # point où on veut évaluer le gradient\ncheck_grad(g, gradient_g, x_test) # compare gradient_g à des accroissements petis de g\n\n0.00022141735630660996\n\n\n\ndef newton(x0, g=g, gradient=gradient_g, hessian=hessian_g, maxiter=10, verbose=True):\n    \"\"\"Solve min g with newton method\"\"\"\n    \n    x = x0.copy()\n    if verbose:\n        strings = [\"Iteration\", \"g(x_k)\", \"max|gradient(x_k)|\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        H = hessian(x)\n        d = gradient(x)\n        \n        if verbose:\n            obj = g(x)\n            strings = [i, obj, abs(d).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        x = x + np.linalg.solve(H, - d)\n\n    return x\n\n\nx0 = rnd.randn(n)\nx = newton(x0)\n\n  Iteration   |     g(x_k)    | max|gradient(x_k)|\n      0       | 3570.345200187844 | 291.7910294967238\n      1       | 1.0704068547572962 | 1.0200174038743626e-13\n      2       | 1.0704068547572965 | 7.507883204027621e-15\n      3       | 1.070406854757296 | 6.106226635438361e-15\n      4       | 1.0704068547572962 | 4.884981308350689e-15\n      5       | 1.070406854757296 | 4.810388198883686e-15\n      6       | 1.0704068547572962 | 6.895525817007808e-15\n      7       | 1.070406854757296 | 5.412337245047638e-15\n      8       | 1.070406854757296 | 5.224987109642143e-15\n      9       | 1.070406854757296 | 5.662137425588298e-15\n\n\n\n\n\n1.3 Optimisation avec contraintes d’égalité\nOn s’intéresse à présent au problème avec contrainte linéaire: \\[ \\min_{\\substack{x \\in \\mathbb{R}^n \\\\ Cx = d}} \\frac{1}{2}\\|Ax - b\\|^2 + \\gamma \\|x \\|^2 \\]\n\nQuestion 6\nDonnez (en justifiant) le système KKT du problème.\n  Notons la fonction objective par \\(g\\) et les contraintes linéaires par \\(h(x) = Cx - d = 0\\). Remearquez ici que h regroupe toutes les contraintes linéaires données par \\(\\langle C_i, x\\rangle - d_i = 0\\) où l’indice i dénote la ième ligne. Notons chacune de ces contraintes par \\(h_i, i=1\\dots p\\)\n\nexistence: Par continuité de h, l’ensemble des contraintes est un fermé. g est continue et clairement coercive, le minimum donc existe.\nconvexité: Comme \\(\\gamma &gt; 0\\), on a pour tout \\(x,h  \\in \\mathbb{R}^n\\): \\[ h^{\\top} \\nabla^2 g(x)h = h^{\\top}(A^{\\top}A + 2\\gamma I_n) = \\|Ah\\|^2 + 2 \\gamma \\|h\\|^2 &gt; 0\\] La hessienne de g est définie positive, donc g est strictivement convexe. La solution est donc unique.\nKKT: Les contraintes sont linéaires, elles sont donc qualifiées sur K, donc toute solution du problème vérifie KKT. Par convexité (+ qualification), KKT est aussi une condition suffisante, donc toute solution de KKT est un minimum. Par unicité, La solution de KKT est la solution du problème. Notons la \\((x, \\mu) \\in \\mathbb{R}^n \\times \\mathbb{R}^p\\) :\n$ {\n\\[\\begin{array}{l}\n            \\nabla g(x) + \\sum_{i=1}^p \\mu_i\\nabla h_i(x) = 0 \\\\\n            h(x) = 0\n         \\end{array}\\]\n. $\n\ni.e $ {\n\\[\\begin{array}{l}\n                A^{\\top}(Ax - b) + 2\\gamma x + C^{\\top}\\mu = 0 \\\\\n                Cx - d = 0\n            \\end{array}\\]\n. $ \n\n\nQuestion 7\nExpliquer comment peut-on utiliser la méthode de Newton pour résoudre le système KKT.\n  Le problème est équivalent à son système KKT, on peut donc résoudre KKT avec la méthode de Newton appliquée à F ci-dessous pour résoudre \\(F(x, \\mu) = 0\\): \\[ F(x, \\mu) = \\left(\\nabla g(x) + \\mu \\nabla h(x), h(x)\\right) \\] La suite de Newton s’écrit donc: \\[(x_{k+1}, \\mu_{k+1}) = (x_{k}, \\mu_{k}) - J_F^{-1}(x_{k}, \\mu_{k}) F(x_{k}, \\mu_{k}) \\]\nOn a donc besoin d’écrire la Jacobienne de F. On a:\n$ F(x, ) = (A^{}(Ax - b) + 2x + C^{}, Cx - d) $\nOn écrit la matricienne Jacobienne de F par blocs:\n$ J_F(x, ) =\n\\[\\begin{pmatrix} A^{\\top}A + 2\\gamma I_n & C^{\\top} \\\\ C & 0 \\end{pmatrix}\\]\n$ \n\n\nQuestion 8\nImplémentez la fonction F dont on veut trouver un zéro et sa matrice Jacobienne.\n\n\nQuestion 9\nImplémentez la version de newton adaptée.\n\np = 5 # nombre de contraintes\nC = rnd.randn(p, n)\nd = rnd.randn(p)\n\ndef F(x, mu):\n    \"\"\"Compute the function F.\"\"\"\n    # On note f1 et f2 les composantes de F\n    f1 = gradient_g(x) + C.T.dot(mu)\n    f2 = C.dot(x) - d\n    f = np.hstack((f1, f2))  # on concatene f1 et f2\n    return f\n    \ndef jac_F(x, mu):\n    \"\"\"Compute the jacobian of F.\"\"\"\n    # on crée une matrice de taille (n + p) x (n + p)\n    J = np.zeros((n + p, n + p))\n    J[:n, :n] = hessian_g(x)\n    J[:n, n:] = C.T\n    J[n:, :n] = C\n    \n    return J\n\ndef newton_constrained(xmu0, F=F, jac=jac_F, maxiter=10, verbose=True):\n    \"\"\"Solve constrained min g with newton method\"\"\"\n    \n    xmu = xmu0.copy()\n    x = xmu[:n]\n    mu = xmu[n:]\n    if verbose:\n        strings = [\"Iteration\", \"max(abs(F(x_k, mu_k)))\"]\n        strings = [s.center(13) for s in strings]\n        strings = \" | \".join(strings)\n        print(strings)\n\n    for i in range(maxiter):\n        J = jac(x, mu)\n        f = F(x, mu)\n        \n        if verbose:\n            strings = [i, abs(f).max()] # On affiche des trucs \n            strings = [str(s).center(13) for s in strings]\n            strings = \" | \".join(strings)\n            print(strings)\n        \n        # A faire\n        xmu = xmu + np.linalg.solve(J, - f)\n        x = xmu[:n]\n        mu = xmu[n:]\n        \n    return x\n\n\nxmu0 = rnd.randn(n + p)\nx = newton_constrained(xmu0)\n\n  Iteration   | max(abs(F(x_k, mu_k)))\n      0       | 159.44416332283464\n      1       | 9.542019951958025e-14\n      2       | 7.212980213111564e-15\n      3       | 4.5449755070592346e-15\n      4       | 7.369105325949477e-15\n      5       | 7.271960811294775e-15\n      6       | 6.04572815421367e-15\n      7       | 6.25888230132432e-15\n      8       | 4.246603069191224e-15\n      9       | 6.300515664747763e-15\n\n\n\nC.dot(x) - d\n\narray([ 1.38777878e-16,  2.22044605e-16, -1.11022302e-16,  3.67761377e-16,\n       -2.22044605e-16])"
  }
]